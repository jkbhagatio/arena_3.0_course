{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is single hidden layer: x -> n_h -> y  (size of x and y is equal)\n",
    "\n",
    "W1 = (n_h, x), W2 = (y, n_h)\n",
    "x = 3 features (should be able to vary this)\n",
    "y = abs(x) \n",
    "n_h = 2 neurons (should be able to vary this)\n",
    "loss = MSE\n",
    "\n",
    "- Vary the activation function used in each layer (e.g. ReLU or identity, by default, ReLU)\n",
    "\n",
    "- Vary the bias value used in each layer (all equal, specific values, or function of index)\n",
    "\n",
    "- Vary the feature sparsity (all equal, specific values, or function of index)\n",
    "\n",
    "- Vary the feature importance (all equal, specific values, or function of index)\n",
    "\n",
    "- Optional: Vary optimizer and lr schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Notebook settings and imports.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive\n",
    "\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "\n",
    "from einops import asnumpy, einsum, rearrange, reduce, repeat, pack, parse_shape, unpack\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from jaxtyping import Float, Int\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly import io as pio\n",
    "from rich import print as rprint\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CisConfig:\n",
    "    n_instances: int = 2\n",
    "    n_feat: int = 2\n",
    "    n_hidden: int = 3\n",
    "    act_fn: List[Callable] = field(default_factory=lambda: [F.relu, F.relu])\n",
    "    b: str | t.Tensor | Callable = field(default_factory=lambda: \"0\")\n",
    "    feat_sparsity: float| t.Tensor | Callable = 0.99\n",
    "    feat_importance: float | t.Tensor | Callable = 1.0\n",
    "    optimizer: Callable = t.optim.Adam\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Ensure attribute values are valid.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cis(nn.Module):\n",
    "    \"\"\"Computation in Superposition toy model.\"\"\"\n",
    "\n",
    "    # Some attribute type hints\n",
    "    W1: Float[t.Tensor, \"n_instances n_feat n_hidden\"]\n",
    "    W2: Float[t.Tensor, \"n_instances n_hidden n_feat\"]\n",
    "    b1: Float[t.Tensor, \"n_instances n_hidden\"]\n",
    "    b2: Float[t.Tensor, \"n_instances n_feat\"]\n",
    "    S: Float[t.Tensor, \"n_instances n_feat\"]  # feature sparsity\n",
    "    I: Float[t.Tensor, \"n_instances n_feat\"]  # feature importance\n",
    "\n",
    "\n",
    "def __init__(self, cfg: CisConfig):\n",
    "    \"\"\"Initializes model params.\"\"\"\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "\n",
    "    # Weights\n",
    "    self.W1 = nn.init.xavier_normal_(t.empty(cfg.n_instances, cfg.n_feat, cfg.n_hidden))\n",
    "    self.W2 = nn.init.xavier_normal_(t.empty(cfg.n_instances, cfg.n_hidden, cfg.n_feat))\n",
    "\n",
    "    # Biases\n",
    "    if cfg.b == \"0\":\n",
    "        self.b1 = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden))\n",
    "        self.b2 = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_feat))\n",
    "    elif isinstance(cfg.b, t.Tensor):\n",
    "        self.b1 = t.Tensor([nn.Parameter(cfg.b[0, :, :]) for i in range(cfg.n_instances)])\n",
    "        self.b2 = t.Tensor([nn.Parameter(cfg.b[1, :, :]) for i in range(cfg.n_instances)])\n",
    "\n",
    "    # Sparsities\n",
    "    if isinstance(self.cfg.feat_sparsity, float):\n",
    "        self.S = t.full((self.cfg.n_feat,), self.cfg.feat_sparsity)\n",
    "    elif callable(self.cfg.feat_sparsity):\n",
    "        self.S = t.tensor([self.cfg.feat_sparsity(i) for i in range(self.cfg.n_feat)])\n",
    "\n",
    "    # Importances\n",
    "    if isinstance(self.cfg.feat_importance, float):\n",
    "        self.I = t.full((self.cfg.n_feat,), self.cfg.feat_importance)\n",
    "    elif callable(self.cfg.feat_importance):\n",
    "        self.I = t.tensor([self.cfg.feat_importance(i) for i in range(self.cfg.n_feat)])\n",
    "\n",
    "\n",
    "def gen_batch(self, batch_sz: int) -> Float[t.Tensor, \"batch_sz n_instances n_feat\"]:\n",
    "    \"\"\"Generates a batch of data (sparse feature vals on [-1, 1]).\"\"\"\n",
    "\n",
    "    # Randomly generate features vals, and for each, randomly determine which samples are non-zero\n",
    "    x = t.rand(batch_sz, self.cfg.n_instances, self.cfg.n_feat) * 2 - 1  # [-1, 1]\n",
    "    is_active = t.rand(batch_sz, self.cfg.n_instances, self.cfg.n_feat) < self.S\n",
    "    \n",
    "    return x * is_active\n",
    "\n",
    "\n",
    "def forward(self, x: Float[t.Tensor, \"batch_sz n_instances n_feat\"]) -> Float[t.Tensor, \"\"]:\n",
    "    \"\"\"Forward pass following the paper's model setup.\"\"\"\n",
    "\n",
    "    # First layer\n",
    "    h = einsum(x, self.W1, \"batch instance feat, feat hidden -> batch instance hidden\")\n",
    "    h = self.cfg.act_fn[0](h + self.b1)\n",
    "\n",
    "    # Second layer\n",
    "    y = einsum(h, self.W2, \"batch instance hidden, hidden feat -> batch instance feat\")\n",
    "    y = self.cfg.act_fn[1](y + self.b2)\n",
    "\n",
    "    # Compute weighted MSE loss\n",
    "    y_true = t.abs(x)\n",
    "    loss = reduce(((y - y_true) ** 2 * self.I), \"batch instance feat -> instance\", \"mean\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def optimize(\n",
    "    self, optimizer: t.optim.Optimizer, batch_sz: int, steps: int, logging_freq: int\n",
    "):\n",
    "    \"\"\"Optimizes the model.\"\"\"\n",
    "\n",
    "    losses = []\n",
    "    pbar = tqdm(range(steps), desc=\"Training\")\n",
    "\n",
    "    for step in pbar:\n",
    "        x = self.gen_batch(batch_sz)\n",
    "        loss = self.forward(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Log progress\n",
    "        if step % logging_freq == 0 or (step + 1 == steps):\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena_3.0_gpu_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
