{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is single hidden layer: x -> n_h -> y  (size of x and y is equal)\n",
    "\n",
    "W1 = (n_h, x), W2 = (y, n_h)\n",
    "x = 3 features (should be able to vary this)\n",
    "y = abs(x) \n",
    "n_h = 2 neurons (should be able to vary this)\n",
    "loss = MSE\n",
    "\n",
    "- Vary the activation function used in each layer (e.g. ReLU or identity, by default, ReLU)\n",
    "\n",
    "- Vary the bias value used in each layer (all equal or specific values)\n",
    "\n",
    "- Vary the feature sparsity (all equal or specific values (e.g. function of index))\n",
    "\n",
    "- Vary the feature importance (all equal or specific values (e.g. function of index))\n",
    "\n",
    "- Optional: Vary optimizer and lr schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Notebook settings and imports.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive\n",
    "\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "\n",
    "from einops import asnumpy, einsum, rearrange, reduce, repeat, pack, parse_shape, unpack\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from jaxtyping import Float, Int\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly import io as pio\n",
    "from rich import print as rprint\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CisConfig:\n",
    "    n_instances: int = 2\n",
    "    n_feat: int = 2\n",
    "    n_hidden: int = 3\n",
    "    act_fn: List[Callable] = field(default_factory=lambda: [F.relu, F.relu])\n",
    "    b1: str | Float[t.Tensor, \"inst hid\"] = field(default_factory=lambda: \"0\")\n",
    "    b2: str | Float[t.Tensor, \"inst hid\"] = field(default_factory=lambda: \"0\")\n",
    "    feat_sparsity: float| t.Tensor = 0.99\n",
    "    feat_importance: float | t.Tensor = 1.0\n",
    "    optimizer: Callable = t.optim.Adam\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Ensure attribute values are valid.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CisConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     S: Float[t\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minst feat\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# feature sparsity\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     I: Float[t\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minst feat\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# feature importance\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[43mCisConfig\u001b[49m):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes model params.\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CisConfig' is not defined"
     ]
    }
   ],
   "source": [
    "class Cis(nn.Module):\n",
    "    \"\"\"Computation in Superposition toy model.\"\"\"\n",
    "\n",
    "    # Some attribute type hints\n",
    "    W1: Float[t.Tensor, \"inst feat hid\"]\n",
    "    W2: Float[t.Tensor, \"inst hid feat\"]\n",
    "    b1: Float[t.Tensor, \"inst hid\"]\n",
    "    b2: Float[t.Tensor, \"inst feat\"]\n",
    "    s: Float[t.Tensor, \"inst feat\"]  # feature sparsity\n",
    "    i: Float[t.Tensor, \"inst feat\"]  # feature importance\n",
    "\n",
    "\n",
    "def __init__(self, cfg: CisConfig):\n",
    "    \"\"\"Initializes model params.\"\"\"\n",
    "    super().__init__()\n",
    "    self.cfg = cfg\n",
    "\n",
    "    # Model Weights\n",
    "    self.W1 = nn.init.xavier_normal_(t.empty(cfg.n_instances, cfg.feat, cfg.n_hidden))\n",
    "    self.W2 = nn.init.xavier_normal_(t.empty(cfg.n_instances, cfg.n_hidden, cfg.feat))\n",
    "\n",
    "    # Model Biases\n",
    "    self.b1 == nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden)) if cfg.b1 == \"0\" else cfg.b1\n",
    "    self.b2 == nn.Parameter(t.zeros(cfg.n_instances, cfg.feat)) if cfg.b2 == \"0\" else cfg.b2\n",
    "\n",
    "    # Sparsities\n",
    "    if isinstance(cfg.feat_sparsity, float):\n",
    "        self.s = t.full((cfg.n_instances, cfg.feat), cfg.feat_sparsity)\n",
    "    else:\n",
    "        self.s = cfg.feat_sparsity\n",
    "\n",
    "    # Importances\n",
    "    if isinstance(self.cfg.feat_importance, float):\n",
    "        self.i = t.full((self.cfg.feat,), self.cfg.feat_importance)\n",
    "    elif callable(self.cfg.feat_importance):\n",
    "        self.i = cfg.feat_importance\n",
    "\n",
    "\n",
    "def gen_batch(self, batch_sz: int) -> Float[t.Tensor, \"batch inst feat\"]:\n",
    "    \"\"\"Generates a batch of data (sparse feature vals on [-1, 1]).\"\"\"\n",
    "\n",
    "    # Randomly generate features vals, and for each, randomly determine which samples are non-zero\n",
    "    x = t.rand(batch_sz, self.cfg.n_instances, self.cfg.feat) * 2 - 1  # [-1, 1]\n",
    "    is_active = t.rand(batch_sz, self.cfg.n_instances, self.cfg.feat) < (1 - self.s)\n",
    "    \n",
    "    return x * is_active\n",
    "\n",
    "\n",
    "def forward(\n",
    "    self, x: Float[t.Tensor, \"batch inst feat\"], fx: Callable = t.abs\n",
    ") -> Float[t.Tensor, \"\"]:\n",
    "    \"\"\"Runs a forward pass through model returning the loss.\"\"\"\n",
    "\n",
    "    # Hidden layer\n",
    "    h = einsum(x, self.W1, \"batch inst feat, feat hid -> batch inst hid\")\n",
    "    h = self.cfg.act_fn[0](h + self.b1)\n",
    "\n",
    "    # Output layer\n",
    "    y = einsum(h, self.W2, \"batch inst hid, hid feat -> batch inst feat\")\n",
    "    y = self.cfg.act_fn[1](y + self.b2)\n",
    "\n",
    "    # Compute weighted MSE loss\n",
    "    y_true = fx(x)\n",
    "    loss = reduce(((y - y_true) ** 2 * self.i), \"batch inst feat -> \", \"mean\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def optimize(\n",
    "    self, optimizer: t.optim.Optimizer, batch_sz: int, steps: int, logging_freq: int\n",
    "):\n",
    "    \"\"\"Optimizes the model.\"\"\"\n",
    "\n",
    "    losses = []\n",
    "    pbar = tqdm(range(steps), desc=\"Training\")\n",
    "\n",
    "    for step in pbar:\n",
    "        x = self.gen_batch(batch_sz)\n",
    "        loss = self.forward(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Log progress\n",
    "        if step % logging_freq == 0 or (step + 1 == steps):\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.rand(2, 2, 3) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1897)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(x, \"batch instance feat -> \", \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2367,  0.0967, -0.2356],\n",
       "         [-0.1621, -0.4485,  0.5213]],\n",
       "\n",
       "        [[-0.3576,  0.8022,  0.2868],\n",
       "         [ 0.1523, -0.2837,  0.5812]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena_3.0_gpu_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
