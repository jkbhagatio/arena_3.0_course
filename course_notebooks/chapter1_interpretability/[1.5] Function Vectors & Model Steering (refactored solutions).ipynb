{"cells":[{"cell_type":"markdown","id":"d580421d","metadata":{"id":"d580421d"},"source":["# [1.5] Function Vectors & Model Steering"]},{"cell_type":"markdown","id":"ede699bb","metadata":{"id":"ede699bb"},"source":["Colab: [exercises](https://colab.research.google.com/drive/1NYjR3tjOiDJ2v8nv3mhrph-_IM4p9goS?usp=sharing) | [solutions](https://colab.research.google.com/drive/1dQ-p8j_cCjHCQ82pc446vxxj5pNd2DvN?usp=sharing)\n","\n","ARENA 3.0 [Streamlit page](https://arena3-chapter1-transformer-interp.streamlit.app/[1.5]_Function_Vectors_&_Model_Steering)\n","\n","Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-28h0xs49u-ZN9ZDbGXl~oCorjbBsSQag), and ask any questions on the dedicated channels for this chapter of material.\n","\n","If you want to flag specific errors / make suggestions regarding the `nnsight` library, you can [join the NDIF Discord group](https://discord.gg/QQQAM45b5z) and get involved!\n","\n","> **Important note** - These exercises are refactored to work with the newest version of `nnsight` library. **Please make sure to update to the most recent nnsight version** (`>=0.2`); if you downloaded nnsight a while ago, some of the function syntax will have changed.  If you're facing issues, you should try and change the argument `remote=True` to `remote=False` (this will require having compute equivalent to an A100; like you'd have in a Colab Pro+ subscription). We've defined a global `REMOTE` variable near the start of the exercises (default value is True)."]},{"cell_type":"markdown","id":"fc7dec9b","metadata":{"id":"fc7dec9b"},"source":["<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/fv_header.png\" width=\"350\">\n"]},{"cell_type":"markdown","id":"29tMgKOSfPT4","metadata":{"id":"29tMgKOSfPT4"},"source":["# Introduction"]},{"cell_type":"markdown","id":"jFb2nW35fRqA","metadata":{"id":"jFb2nW35fRqA"},"source":["These exercises serve as an exploration of the following question: ***can we steer a model to produce different outputs / have a different behaviour, by intervening on the model's forward pass using vectors found by non gradient descent-based methods?***\n","\n","The majority of the exercises focus on [function vectors](https://functions.baulab.info/): vectors which are extracted from forward passes on in-context learning (ICL) tasks, and added to the residual stream in order to trigger the execution of this task from a zero-shot prompt. The diagram below illustrates this.\n","\n","<img src=\"https://functions.baulab.info/images/Paper/fv-demonstrations.png\" width=\"650\">\n","\n","The exercises also take you through use of the `nnsight` library, which is designed to support this kind of work (and other interpretability research) on very large language models - i.e. larger than models like GPT2-Small which you might be used to at this point in the course.\n","\n","The final set of exercises look at Alex Turner et al's work on [steering vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector), which is conceptually related but has different aims and methodologies."]},{"cell_type":"markdown","id":"c5CF2fOYLt1x","metadata":{"id":"c5CF2fOYLt1x"},"source":["## Content & Learning Objectives\n"]},{"cell_type":"markdown","id":"3DdwJ_RBPtXg","metadata":{"id":"3DdwJ_RBPtXg"},"source":["### 1️⃣ Introduction to `nnsight`\n","\n","In this section, you'll learn the basics of how to use the `nnsight` library: running forward passes on your model, and saving the internal states. You'll also learn some basics of HuggingFace models which translate over into `nnsight` models (e.g. tokenization, and how to work with model output).\n","\n","> ##### Learning Objectives\n",">\n","> * Learn the basics of the `nnsight` library, and what it can be useful for\n","> * Use it to extract & visualise GPT-J-6B's internal activations\n","\n","<br>\n","\n","### 2️⃣ Task-encoding hidden states\n","\n","We'll begin with the following question, posed by the Function Vectors paper:\n","\n","> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*\n","\n","We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.\n","\n","This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.\n","\n","(Note - this section structurally follows section 2.1 of the function vectors paper).\n","\n","> ##### Learning Objectives\n",">\n","> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself\n","> * Reproduce the \"h-vector results\" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts\n","\n","<br>\n","\n","### 3️⃣ Function Vectors\n","\n","In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.\n","\n","We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `\"When you think of Netherlands, you usually think of\"` by talking about Amsterdam.\n","\n","(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).\n","\n","> ##### Learning Objectives\n",">\n","> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task\n","> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head\n","> * Learn how to use `nnsight` for multi-token generation\n","\n","<br>\n","\n","### 4️⃣ Steering Vectors in GPT2-XL\n","\n","Here, we discuss a different but related set of research: Alex Turner's work on steering vectors. This also falls under the umbrella of \"interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour\", but it has a different setup, objectives, and approach.\n","\n","> ##### Learning Objectives\n",">\n","> * Understand the goals & main results from Alex Turner et al's work on steering vectors\n","> * Reproduce the changes in behaviour described in their initial post\n","\n","<br>\n","\n","### 5️⃣ Bonus\n","\n","Lastly, we discuss some possible extensions of function vectors & steering vectors work, which is currently an exciting area of development (e.g. with a paper on steering Llama 2-13b coming out as recently as December 2023)."]},{"cell_type":"markdown","id":"97292f90-5c2e-444d-88e4-41265e8ad0d9","metadata":{"id":"97292f90-5c2e-444d-88e4-41265e8ad0d9"},"source":["# Setup (don't read, just run!)"]},{"cell_type":"code","execution_count":null,"id":"bC46t3VqNu0S","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bC46t3VqNu0S","executionInfo":{"status":"ok","timestamp":1718375641661,"user_tz":-60,"elapsed":59586,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"a32b8572-0899-4d9c-995c-a508b527f090"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nnsight in /usr/local/lib/python3.10/dist-packages (0.2.19)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from nnsight) (4.41.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from nnsight) (3.20.3)\n","Requirement already satisfied: python-socketio[client] in /usr/local/lib/python3.10/dist-packages (from nnsight) (5.11.2)\n","Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.19.1)\n","Requirement already satisfied: pydantic>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.7.4)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from nnsight) (2.3.0+cu121)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.1.99)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.18.0+cu121)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.31.0)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.29.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnsight) (0.8.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.0->nnsight) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.0->nnsight) (2.18.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.0->nnsight) (4.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->nnsight) (0.23.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->nnsight) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->nnsight) (12.5.40)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (6.0.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight) (0.4.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (5.2.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (2.32.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight) (9.4.0)\n","Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (0.23.1)\n","Requirement already satisfied: python-engineio>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (4.9.1)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->nnsight) (4.66.4)\n","Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from python-engineio>=4.8.0->python-socketio[client]->nnsight) (1.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers->nnsight) (2024.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->nnsight) (3.19.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->nnsight) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->nnsight) (1.3.0)\n","Requirement already satisfied: wsproto in /usr/local/lib/python3.10/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight) (1.2.0)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio[client]->nnsight) (0.14.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n","Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.30)\n","Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n","Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (2.1.0)\n","Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.31.0)\n","Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n","Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n","Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.20.0)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n","Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n","Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.30)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.25.2)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.1.99)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.3.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.4)\n","Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.41.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n","Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.17.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.14.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.5)\n","Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer_lens) (12.5.40)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.5.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n","Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n","  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-9p1k_bz7\n","  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-9p1k_bz7\n","  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 1e6129d08cae7af9242d9ab5d3ed322dd44b4dd3\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==0.0.0) (5.2.0)\n","Requirement already satisfied: numpy<2.0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==0.0.0) (1.25.2)\n","Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==0.0.0) (2.3.0+cu121)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.19.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0,>=2.0->circuitsvis==0.0.0) (12.5.40)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0,>=2.0->circuitsvis==0.0.0) (1.3.0)\n","Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.36.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.12.2)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n","Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.1)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.4)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.8)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n","Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n","Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n","Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n","Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.1.1)\n","Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n","Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.7.4)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.18.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"]}],"source":["try:\n","    import google.colab # type: ignore\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","\n","import os, sys\n","chapter = \"chapter1_transformer_interp\"\n","repo = \"ARENA_3.0\"\n","\n","if IN_COLAB:\n","    # Install packages\n","    %pip install nnsight\n","    %pip install einops\n","    %pip install openai==0.28\n","    %pip install jaxtyping\n","    %pip install plotly\n","    %pip install transformer_lens\n","    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n","    %pip install gradio typing-extensions\n","    %pip install --upgrade pydantic\n","\n","    # Code to download the necessary files (e.g. solutions, test funcs)\n","    if not os.path.exists(f\"/content/{chapter}\"):\n","        !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n","        !unzip /content/main.zip 'ARENA_3.0-main/chapter1_transformer_interp/exercises/*'\n","        sys.path.append(f\"/content/{repo}-main/{chapter}/exercises\")\n","        os.remove(\"/content/main.zip\")\n","        os.rename(f\"{repo}-main/{chapter}\", chapter)\n","        os.rmdir(f\"{repo}-main\")\n","        os.chdir(f\"{chapter}/exercises\")\n","else:\n","    chapter_dir = r\"./\" if chapter in os.listdir() else os.getcwd().split(chapter)[0]\n","    sys.path.append(chapter_dir + f\"{chapter}/exercises\")"]},{"cell_type":"code","execution_count":null,"id":"h0hiZDSMm7-m","metadata":{"id":"h0hiZDSMm7-m"},"outputs":[],"source":["import time\n","from pathlib import Path\n","from typing import List, Optional, Tuple, Union\n","import circuitsvis as cv\n","import numpy as np\n","import openai\n","import plotly.express as px\n","import torch as t\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from IPython.display import display\n","from jaxtyping import Float, Int\n","from rich import print as rprint\n","from rich.table import Table\n","from torch import Tensor\n","from tqdm import tqdm\n","import einops\n","import os\n","import sys\n","import gdown\n","import zipfile\n","from IPython.display import clear_output\n","from collections import defaultdict\n","\n","import nnsight\n","from nnsight import LanguageModel\n","from nnsight.intervention import InterventionProxy\n","from nnsight import CONFIG\n","\n","# Hide bunch of info logging messages from nnsight\n","import logging\n","logging.disable(sys.maxsize)\n","\n","device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n","\n","t.set_grad_enabled(False);\n","\n","# Make sure exercises are in the path\n","exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n","section_dir = (exercises_dir / \"part5_function_vectors_and_model_steering\").resolve()\n","if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n","\n","from plotly_utils import imshow\n","import part5_function_vectors_and_model_steering.solutions as solutions\n","import part5_function_vectors_and_model_steering.tests as tests"]},{"cell_type":"markdown","id":"fae35f9e","metadata":{"id":"fae35f9e"},"source":["# 1️⃣ Introduction to `nnsight`"]},{"cell_type":"markdown","id":"zTqg0RAxiN-8","metadata":{"id":"zTqg0RAxiN-8"},"source":["In this section, you'll learn the basics of how to use the nnsight library: running forward passes on your model, and saving the internal states. You'll also learn some basics of HuggingFace models which translate over into nnsight models (e.g. tokenization, and how to work with model output).\n","\n","> ##### Learning Objectives\n",">\n","> * Learn the basics of the `nnsight` library, and what it can be useful for\n","> * Use it to extract & visualise GPT-J-6B's internal activations\n","\n","<br>\n","\n","---\n"]},{"cell_type":"markdown","id":"c43466c5","metadata":{"id":"c43466c5"},"source":["## Important syntax\n","\n","Here, we'll discuss some important syntax for interacting with `nnsight` models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as `nnsight` models, and some of it (e.g. forward passes) is specific to `nnsight`, i.e. it would work differently if you just had a standard HuggingFace model. Make sure to keep this distinction in mind, otherwise syntax can get confusing!\n","\n","### Model config\n","\n","Each model comes with a `model.config`, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with `model.config`. Run the code below to see this in action, and to define some useful variables for later."]},{"cell_type":"code","execution_count":null,"id":"ShVWMHFh7Ock","metadata":{"id":"ShVWMHFh7Ock","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718375642258,"user_tz":-60,"elapsed":602,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"989f250d-28c6-402f-e554-d7592cc01cdc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Number of heads: 16\n","Number of layers: 28\n","Model dimension: 4096\n","Head dimension: 256\n","\n","Entire config:  GPTJConfig {\n","  \"_name_or_path\": \"EleutherAI/gpt-j-6b\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPTJForCausalLM\"\n","  ],\n","  \"attn_pdrop\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gptj\",\n","  \"n_embd\": 4096,\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 28,\n","  \"n_positions\": 2048,\n","  \"resid_pdrop\": 0.0,\n","  \"rotary\": true,\n","  \"rotary_dim\": 64,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50,\n","      \"temperature\": 1.0\n","    }\n","  },\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"GPT2Tokenizer\",\n","  \"transformers_version\": \"4.41.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50400\n","}\n","\n"]}],"source":["model = LanguageModel('EleutherAI/gpt-j-6b', device_map='auto')\n","tokenizer = model.tokenizer\n","\n","N_HEADS = model.config.n_head\n","N_LAYERS = model.config.n_layer\n","D_MODEL = model.config.n_embd\n","D_HEAD = D_MODEL // N_HEADS\n","\n","print(f\"Number of heads: {N_HEADS}\")\n","print(f\"Number of layers: {N_LAYERS}\")\n","print(f\"Model dimension: {D_MODEL}\")\n","print(f\"Head dimension: {D_HEAD}\\n\")\n","\n","print(\"Entire config: \", model.config)"]},{"cell_type":"markdown","id":"7d6e143e","metadata":{"id":"7d6e143e"},"source":["### Tokenizers\n","\n","A model comes with a tokenizer, accessable with `model.tokenizer` (just like TransformerLens). Unlike TransformerLens, we won't be using utility functions like `model.to_str_tokens`, instead we'll be using the tokenizer directly. Some important functions for today's exercises are:\n","\n","* `tokenizer` (i.e. just calling it on some input)\n","    * This takes in a string (or list of strings) and returns the tokenized version.\n","    * It will return a dictionary, always containing `input_ids` (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. `attention_mask` - see dropdown).\n","    * Other useful arguments for this function:\n","        * `return_tensors` - if this is `\"pt\"`, you'll get results returned as PyTorch tensors, rather than lists (which is the default).\n","        * `padding` - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).\n","* `tokenizer.decode`\n","    * This takes in tokens, and returns the decoded string.\n","    * If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).\n","* `tokenizer.batch_decode`\n","    * Equivalent to `tokenizer.decode`, but it doesn't concatenate.\n","    * If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.\n","* `tokenizer.tokenize`\n","    * Takes in a string, and returns a list of strings.\n","\n","Run the code below to see some examples of these functions in action."]},{"cell_type":"code","execution_count":null,"id":"RwJmEfYJ7Qq9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718375642258,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"},"user_tz":-60},"id":"RwJmEfYJ7Qq9","outputId":"c64a3293-c873-43a9-c811-19813bb57951"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[1212, 1276,  307, 3635]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n","I never could get the hang of Thursdays.\n","['These', ' words', ' will', ' be', ' split', ' up']\n","['This sentence will be together', 'So will this one']\n","['This', 'Ġsentence', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']\n"]}],"source":["# Calling tokenizer returns a dictionary, containing input ids & other data.\n","# If returned as a tensor, then by default it will have a batch dimension.\n","print(tokenizer(\"This must be Thursday\", return_tensors=\"pt\"))\n","\n","# Decoding a list of integers, into a concatenated string.\n","print(tokenizer.decode([40, 1239, 714, 651, 262, 8181, 286, 48971, 12545, 13]))\n","\n","# Using batch decode, on both 1D and 2D input.\n","print(tokenizer.batch_decode([4711, 2456, 481, 307, 6626, 510]))\n","print(tokenizer.batch_decode([[1212, 6827, 481, 307, 1978], [2396, 481, 428, 530]]))\n","\n","# Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).\n","print(tokenizer.tokenize(\"This sentence will be tokenized\"))"]},{"cell_type":"markdown","id":"1c800352","metadata":{"id":"1c800352"},"source":["<details>\n","<summary>Note on <code>attention_mask</code> (optional)</summary>\n","\n","`attention_mask`, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don't allow these tokens to be attended to). This is useful when you have to do padding. For example:\n","\n","```python\n","model.tokenizer([\"Hello world\", \"Hello\"], return_tensors=\"pt\", padding=True)\n","```\n","\n","will return:\n","\n","```\n","{\n","    'attention_mask': tensor([[1, 1], [0, 1]]),\n","    'input_ids': tensor([[15496,   995], [50256, 15496]])\n","}\n","```\n","\n","We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.\n","\n","</details>\n","\n","### Model outputs\n","\n","At a high level, there are 2 ways to run our model: using the `forward` method and the `generate` method. We'll focus on `forward` for now, and we'll discuss `generate` when it comes to multi-token generation later.\n","\n","The default behaviour of `forward` in normal HuggingFace models is to return an object containing logits (and optionally a bunch of other things). In `nnsight` we use `trace` instead of `forward` and anything that we choose to return is explicitly returned inside the context manager.\n","\n","Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below."]},{"cell_type":"code","execution_count":null,"id":"aW-aeDv_UtTJ","metadata":{"id":"aW-aeDv_UtTJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718374918321,"user_tz":-60,"elapsed":7628,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"0fa22724-edf0-4c86-f16d-6ada4ebe246a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 367k/367k [00:00<00:00, 393kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","logits.shape = torch.Size([50400]) = (vocab_size,)\n","Predicted token ID = 6342\n","Predicted token = ' Paris'\n","\n","resid.shape = torch.Size([1, 10, 4096]) = (batch_size, seq_len, d_model)\n"]}],"source":["REMOTE = True # You should change this to False if the nnsight library is suffering from server issues.\n","# Please join the NDIF community Discord (https://nnsight.net/status/) and request a key\n","CONFIG.set_default_api_key(\"ezGOyJezZT2DOkuP2sGQ\") # yNPCSsk3WcPxU6E43M7K\n","\n","prompt = 'The Eiffel Tower is in the city of'\n","\n","with model.trace(remote=REMOTE) as runner:\n","    with runner.invoke(prompt) as invoker:\n","\n","        # Save the model's hidden states\n","        hidden_states = model.transformer.h[-1].output[0].save()\n","\n","        # Save the model's logit output\n","        logits = model.lm_head.output[0, -1].save()\n","\n","# Get the model's logit output, and it's next token prediction\n","print(f\"\\nlogits.shape = {logits.value.shape} = (vocab_size,)\")\n","\n","predicted_token_id = logits.value.argmax().item()\n","print(f\"Predicted token ID = {predicted_token_id}\")\n","print(f\"Predicted token = {tokenizer.decode(predicted_token_id)!r}\")\n","\n","# Print the shape of the model's residual stream\n","print(f\"\\nresid.shape = {hidden_states.value.shape} = (batch_size, seq_len, d_model)\")"]},{"cell_type":"markdown","id":"28a06fca-66e8-4c42-abe1-0e3c09d65c27","metadata":{"id":"28a06fca-66e8-4c42-abe1-0e3c09d65c27"},"source":["Lets go over this piece by piece.\n","\n","**First, we create a context block** by calling `.trace(...)` on the model object. This denotes that we wish to generate tokens given some prompts.\n","\n","```python\n","with model.trace(remote=REMOTE) as runner:\n","```\n","\n","By default, running this will cause your model to be loaded & run locally, but by passing `remote=True`, it causes the model to be run on the server instead. This is very useful when working with models too large to fit on your machine (or even models which can fit on your machine, but run slowly due to their size).\n","\n","Calling `.trace(...)` does not actually initialize or run the model. Only after the `with ... as runner:` block is exited is the model actually loaded and run. All operations in the block are \"proxies\" which essentially creates a graph of operations we wish to carry out later.\n","\n","**Within the runner context,** we create invocation contexts to specify the actual prompts we want to run.\n","\n","```python\n","with runner.invoke(prompt) as invoker:\n","```\n","\n","**Within an invoke context**, all operations/interventions will be applied to the processing of the prompt. Models can be run on a variety of input formats: strings, lists of tokens, tensors of tokens, etc.\n","\n","This is all we actually need to run a forward pass on the model. We could replace the `hidden_states` line with just `pass`, and we'd still be able to access the model output in the same way. But the most interesting part of `nnsight` is the ability to access the model's internal states (like you might already have done with TransformerLens). Let's see how this works!\n","\n","```python\n","hidden_states = model.transformer.h[-1].output[0].save()\n","```\n","\n","On this line we're saying: access the last layer of the transformer `model.transformer.h[-1]`, access this layer's output `.output` (which is a tuple of tensors), index the first tensor in this tuple `.output[0]`, and save it `.save()`.\n","\n","Let's break down this line in a bit more detail:\n","\n","* `model.transformer.h[-1]` is a way of indexing an appropriate module in the transformer. If you `print(model)`, you'll see that it consists of `transformer` and `lm_head` (for \"language modelling head\"). The `transformer` module is made up of embeddings & dropout, a series of layers (called `.h`, for \"hidden states\"), and a final layernorm. So indexing `.h[-1]` gives you the final layer.\n","    * Note - it's often useful to visit the documentation page for whatever model you're working on, e.g. you can find GPT-J [here](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html). Not all models will have a nice uniform standardized architecture like you might be used to in TransformerLens!\n","* `.output[0]` gives you this module's output, as a **proxy**.\n","    * The output of a module is often a tuple (again, you can see on the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) what the output of each module is). In this case, it's a tuple of 2 tensors, the first of which is the actual layer output (the thing we want).\n","    * When debugging, you can call `.shape` on a proxy. This will even work if the proxy represents a tuple of tensors; you'll get a tuple of all the sizes of these tensors.\n","    * Doing operations on a proxy still returns a proxy - this is why we can index into the `output` proxy tuple and get a proxy tensor!\n","    * You can also use `.input` to access the inputs to a module - this works in the same way (often also stored as a tuple).\n","* `.save()` takes this proxy output, and returns the actual object.\n","    * To be more specific, this informs the computation graph to clone the value of a proxy, allowing us to access the value of a proxy after the forward pass.\n","    * During processing of the intervention computational graph we are building, when the value of a proxy is no longer ever needed, its value is dereferenced and destroyed. If you've saved it, then you'll be able to access the value of the proxy after this happens (i.e. outside the context manager), using the `.value` attribute.\n","\n","<details>\n","<summary>Optional exercise - we mentioned that <code>.output</code> returns a tuple of 2 tensors. Can you find what the second tensor in this tuple is?</summary>\n","\n","The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called `present`. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we're only generating one new token, these are just the full keys and values.\n","\n","</details>\n","\n","The next command:\n","\n","```python\n","logits = model.lm_head.output[0, -1].save()\n","```\n","\n","works in a very similar way. If you look at the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html), you'll see that `lm_head` is the linear layer mapping the final value of the residual stream to the final logits. The output is just a single tensor, not a tuple of tensors. Our indexing returns the vector of logits for the token after the last one on the sequence.\n","\n","If you've worked with Hugging Face models then you might be used to getting logits directly from the model output, but here we generally extract logits from the model internals just like any other activation because this allows us to **control exactly what we return.** If we return lots of very large tensors, this can take quite a while to download from the server (remember that `d_vocab` is often very large for transformers, i.e. around 50k). See the \"which objects to save\" section below for more discussion on this.\n","\n","### Output vs input\n","\n","You can also extract a module's input using `.input`, although the syntax is slightly different. Whereas `.output` gives you the same object as returned by the module's forward function, `.input` gives you a tuple of (tuple of args, dictionary of kwargs), with args and kwargs being the input arguments to the forward function. This usually means you'll need to use `.input[0][0]` to get a tensor input to a particular module. If you're not sure exactly what the `.input` object type will be, then you can debug by printing its shape (remember printing shape works for tuples containing `InterventionProxy` objects, it's not just something which works on objects corresponding to acual tensors!).\n","\n","### Which objects to save\n","\n","Note that we saved `logits` above, which is a vector of length 50k. In general, it's best to save as small an object as possible, because this reduces the size of object you'll have to download from the server. For example, if you only want the next token completions, just argmax the logits and then save the result! All basic tensor operations can be performed within your context manager."]},{"cell_type":"markdown","id":"e4cd28ce","metadata":{"id":"e4cd28ce"},"source":["## Putting this into practice"]},{"cell_type":"markdown","id":"db99e061","metadata":{"id":"db99e061"},"source":["### Exercise - visualize attention heads\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-20 minutes on this exercise.\n","```"]},{"cell_type":"markdown","id":"64d605fd","metadata":{"id":"64d605fd"},"source":["We just covered a lot of content, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:\n","\n","```python\n","cv.attention.attention_patterns(\n","    tokens=tokens,\n","    attention=attention,\n",")\n","```\n","\n","where `tokens` is a list of strings, and `attention` is a tensor of shape `(num_heads, num_tokens, num_tokens)`.\n","\n","If you're stuck, [here's a link](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) to the source code for GPT-J. Look for how the attention patterns are calculated, within the `GPTJAttention` block.\n","\n","*Note - this model uses dropout on the attention probabilities, as you'll probably notice from looking at the source code in the link above. This won't affect the model's behaviour because dropout is disabled in inference mode (and using the `generate` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.*\n","\n","<details>\n","<summary>Aside - inference mode</summary>\n","\n","Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).\n","\n","If you want to run the model without inference mode, you can wrap your code in `with model.trace(inference=False):`. However, you don't need to worry about this for the purposes of these exercises.\n","\n","</details>\n","\n","If you're stuck on how to reference the right module, see the following hint:\n","\n","<details>\n","<summary>Hint - what module you should get attention from</summary>\n","\n","You want to extract attention from `model.transformer.h[0].attn.attn_dropout.input`. If you used `.output`, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.\n","\n","Remember, you need to use `.input[0]` to get the tuple of inputs, then `.input[0][0]` to get the tensor input.\n","\n","</details>\n","\n","<details>\n","<summary>Aside - GPT2 tokenizer uses special characters to represent space </summary>\n","\n","GPT2 tokenizer uses \"Ġ\" to represent prepended space. So [\"My\", \" name\", \" is\", \" James\"] will be tokenized as [\"My\", \"Ġname\", \"Ġis\", \"ĠJames\"]. Make sure you replace \"Ġ\" with an actual space.\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"id":"c087195b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"c087195b","outputId":"091ddcfb-216a-46eb-e12c-fbd6fcb9d37d","executionInfo":{"status":"ok","timestamp":1718090257209,"user_tz":-60,"elapsed":5160,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 7.64k/7.64k [00:00<00:00, 11.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Layer 0 Head Attention Patterns:\n"]},{"output_type":"display_data","data":{"text/plain":["<circuitsvis.utils.render.RenderedHTML at 0x7d811f323a00>"],"text/html":["<div id=\"circuits-vis-4ea694b3-a280\" style=\"margin: 15px 0;\"/>\n","    <script crossorigin type=\"module\">\n","    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n","    render(\n","      \"circuits-vis-4ea694b3-a280\",\n","      AttentionPatterns,\n","      {\"tokens\": [\"The\", \" E\", \"iff\", \"el\", \" Tower\", \" is\", \" in\", \" the\", \" city\", \" of\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.829063355922699, 0.1709366738796234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10449792444705963, 0.8631790280342102, 0.03232300281524658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0021944455802440643, 0.010442350059747696, 0.9758204221725464, 0.011542811058461666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019575104117393494, 0.0005238913581706583, 0.8991208672523499, 0.07888323813676834, 0.00189687788952142, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09968282282352448, 0.002919906284660101, 0.002329285955056548, 0.008620471693575382, 0.11867355555295944, 0.7677739262580872, 0.0, 0.0, 0.0, 0.0], [0.136972576379776, 0.035879284143447876, 0.009825360029935837, 0.013134370557963848, 0.03671043738722801, 0.5205317139625549, 0.246946319937706, 0.0, 0.0, 0.0], [0.09312193840742111, 0.008496231399476528, 0.0036471805069595575, 0.006210183259099722, 0.0073285480029881, 0.22380821406841278, 0.0488654188811779, 0.6085222959518433, 0.0, 0.0], [0.022006452083587646, 0.00582344364374876, 0.005709549877792597, 0.003698140848428011, 0.0571850910782814, 0.18564166128635406, 0.06623345613479614, 0.5254098773002625, 0.12829236686229706, 0.0], [0.004313653334975243, 0.000997802708297968, 0.00022000994067639112, 0.001366659882478416, 0.005537915974855423, 0.035766761749982834, 0.008457942865788937, 0.06083757057785988, 0.7149894833564758, 0.16751216351985931]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7337518930435181, 0.26624807715415955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01838100515305996, 0.9661518931388855, 0.01546707097440958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2840648293495178, 0.4624059498310089, 0.08217739313840866, 0.17135180532932281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2925432622432709, 0.025398895144462585, 0.23610804975032806, 0.11693126708269119, 0.32901859283447266, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23338383436203003, 0.06438979506492615, 0.05457119271159172, 0.04971351474523544, 0.4808809161186218, 0.11706069111824036, 0.0, 0.0, 0.0, 0.0], [0.1657487154006958, 0.03628508746623993, 0.03997809812426567, 0.039133112877607346, 0.41519615054130554, 0.12842558324337006, 0.17523330450057983, 0.0, 0.0, 0.0], [0.180885449051857, 0.027216656133532524, 0.036469731479883194, 0.02599567547440529, 0.15392789244651794, 0.1973164975643158, 0.16624361276626587, 0.21194443106651306, 0.0, 0.0], [0.0618751086294651, 0.01634237729012966, 0.053551092743873596, 0.02586762234568596, 0.38871869444847107, 0.02492690645158291, 0.0700240358710289, 0.0709606260061264, 0.2877334654331207, 0.0], [0.10785522311925888, 0.010135273449122906, 0.03009822964668274, 0.015300117433071136, 0.13075703382492065, 0.06642568111419678, 0.0843547135591507, 0.17502063512802124, 0.22528457641601562, 0.15476849675178528]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9375947713851929, 0.062405239790678024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5738810896873474, 0.38825109601020813, 0.037867799401283264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6073165535926819, 0.18196089565753937, 0.10958210378885269, 0.10114040225744247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26133978366851807, 0.4545663893222809, 0.04196811467409134, 0.10260380804538727, 0.13952194154262543, 0.0, 0.0, 0.0, 0.0, 0.0], [0.751213550567627, 0.030134251341223717, 0.03863784298300743, 0.034449756145477295, 0.052645716816186905, 0.09291891753673553, 0.0, 0.0, 0.0, 0.0], [0.08130470663309097, 0.008510570041835308, 0.011866837739944458, 0.003835457842797041, 0.04794764891266823, 0.13581953942775726, 0.7107152342796326, 0.0, 0.0, 0.0], [0.16110482811927795, 0.0050459858030080795, 0.006499174982309341, 0.006183331366628408, 0.004202309530228376, 0.00644022086635232, 0.0991506427526474, 0.7113734483718872, 0.0, 0.0], [0.02317010797560215, 0.008998817764222622, 0.03188164532184601, 0.01096675731241703, 0.7636091113090515, 0.008237290196120739, 0.0769900530576706, 0.03395511209964752, 0.04219113290309906, 0.0], [0.116624616086483, 0.004521341063082218, 0.005119765643030405, 0.006424611900001764, 0.010177545249462128, 0.008108649402856827, 0.19665247201919556, 0.48110663890838623, 0.012579270638525486, 0.15868504345417023]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873823523521423, 0.012617628090083599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7056407928466797, 0.03949311003088951, 0.2548661530017853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5494458675384521, 0.0274152010679245, 0.19888122379779816, 0.2242577224969864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13006025552749634, 0.023235520347952843, 0.3721879720687866, 0.3598882257938385, 0.11462805420160294, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1305249184370041, 0.021449703723192215, 0.1289438009262085, 0.27416515350341797, 0.057148322463035583, 0.38776811957359314, 0.0, 0.0, 0.0, 0.0], [0.2504964768886566, 0.0070879533886909485, 0.09113315492868423, 0.17518386244773865, 0.07337389141321182, 0.1366763412952423, 0.2660483121871948, 0.0, 0.0, 0.0], [0.11081759631633759, 0.01095819566398859, 0.04083588346838951, 0.06034962460398674, 0.03203092887997627, 0.09623464196920395, 0.19093497097492218, 0.45783811807632446, 0.0, 0.0], [0.05363741144537926, 0.025838645175099373, 0.09583625942468643, 0.18366417288780212, 0.050549477338790894, 0.13348351418972015, 0.23405805230140686, 0.17247039079666138, 0.05046213045716286, 0.0], [0.0948794037103653, 0.01575114205479622, 0.05621328949928284, 0.10723131895065308, 0.03950320929288864, 0.10456929355859756, 0.09332020580768585, 0.25014743208885193, 0.06267368793487549, 0.175710991024971]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8586137890815735, 0.14138615131378174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5009571313858032, 0.1769222617149353, 0.3221205770969391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018606580793857574, 0.11020754277706146, 0.8365734219551086, 0.034612443298101425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47340667247772217, 0.14704596996307373, 0.05715087056159973, 0.25223010778427124, 0.07016639411449432, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49333447217941284, 0.008234941400587559, 0.008012527599930763, 0.021872613579034805, 0.039670638740062714, 0.4288748502731323, 0.0, 0.0, 0.0, 0.0], [0.0806049332022667, 0.003389318473637104, 0.0021717180497944355, 0.0026488041039556265, 0.036696914583444595, 0.7740417122840881, 0.1004466637969017, 0.0, 0.0, 0.0], [0.0843658372759819, 0.008884117007255554, 0.006725005805492401, 0.013129661791026592, 0.01784602552652359, 0.1932033896446228, 0.16955676674842834, 0.5062891840934753, 0.0, 0.0], [0.13414528965950012, 0.014364572241902351, 0.005851211957633495, 0.011265160515904427, 0.009097116068005562, 0.16818930208683014, 0.325139582157135, 0.22524230182170868, 0.10670538246631622, 0.0], [2.4521175873815082e-05, 1.964564035006333e-05, 0.00011441286915214732, 3.142992500215769e-05, 5.566889012698084e-05, 0.001338052679784596, 0.026846611872315407, 0.0024312378372997046, 0.965787947177887, 0.00335049070417881]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5130533576011658, 0.48694664239883423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39759111404418945, 0.5228424668312073, 0.07956641167402267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2355359047651291, 0.22942200303077698, 0.208502858877182, 0.3265392482280731, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2904796302318573, 0.1287231743335724, 0.29519984126091003, 0.1299135386943817, 0.15568386018276215, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7625571489334106, 0.027590610086917877, 0.027350930497050285, 0.025809288024902344, 0.034936487674713135, 0.12175562977790833, 0.0, 0.0, 0.0, 0.0], [0.4462547302246094, 0.008537101559340954, 0.028795873746275902, 0.03293594345450401, 0.04105942323803902, 0.32290613651275635, 0.11951076984405518, 0.0, 0.0, 0.0], [0.28493914008140564, 0.011074135079979897, 0.009190276265144348, 0.019453978165984154, 0.02698729932308197, 0.1949893683195114, 0.3496210277080536, 0.10374484956264496, 0.0, 0.0], [0.05542522668838501, 0.06413155049085617, 0.03741440176963806, 0.0557597391307354, 0.5587040185928345, 0.05922079458832741, 0.06786544620990753, 0.04013598710298538, 0.06134285032749176, 0.0], [0.18957599997520447, 0.007861902937293053, 0.010476204566657543, 0.012908213771879673, 0.010497426614165306, 0.10349422693252563, 0.07343607395887375, 0.42990538477897644, 0.024574777111411095, 0.1372697949409485]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8735915422439575, 0.1264083981513977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40111470222473145, 0.510547935962677, 0.08833732455968857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0276184044778347, 0.05665257200598717, 0.8718301653862, 0.043898846954107285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07733336091041565, 0.365313321352005, 0.11332366615533829, 0.32430577278137207, 0.11972389370203018, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6556440591812134, 0.06300178915262222, 0.05083248019218445, 0.07888036221265793, 0.05136635899543762, 0.10027500241994858, 0.0, 0.0, 0.0, 0.0], [0.18654294312000275, 0.10325086861848831, 0.06530754268169403, 0.11005346477031708, 0.0751693844795227, 0.19053882360458374, 0.2691369950771332, 0.0, 0.0, 0.0], [0.09667771309614182, 0.04255345091223717, 0.023608172312378883, 0.056543417274951935, 0.04837780073285103, 0.12254248559474945, 0.33910509943962097, 0.2705918550491333, 0.0, 0.0], [0.06725482642650604, 0.07902789115905762, 0.03773766756057739, 0.05301017686724663, 0.029360629618167877, 0.0795680433511734, 0.43945878744125366, 0.13144788146018982, 0.08313415944576263, 0.0], [0.06725265830755234, 0.03356023505330086, 0.027685031294822693, 0.05884738638997078, 0.039368290454149246, 0.08749473839998245, 0.16885916888713837, 0.14700569212436676, 0.16588222980499268, 0.20404455065727234]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8174954056739807, 0.1825045496225357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11346054822206497, 0.8076119422912598, 0.07892753183841705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01706482097506523, 0.20547018945217133, 0.5517478585243225, 0.22571715712547302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01114780642092228, 0.024038558825850487, 0.7872785925865173, 0.09360409528017044, 0.08393097668886185, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007123120129108429, 0.03321105241775513, 0.11185067892074585, 0.11278770864009857, 0.4432483911514282, 0.2917790412902832, 0.0, 0.0, 0.0, 0.0], [0.006628012750297785, 0.01584106683731079, 0.02336236461997032, 0.028282102197408676, 0.07155676931142807, 0.7880682945251465, 0.06626137346029282, 0.0, 0.0, 0.0], [0.000461140793049708, 0.001208329456858337, 0.0010963635286316276, 0.0020610259380191565, 0.004222367890179157, 0.05276842787861824, 0.8986181616783142, 0.039564114063978195, 0.0, 0.0], [0.0044714342802762985, 0.0030466862954199314, 0.02520662173628807, 0.01847776584327221, 0.028084391728043556, 0.12872432172298431, 0.13313446938991547, 0.4756622314453125, 0.18319211900234222, 0.0], [0.0009114157874137163, 0.0009552822448313236, 0.008818530477583408, 0.006785869132727385, 0.011713468469679356, 0.05196184664964676, 0.09358129650354385, 0.21091453731060028, 0.4260741174221039, 0.18828365206718445]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3345664441585541, 0.6654335260391235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21248258650302887, 0.46171849966049194, 0.3257989287376404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2169112116098404, 0.36339521408081055, 0.2873225808143616, 0.1323709934949875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14811809360980988, 0.336478590965271, 0.20350433886051178, 0.07528090476989746, 0.23661811649799347, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08503732085227966, 0.18161188066005707, 0.24255554378032684, 0.1580277383327484, 0.19204945862293243, 0.14071805775165558, 0.0, 0.0, 0.0, 0.0], [0.1245218813419342, 0.11159592121839523, 0.1399824172258377, 0.08270400017499924, 0.17941512167453766, 0.23548920452594757, 0.12629149854183197, 0.0, 0.0, 0.0], [0.02802964486181736, 0.0673457682132721, 0.0904739499092102, 0.048100393265485764, 0.14895549416542053, 0.16570572555065155, 0.4358972907066345, 0.015491745434701443, 0.0, 0.0], [0.08234736323356628, 0.25596991181373596, 0.08787787705659866, 0.08200085908174515, 0.2488078624010086, 0.08278200030326843, 0.04382321611046791, 0.0316946804523468, 0.08469618856906891, 0.0], [0.05206223204731941, 0.0878521278500557, 0.08251845836639404, 0.06066238135099411, 0.09179579466581345, 0.10067927092313766, 0.2289460450410843, 0.04295730218291283, 0.11332008987665176, 0.13920629024505615]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6521741151809692, 0.34782588481903076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47260844707489014, 0.4507277309894562, 0.07666382938623428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4552553594112396, 0.1926717609167099, 0.05873097851872444, 0.2933419942855835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47204846143722534, 0.1402479112148285, 0.06089228019118309, 0.2702024281024933, 0.056608956307172775, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32822537422180176, 0.12461026757955551, 0.04554545134305954, 0.1525512933731079, 0.07841414213180542, 0.27065348625183105, 0.0, 0.0, 0.0, 0.0], [0.28737160563468933, 0.08126499503850937, 0.02019696868956089, 0.060919586569070816, 0.056354962289333344, 0.21469877660274506, 0.27919310331344604, 0.0, 0.0, 0.0], [0.20940184593200684, 0.09117228537797928, 0.03148096054792404, 0.06984076648950577, 0.04402376711368561, 0.08230171352624893, 0.19345219433307648, 0.27832648158073425, 0.0, 0.0], [0.19073542952537537, 0.02470625750720501, 0.004702771548181772, 0.009100217372179031, 0.011310771107673645, 0.2618277668952942, 0.20821842551231384, 0.21922342479228973, 0.07017490267753601, 0.0], [0.10458999872207642, 0.018862435594201088, 0.00671273423358798, 0.010519923642277718, 0.01439316850155592, 0.12164115905761719, 0.14099563658237457, 0.2790115475654602, 0.048524852842092514, 0.25474855303764343]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5327021479606628, 0.46729788184165955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.289417028427124, 0.428524374961853, 0.28205856680870056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002700652927160263, 0.00782750267535448, 0.9693534970283508, 0.020118290558457375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2466934621334076, 0.1714750975370407, 0.23301288485527039, 0.12211649864912033, 0.22670210897922516, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25555747747421265, 0.039658550173044205, 0.028971197083592415, 0.020663239061832428, 0.011494031175971031, 0.64365553855896, 0.0, 0.0, 0.0, 0.0], [0.20796456933021545, 0.031587760895490646, 0.02388354018330574, 0.030746985226869583, 0.015544518828392029, 0.20087410509586334, 0.4893985390663147, 0.0, 0.0, 0.0], [0.0770120620727539, 0.010727898217737675, 0.005023231264203787, 0.008057606406509876, 0.001251623616553843, 0.0298862736672163, 0.1240277886390686, 0.7440134882926941, 0.0, 0.0], [0.09852536767721176, 0.08819780498743057, 0.07811671495437622, 0.03327465429902077, 0.26161885261535645, 0.0522458478808403, 0.16732537746429443, 0.09561893343925476, 0.12507642805576324, 0.0], [0.0018261073855683208, 0.0012838452821597457, 0.0008565884782001376, 0.0016095229657366872, 0.004616640508174896, 0.0024215364828705788, 0.0035190077032893896, 0.005768943578004837, 0.942150890827179, 0.03594691678881645]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4359162151813507, 0.5640837550163269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3431054949760437, 0.19373436272144318, 0.46316006779670715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18442286550998688, 0.08524703234434128, 0.24324296414852142, 0.48708710074424744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21567150950431824, 0.13079509139060974, 0.24097095429897308, 0.25489264726638794, 0.1576698124408722, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05023786053061485, 0.0387394018471241, 0.05000477656722069, 0.04733561724424362, 0.03742911294102669, 0.7762531638145447, 0.0, 0.0, 0.0, 0.0], [0.0511484257876873, 0.04080701619386673, 0.06932121515274048, 0.03621925413608551, 0.033142633736133575, 0.16102084517478943, 0.6083406209945679, 0.0, 0.0, 0.0], [0.025111310184001923, 0.03258886933326721, 0.0447932630777359, 0.03725753352046013, 0.021802019327878952, 0.04561688005924225, 0.07765313982963562, 0.7151769399642944, 0.0, 0.0], [0.07588835805654526, 0.05597361922264099, 0.10367069393396378, 0.040440693497657776, 0.04906555637717247, 0.16095595061779022, 0.14304009079933167, 0.3384031355381012, 0.03256191313266754, 0.0], [0.040442321449518204, 0.062470775097608566, 0.12395953387022018, 0.08377139270305634, 0.06365848332643509, 0.07885750383138657, 0.07767990231513977, 0.16915501654148102, 0.04937928915023804, 0.25062575936317444]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8666064143180847, 0.13339360058307648, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7037226557731628, 0.08050073683261871, 0.21577665209770203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5062465667724609, 0.06629224866628647, 0.06981617957353592, 0.35764503479003906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3774057626724243, 0.028806496411561966, 0.14948351681232452, 0.16624045372009277, 0.2780637741088867, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21135659515857697, 0.042256664484739304, 0.1344122439622879, 0.1048489511013031, 0.3652101755142212, 0.14191536605358124, 0.0, 0.0, 0.0, 0.0], [0.10934919118881226, 0.02916078455746174, 0.10378802567720413, 0.075887031853199, 0.5279759168624878, 0.05085793882608414, 0.1029810756444931, 0.0, 0.0, 0.0], [0.07791788876056671, 0.03218723088502884, 0.031317804008722305, 0.05185360088944435, 0.08924022316932678, 0.04645167663693428, 0.1844736784696579, 0.48655787110328674, 0.0, 0.0], [0.14273376762866974, 0.008835562504827976, 0.03521491587162018, 0.022769758477807045, 0.18999792635440826, 0.028453759849071503, 0.034367796033620834, 0.2331274449825287, 0.304499089717865, 0.0], [0.061289604753255844, 0.017167722806334496, 0.07880361378192902, 0.032033663243055344, 0.2612563669681549, 0.018978124484419823, 0.030867211520671844, 0.11583197116851807, 0.2344798445701599, 0.14929187297821045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8212166428565979, 0.1787833869457245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3396759331226349, 0.5219749212265015, 0.13834914565086365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04702942073345184, 0.19365976750850677, 0.6941654682159424, 0.06514535844326019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07624934613704681, 0.044330328702926636, 0.046862393617630005, 0.8197377920150757, 0.012820098549127579, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2521655261516571, 0.07540462911128998, 0.08068394660949707, 0.11423362046480179, 0.16821323335170746, 0.3092989921569824, 0.0, 0.0, 0.0, 0.0], [0.09420902281999588, 0.018169622868299484, 0.02836076356470585, 0.03727712109684944, 0.08318651467561722, 0.5231130123138428, 0.2156839221715927, 0.0, 0.0, 0.0], [0.07451328635215759, 0.020724771544337273, 0.023791687563061714, 0.04119531437754631, 0.053938884288072586, 0.26805758476257324, 0.19976764917373657, 0.3180108368396759, 0.0, 0.0], [0.030594052746891975, 0.012348086573183537, 0.009961707517504692, 0.00667151901870966, 0.00340226199477911, 0.06109628453850746, 0.1632358729839325, 0.6863061785697937, 0.02638396807014942, 0.0], [0.025209803134202957, 0.0054762898944318295, 0.005883367732167244, 0.007687628269195557, 0.01425018347799778, 0.1282559633255005, 0.13382911682128906, 0.30831378698349, 0.15664087235927582, 0.21445296704769135]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9453611969947815, 0.05463882163167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7012258172035217, 0.16970069706439972, 0.12907344102859497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7403305172920227, 0.11875198036432266, 0.036210641264915466, 0.10470689833164215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12056379020214081, 0.08851046860218048, 0.1505507528781891, 0.541449785232544, 0.09892519563436508, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2859649956226349, 0.06796985119581223, 0.025052232667803764, 0.04154035821557045, 0.1758442372083664, 0.40362831950187683, 0.0, 0.0, 0.0, 0.0], [0.2957458794116974, 0.045471444725990295, 0.023551631718873978, 0.028333501890301704, 0.07833414524793625, 0.24397581815719604, 0.2845876216888428, 0.0, 0.0, 0.0], [0.08486080914735794, 0.0258964691311121, 0.01629001274704933, 0.00898955948650837, 0.014236276037991047, 0.18315380811691284, 0.2371562272310257, 0.42941683530807495, 0.0, 0.0], [0.04092159867286682, 0.04880622401833534, 0.017381006851792336, 0.010911515913903713, 0.2677485942840576, 0.06343573331832886, 0.3065471351146698, 0.12647491693496704, 0.11777326464653015, 0.0], [0.1407722681760788, 0.026756651699543, 0.005762416869401932, 0.009632839821279049, 0.013735930435359478, 0.07092715799808502, 0.10942108929157257, 0.35359933972358704, 0.02133893221616745, 0.2480534315109253]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9203740954399109, 0.0796259343624115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2025885283946991, 0.630609929561615, 0.1668015718460083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002432319801300764, 0.001892964355647564, 0.9721331596374512, 0.023541558533906937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2419130951166153, 0.005510880146175623, 0.17049060761928558, 0.2913922965526581, 0.29069313406944275, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2440098375082016, 0.009351827204227448, 0.017741581425070763, 0.0345558263361454, 0.009871198795735836, 0.6844697594642639, 0.0, 0.0, 0.0, 0.0], [0.14732974767684937, 0.015364319086074829, 0.00456368550658226, 0.011315359733998775, 0.043744541704654694, 0.525348961353302, 0.2523333430290222, 0.0, 0.0, 0.0], [0.07099167257547379, 0.002983165206387639, 0.008780458010733128, 0.010933435522019863, 0.01272235345095396, 0.1314404308795929, 0.1681986302137375, 0.5939497947692871, 0.0, 0.0], [0.034700751304626465, 0.0020962196867913008, 0.0035872471053153276, 0.004411232192069292, 0.33813515305519104, 0.19166672229766846, 0.04075455293059349, 0.20953263342380524, 0.1751154661178589, 0.0], [0.04594593122601509, 0.00358470412902534, 0.003561503952369094, 0.013692683540284634, 0.0227326862514019, 0.04340610280632973, 0.0425075888633728, 0.266926646232605, 0.14452910423278809, 0.4131131172180176]]]}\n","    )\n","    </script>"]},"metadata":{}}],"source":["# YOUR CODE HERE - extract and visualize attention\n","with model.trace(remote=REMOTE) as runner:\n","    with runner.invoke(prompt) as invoker:\n","        attn_patterns = model.transformer.h[0].attn.attn_dropout.input[0][0].save()\n","\n","# Get string tokens (replacing special character for spaces)\n","str_tokens = model.tokenizer.tokenize(prompt)\n","str_tokens = [s.replace('Ġ', ' ') for s in str_tokens]\n","\n","# Attention patterns (squeeze out the batch dimension)\n","attn_patterns_value = attn_patterns.value.squeeze(0)\n","\n","print(\"Layer 0 Head Attention Patterns:\")\n","display(cv.attention.attention_patterns(\n","    tokens=str_tokens,\n","    attention=attn_patterns_value,\n","))"]},{"cell_type":"markdown","id":"9a16b712","metadata":{"id":"9a16b712"},"source":["<details>\n","<summary>Solution (and explanation)</summary>\n","\n","```python\n","with model.trace(remote=REMOTE) as runner:\n","    with runner.invoke(prompt) as invoker:\n","        attn_patterns = model.transformer.h[0].attn.attn_dropout.input[0][0].save()\n","\n","# Get string tokens (replacing special character for spaces)\n","str_tokens = model.tokenizer.tokenize(prompt)\n","str_tokens = [s.replace('Ġ', ' ') for s in str_tokens]\n","\n","# Attention patterns (squeeze out the batch dimension)\n","attn_patterns_value = attn_patterns.value.squeeze(0)\n","\n","print(\"Layer 0 Head Attention Patterns:\")\n","display(cv.attention.attention_patterns(\n","    tokens=str_tokens,\n","    attention=attn_patterns_value,\n","))\n","```\n","\n","Explanation:\n","\n","* Within the context managers:\n","    * We access the attention patterns by taking the input to the `attn_dropout`.\n","        * From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed `nn.Softmax` module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.\n","        * Because of the previously discussed point about dropout not working in inference mode, we could also use the output of `attn_dropout`, and get the same values.\n","    * We use the `.save()` method to save the attention patterns (as an object).\n","* Outside of the context managers:\n","    * We use the `tokenize` method to tokenize the prompt.\n","    * We use the `.value` to access the actual value of the intervention proxy `attn_patterns`.\n","        * This returns a tuple of length-1, so we index into it to get the actual tensor, then squeeze to remove the batch dimension.\n","        \n","</details>\n"]},{"cell_type":"markdown","id":"48aa385f","metadata":{"id":"48aa385f"},"source":["As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using `model.transformer.h[0].attn.q_proj.output` will give you the query vectors, and `k_proj` for the key vectors. However, one thing to be wary of is that GPT-J uses **rotary embeddings**, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See [here](https://blog.eleuther.ai/rotary-embeddings/) for an in-depth discussion of rotary embeddings, and [here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bef36Bf9k7FYsCt1DpzCw6eV) for some rough intuitions."]},{"cell_type":"markdown","id":"886c7964","metadata":{"id":"886c7964"},"source":["# 2️⃣ Task-encoding hidden states"]},{"cell_type":"markdown","id":"4UNdoTCCiU-z","metadata":{"id":"4UNdoTCCiU-z"},"source":["We'll begin with the following question, posed by the Function Vectors paper:\n","\n","> *When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task $T$, do any hidden states encode the task itself?*\n","\n","We'll prove that the answer is yes, by constructing a vector $h$ from a set of ICL prompts for the **antonym task**, and intervening with our vector to make our model produce antonyms on zero-shot prompts.\n","\n","This will require you to learn how to perform causal interventions with `nnsight`, not just save activations.\n","\n","(Note - this section structurally follows section 2.1 of the function vectors paper).\n","\n","\n","> ##### Learning Objectives\n",">\n","> * Understand how `nnsight` can be used to perform causal interventions, and perform some yourself\n","> * Reproduce the \"h-vector results\" from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts\n","\n","<br>\n","\n","---"]},{"cell_type":"markdown","id":"7NLsqpK5rz9C","metadata":{"id":"7NLsqpK5rz9C"},"source":["## ICL Task"]},{"cell_type":"markdown","id":"382f647b","metadata":{"id":"382f647b"},"source":["### Exercise (optional) - generate your own antonym pairs\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴⚪\n","Importance: 🔵🔵⚪⚪⚪\n","\n","If you choose to do this exercise, you should spend up to 10-30 minutes on it - depending on your familiarity with the OpenAI Python API.\n","```\n","\n","We've provided you two options for the antonym dataset you'll use in these exercises.\n","\n","1. Firstly, we've provided you a list of word pairs, in the file `data/antonym_pairs.txt`.\n","2. Secondly, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise).\n","\n","If you just want to use the provided list of words, skip this exercise and run the code below to load in the dataset from the text file. Alternatively, if you want to generate your own dataset, you can fill in the function `generate_dataset` below, which should query GPT-4 and get a list of antonym pairs.\n","\n","See [here](https://platform.openai.com/docs/guides/gpt/chat-completions-api) for a guide to using the chat completions API, if you haven't already used it. Use the two dropdowns below (in order) for some guidance.\n","\n","<details>\n","<summary>Getting started #1</summary>\n","\n","Here is a recommended template:\n","\n","```python\n","response = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": antonym_task},\n","        {\"role\": \"assistant\", \"content\": start_of_response},\n","    ]\n",")\n","```\n","\n","where `antonym_task` explains the antonym task, and `start_of_respose` gives the model a prompt to start from (e.g. \"Sure, here are some antonyms: ...\"), to guide its subsequent behaviour.\n","\n","</details>\n","\n","<details>\n","<summary>Getting started #2</summary>\n","\n","Here is an template you might want to use for the actual request:\n","\n","```python\n","example_antonyms = \"old: young, top: bottom, awake: asleep, future: past, \"\n","\n","response = openai.ChatCompletion.create(\n","    model=\"gpt-4\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": f\"Give me {N} examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym.\"},\n","        {\"role\": \"assistant\", \"content\": f\"Sure! Here are {N} pairs of antonyms satisfying this specification: {example_antonyms}\"},\n","    ]\n",")\n","```\n","\n","where `N` is the function argument. Note that we've provided a few example antonyms, and appended them to the start of GPT4's completion. This is a classic trick to guide the rest of the output (in fact, it's commonly used in adversarial attacks).\n","\n","</details>"]},{"cell_type":"markdown","id":"9fa4613f","metadata":{"id":"9fa4613f"},"source":["Note - it's possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won't worry too much about this. When it comes to testing out our zero-shot intervention, we'll make sure to only use cases where GPT-J can actually solve it."]},{"cell_type":"code","execution_count":null,"id":"34f9069c","metadata":{"id":"34f9069c"},"outputs":[],"source":["openai.api_key = \"insert-your-key-here!\"\n","\n","def generate_antonym_dataset(N: int):\n","    '''\n","    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.\n","    '''\n","    assert openai.api_key != \"insert-your-key-here!\", \"Please insert your own key before running this function!\"\n","\n","    response = openai.ChatCompletion.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": \"Generate 100 pairs of antonyms in the form of a list of 2-tuples. For example, [['old', 'young'], ['top', bottom'],['awake', 'asleep']...].\"},\n","        {\"role\": \"assistant\", \"content\": \"Sure, here is a list of 100 antonyms: \"}])\n","    return response\n","\n","if openai.api_key != \"insert-your-key-here!\":\n","    ANTONYM_PAIRS = generate_antonym_dataset(100)\n","    # Save the word pairs in a text file\n","    with open(section_dir / \"data\" / \"my_antonym_pairs.txt\", \"w\") as f:\n","        for word_pair in ANTONYM_PAIRS:\n","            f.write(f\"{word_pair[0]} {word_pair[1]}\\n\")\n","\n","# Load the word pairs from the text file\n","with open(section_dir / \"data\" / \"antonym_pairs.txt\", \"r\") as f:\n","    ANTONYM_PAIRS = [line.split() for line in f.readlines()]"]},{"cell_type":"code","execution_count":null,"id":"744ab4e4","metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1718090257210,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"},"user_tz":-60},"id":"744ab4e4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"75e8967a-21e2-4977-e530-9ccf85a46810"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['old', 'young'],\n"," ['top', 'bottom'],\n"," ['awake', 'asleep'],\n"," ['future', 'past'],\n"," ['appear', 'disappear'],\n"," ['early', 'late'],\n"," ['empty', 'full'],\n"," ['innocent', 'guilty'],\n"," ['ancient', 'modern'],\n"," ['arrive', 'depart']]"]},"metadata":{},"execution_count":8}],"source":["ANTONYM_PAIRS[:10]"]},{"cell_type":"markdown","id":"fecc199e","metadata":{"id":"fecc199e"},"source":["## ICL Dataset"]},{"cell_type":"markdown","id":"ee713227","metadata":{"id":"ee713227"},"source":["To handle this list of word pairs, we've given you some helpful classes.\n","\n","Firstly, there's the `ICLSequence` class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works."]},{"cell_type":"code","execution_count":null,"id":"f477b99a","metadata":{"executionInfo":{"elapsed":729,"status":"ok","timestamp":1718375662840,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"},"user_tz":-60},"id":"f477b99a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b717a66-2bcf-4600-8ddf-3d6ae6014627"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuple-representation of the sequence:\n","(hot, cold), (yes, no), (in, out), up ->\n","\n","Actual prompt, which will be fed into the model:\n","Q: hot\n","A: cold\n","\n","Q: yes\n","A: no\n","\n","Q: in\n","A: out\n","\n","Q: up\n","A:\n"]}],"source":["class ICLSequence:\n","    '''\n","    Class to store a single antonym sequence.\n","\n","    Uses the default template \"Q: {x}\\nA: {y}\" (with separate pairs split by \"\\n\\n\").\n","    '''\n","    def __init__(self, word_pairs: List[List[str]]):\n","        self.word_pairs = word_pairs\n","        self.x, self.y = zip(*word_pairs)\n","\n","    def __len__(self):\n","        return len(self.word_pairs)\n","\n","    def __getitem__(self, idx: int):\n","        return self.word_pairs[idx]\n","\n","    def prompt(self):\n","        '''Returns the prompt, which contains all but the second element in the last word pair.'''\n","        p = \"\\n\\n\".join([f\"Q: {x}\\nA: {y}\" for x, y in self.word_pairs])\n","        return p[:-len(self.completion())]\n","\n","    def completion(self):\n","        '''Returns the second element in the last word pair (with padded space).'''\n","        return \" \" + self.y[-1]\n","\n","    def __str__(self):\n","        '''Prints a readable string representation of the prompt & completion (indep of template).'''\n","        return f\"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->\".strip(\", \")\n","\n","\n","word_list = [[\"hot\", \"cold\"], [\"yes\", \"no\"], [\"in\", \"out\"], [\"up\", \"down\"]]\n","seq = ICLSequence(word_list)\n","\n","print(\"Tuple-representation of the sequence:\")\n","print(seq)\n","print(\"\\nActual prompt, which will be fed into the model:\")\n","print(seq.prompt())"]},{"cell_type":"markdown","id":"702cb3db","metadata":{"id":"702cb3db"},"source":["Secondly, we have the `ICLDataset` class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset)."]},{"cell_type":"code","execution_count":null,"id":"b0b4eea8","metadata":{"id":"b0b4eea8"},"outputs":[],"source":["class ICLDataset:\n","    '''\n","    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency\n","    between the corrupted and clean datasets.\n","\n","    Inputs:\n","        word_pairs:\n","            list of ICL task, e.g. [[\"old\", \"young\"], [\"top\", \"bottom\"], ...] for the antonym task\n","        size:\n","            number of prompts to generate\n","        n_prepended:\n","            number of antonym pairs before the single-word ICL task\n","        bidirectional:\n","            if True, then we also consider the reversed antonym pairs\n","        corrupted:\n","            if True, then the second word in each pair is replaced with a random word\n","        seed:\n","            random seed, for consistency & reproducibility\n","    '''\n","\n","    def __init__(\n","        self,\n","        word_pairs: List[List[str]],\n","        size: int,\n","        n_prepended: int,\n","        bidirectional: bool = True,\n","        seed: int = 0,\n","        corrupted: bool = False,\n","    ):\n","        assert n_prepended+1 <= len(word_pairs), \"Not enough antonym pairs in dataset to create prompt.\"\n","\n","        self.word_pairs = word_pairs\n","        self.word_list = [word for word_pair in word_pairs for word in word_pair]\n","        self.size = size\n","        self.n_prepended = n_prepended\n","        self.bidirectional = bidirectional\n","        self.corrupted = corrupted\n","        self.seed = seed\n","\n","        self.seqs = []\n","        self.prompts = []\n","        self.completions = []\n","\n","        # Generate the dataset (by choosing random word pairs, and constructing `ICLSequence` objects)\n","        for n in range(size):\n","            np.random.seed(seed + n)\n","            random_pairs = np.random.choice(len(self.word_pairs), n_prepended+1, replace=False)\n","            # Randomize the order of each word pair (x, y). If not bidirectional, we always have x -> y not y -> x\n","            random_orders = np.random.choice([1, -1], n_prepended+1)\n","            if not(bidirectional): random_orders[:] = 1\n","            word_pairs = [self.word_pairs[pair][::order] for pair, order in zip(random_pairs, random_orders)]\n","            # If corrupted, then replace y with a random word in all (x, y) pairs except the last one\n","            if corrupted:\n","                for i in range(len(word_pairs) - 1):\n","                    word_pairs[i][1] = np.random.choice(self.word_list)\n","            seq = ICLSequence(word_pairs)\n","\n","            self.seqs.append(seq)\n","            self.prompts.append(seq.prompt())\n","            self.completions.append(seq.completion())\n","\n","    def create_corrupted_dataset(self):\n","        '''Creates a corrupted version of the dataset (with same random seed).'''\n","        return ICLDataset(self.word_pairs, self.size, self.n_prepended, self.bidirectional, corrupted=True, seed=self.seed)\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, idx: int):\n","        return self.seqs[idx]"]},{"cell_type":"markdown","id":"bdcd4b89","metadata":{"id":"bdcd4b89"},"source":["You can see how this dataset works below. **Note that the correct completions have a prepended space**, because this is how the antonym prompts are structured - the answers are tokenized as `\"A: answer\" -> [\"A\", \":\", \" answer\"]`. Forgetting prepended spaces is a classic mistake when working with transformers!"]},{"cell_type":"code","execution_count":null,"id":"cda6ba22","metadata":{"id":"cda6ba22","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1718374222431,"user_tz":-60,"elapsed":627,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"8d5cfbe4-8a2f-4b5e-f64e-73aa885b4d17"},"outputs":[{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mPrompt                                              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ (right, left), (maximum, minimum), melt ->           │ ' freeze'          │\n","│ (minimum, maximum), (old, new), punishment ->        │ ' reward'          │\n","│ (arrogant, humble), (blunt, sharp), compulsory ->    │ ' voluntary'       │\n","│ (inside, outside), (freeze, melt), full ->           │ ' empty'           │\n","│ (reject, accept), (awake, asleep), dusk ->           │ ' dawn'            │\n","│ (invisible, visible), (punishment, reward), heavy -> │ ' light'           │\n","│ (victory, defeat), (forward, backward), young ->     │ ' old'             │\n","│ (up, down), (compulsory, voluntary), right ->        │ ' wrong'           │\n","│ (open, closed), (domestic, foreign), brave ->        │ ' cowardly'        │\n","│ (under, over), (past, future), increase ->           │ ' decrease'        │\n","└──────────────────────────────────────────────────────┴────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Prompt                                               </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ (right, left), (maximum, minimum), melt -&gt;           │ ' freeze'          │\n","│ (minimum, maximum), (old, new), punishment -&gt;        │ ' reward'          │\n","│ (arrogant, humble), (blunt, sharp), compulsory -&gt;    │ ' voluntary'       │\n","│ (inside, outside), (freeze, melt), full -&gt;           │ ' empty'           │\n","│ (reject, accept), (awake, asleep), dusk -&gt;           │ ' dawn'            │\n","│ (invisible, visible), (punishment, reward), heavy -&gt; │ ' light'           │\n","│ (victory, defeat), (forward, backward), young -&gt;     │ ' old'             │\n","│ (up, down), (compulsory, voluntary), right -&gt;        │ ' wrong'           │\n","│ (open, closed), (domestic, foreign), brave -&gt;        │ ' cowardly'        │\n","│ (under, over), (past, future), increase -&gt;           │ ' decrease'        │\n","└──────────────────────────────────────────────────────┴────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=False)\n","\n","table = Table(\"Prompt\", \"Correct completion\")\n","for seq, completion in zip(dataset.seqs, dataset.completions):\n","    table.add_row(str(seq), repr(completion))\n","\n","rprint(table)"]},{"cell_type":"markdown","id":"8083654d","metadata":{"id":"8083654d"},"source":["Compare this output to what it looks like when `corrupted=True`. Each of the pairs before the last one has their second element replaced with a random one (but the last pair is unchanged)."]},{"cell_type":"code","execution_count":null,"id":"12ba4c23","metadata":{"id":"12ba4c23","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1718374224293,"user_tz":-60,"elapsed":10,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"d32cc9e9-476f-4c91-acee-8c7019f7db90"},"outputs":[{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mPrompt                                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ (right, private), (maximum, destroy), melt ->     │ ' freeze'          │\n","│ (minimum, increase), (old, sharp), punishment ->  │ ' reward'          │\n","│ (arrogant, humble), (blunt, deep), compulsory ->  │ ' voluntary'       │\n","│ (inside, voluntary), (freeze, exterior), full ->  │ ' empty'           │\n","│ (reject, profit), (awake, start), dusk ->         │ ' dawn'            │\n","│ (invisible, birth), (punishment, spend), heavy -> │ ' light'           │\n","│ (victory, rich), (forward, honest), young ->      │ ' old'             │\n","│ (up, lie), (compulsory, short), right ->          │ ' wrong'           │\n","│ (open, soft), (domestic, anxious), brave ->       │ ' cowardly'        │\n","│ (under, melt), (past, young), increase ->         │ ' decrease'        │\n","└───────────────────────────────────────────────────┴────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Prompt                                            </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ (right, private), (maximum, destroy), melt -&gt;     │ ' freeze'          │\n","│ (minimum, increase), (old, sharp), punishment -&gt;  │ ' reward'          │\n","│ (arrogant, humble), (blunt, deep), compulsory -&gt;  │ ' voluntary'       │\n","│ (inside, voluntary), (freeze, exterior), full -&gt;  │ ' empty'           │\n","│ (reject, profit), (awake, start), dusk -&gt;         │ ' dawn'            │\n","│ (invisible, birth), (punishment, spend), heavy -&gt; │ ' light'           │\n","│ (victory, rich), (forward, honest), young -&gt;      │ ' old'             │\n","│ (up, lie), (compulsory, short), right -&gt;          │ ' wrong'           │\n","│ (open, soft), (domestic, anxious), brave -&gt;       │ ' cowardly'        │\n","│ (under, melt), (past, young), increase -&gt;         │ ' decrease'        │\n","└───────────────────────────────────────────────────┴────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=True)\n","\n","table = Table(\"Prompt\", \"Correct completion\")\n","for seq, completions in zip(dataset.seqs, dataset.completions):\n","    table.add_row(str(seq), repr(completions))\n","\n","rprint(table)"]},{"cell_type":"markdown","id":"74a8d9e3","metadata":{"id":"74a8d9e3"},"source":["<details>\n","<summary>Aside - the <code>rich</code> library</summary>\n","\n","The `rich` library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It's not necessary for this workshop, but it's a nice little tool to have in your toolbox.\n","\n","The most important function is `rich.print` (usually imported as `rprint`). This can print basic strings, but it also supports the following syntax for printing colors:\n","\n","```python\n","rprint(\"[green]This is green text[/], this is default color\")\n","```\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-1.png\" width=\"350\">\n","\n","and for making text bold / underlined:\n","\n","```python\n","rprint(\"[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].\")\n","```\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-2.png\" width=\"350\">\n","\n","It can also print tables:\n","\n","```python\n","from rich.table import Table\n","\n","table = Table(\"Col1\", \"Col2\", title=\"Title\") # title is optional\n","table.add_row(\"A\", \"a\")\n","table.add_row(\"B\", \"b\")\n","\n","rprint(table)\n","```\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rprint-3.png\" width=\"150\">\n","\n","The text formatting (bold, underlined, colors, etc) is also supported within table cells.\n","\n","</details>"]},{"cell_type":"markdown","id":"VWPUGc9pr_48","metadata":{"id":"VWPUGc9pr_48"},"source":["## Task-encoding vector"]},{"cell_type":"markdown","id":"f63e9504","metadata":{"id":"f63e9504"},"source":["### Exercise - forward pass on antonym dataset\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","You should fill in the `calculate_h` function below. It should:\n","\n","* Run a forward pass on the model with the dataset prompts (i.e. the `.prompts` attribute), using the `nnsight` syntax we've demonstrated previously,\n","* Return a tuple of the model's output (i.e. a list of its string-token completions, one for each prompt in the batch) and the residual stream value at the end of layer `layer` (e.g. if `layer = -1`, this means the final value of the residual stream before we convert into logits).\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-1.png\" width=\"900\">\n","\n","You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last `-1` token (where the model makes the antonym prediction), and same for the completions.\n","\n","<details>\n","<summary> Help - I'm not sure how to run (and index into) a batch of inputs.</summary>\n","\n","If we pass a list of strings to the `generator.invoke` function, this will be tokenized with padding automatically.\n","\n","The type of padding which is applied is **left padding**, meaning if you index at sequence position `-1`, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"id":"21a77af1","metadata":{"executionInfo":{"elapsed":4880,"status":"ok","timestamp":1718375673908,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"},"user_tz":-60},"id":"21a77af1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7e66c3ea-6613-4f0d-eafa-3859b7220800"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 17.9k/17.9k [00:00<00:00, 96.4kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["All tests in `test_calculate_h` passed.\n"]}],"source":["def calculate_h(model: LanguageModel, dataset: ICLDataset, layer: int = -1) -> Tuple[List[str], Tensor]:\n","    '''\n","    Averages over the model's hidden representations on each of the prompts in `dataset` at layer `layer`, to produce\n","    a single vector `h`.\n","\n","    Inputs:\n","        model: LanguageModel\n","            the transformer you're doing this computation with\n","        dataset: ICLDataset\n","            the dataset whose prompts `dataset.prompts` you're extracting the activations from (at the last seq pos)\n","        layer: int\n","            the layer you're extracting activations from\n","\n","    Returns:\n","        completions: List[str]\n","            list of model completion strings (i.e. the strings the model predicts to follow the last token)\n","        h: Tensor\n","            average hidden state tensor at final sequence position, of shape (d_model,)\n","    '''\n","    with model.trace(remote=REMOTE) as runner:\n","      with runner.invoke(dataset.prompts) as invoker:\n","        h = model.transformer.h[layer].output[0][:, -1].mean(dim=0).save()\n","        logit = model.lm_head.output[:, -1]\n","        next_tok_id = logit.argmax(dim=-1).save()\n","\n","    next_tokens = model.tokenizer.batch_decode(next_tok_id.value)\n","    return next_tokens, h.value\n","\n","\n","tests.test_calculate_h(calculate_h, model)"]},{"cell_type":"markdown","id":"0e8b294d","metadata":{"id":"0e8b294d"},"source":["We've provided you with a helper function, which displays the model's output on the antonym dataset (and highlights the examples where the model's prediction is correct). Note, we're using the `repr` function, because a lot of the completions are line breaks, and this helps us see them more clearly!\n","\n","If the antonyms dataset was constructed well, you should find that the model's completion is correct most of the time, and most of its mistakes are either copying (e.g. predicting `wet -> wet` rather than `wet -> dry`) or understandable completions which shouldn't really be considered mistakes (e.g. predicting `right -> left` rather than `right -> wrong`). If we were being rigorous, we'd want to filter this dataset to make sure it only contains examples where the model can correctly perform the task - but for these exercises, we won't worry about this."]},{"cell_type":"code","execution_count":null,"id":"c7e4b849","metadata":{"id":"c7e4b849"},"outputs":[],"source":["def display_model_completions_on_antonyms(\n","    model: LanguageModel,\n","    dataset: ICLDataset,\n","    completions: List[str],\n","    num_to_display: int = 20,\n",") -> None:\n","    table = Table(\"Prompt (tuple representation)\", \"Model's completion\\n(green=correct)\", \"Correct completion\", title=\"Model's antonym completions\")\n","\n","    for i in range(min(len(completions), num_to_display)):\n","\n","        # Get model's completion, and correct completion\n","        completion = completions[i]\n","        correct_completion = dataset.completions[i]\n","        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[0].replace('Ġ', ' ')\n","        seq = dataset.seqs[i]\n","\n","        # Color code the completion based on whether it's correct\n","        is_correct = (completion == correct_completion_first_token)\n","        completion = f\"[b green]{repr(completion)}[/]\" if is_correct else repr(completion)\n","\n","        table.add_row(str(seq), completion, repr(correct_completion))\n","\n","    rprint(table)"]},{"cell_type":"code","execution_count":null,"id":"19f556fb","metadata":{"executionInfo":{"elapsed":4265,"status":"ok","timestamp":1718375680560,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"},"user_tz":-60},"id":"19f556fb","colab":{"base_uri":"https://localhost:8080/","height":464},"outputId":"d038b395-1d83-4705-d8b9-59a7d4c72de7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 18.0k/18.0k [00:00<00:00, 96.7kB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[3m                                    Model's antonym completions                                    \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m                                                       \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n","┃\u001b[1m \u001b[0m\u001b[1mPrompt (tuple representation)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(green=correct)   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ (right, left), (maximum, minimum), melt ->            │ ' melt'            │ ' freeze'          │\n","│ (minimum, maximum), (old, new), punishment ->         │ \u001b[1;32m' reward'\u001b[0m          │ ' reward'          │\n","│ (arrogant, humble), (blunt, sharp), compulsory ->     │ ' optional'        │ ' voluntary'       │\n","│ (inside, outside), (freeze, melt), full ->            │ \u001b[1;32m' empty'\u001b[0m           │ ' empty'           │\n","│ (reject, accept), (awake, asleep), dusk ->            │ \u001b[1;32m' dawn'\u001b[0m            │ ' dawn'            │\n","│ (invisible, visible), (punishment, reward), heavy ->  │ \u001b[1;32m' light'\u001b[0m           │ ' light'           │\n","│ (victory, defeat), (forward, backward), young ->      │ \u001b[1;32m' old'\u001b[0m             │ ' old'             │\n","│ (up, down), (compulsory, voluntary), right ->         │ \u001b[1;32m' wrong'\u001b[0m           │ ' wrong'           │\n","│ (open, closed), (domestic, foreign), brave ->         │ \u001b[1;32m' cowardly'\u001b[0m        │ ' cowardly'        │\n","│ (under, over), (past, future), increase ->            │ \u001b[1;32m' decrease'\u001b[0m        │ ' decrease'        │\n","│ (inside, outside), (melt, freeze), over ->            │ \u001b[1;32m' under'\u001b[0m           │ ' under'           │\n","│ (solid, liquid), (backward, forward), open ->         │ \u001b[1;32m' closed'\u001b[0m          │ ' closed'          │\n","│ (optimist, pessimist), (invisible, visible), brave -> │ \u001b[1;32m' cowardly'\u001b[0m        │ ' cowardly'        │\n","│ (noisy, quiet), (sell, buy), north ->                 │ \u001b[1;32m' south'\u001b[0m           │ ' south'           │\n","│ (guilty, innocent), (birth, death), victory ->        │ \u001b[1;32m' defeat'\u001b[0m          │ ' defeat'          │\n","│ (answer, question), (noisy, quiet), ancient ->        │ \u001b[1;32m' modern'\u001b[0m          │ ' modern'          │\n","│ (on, off), (success, failure), flexible ->            │ \u001b[1;32m' rigid'\u001b[0m           │ ' rigid'           │\n","│ (junior, senior), (arrive, depart), punishment ->     │ \u001b[1;32m' reward'\u001b[0m          │ ' reward'          │\n","│ (loose, tight), (learn, teach), new ->                │ ' new'             │ ' old'             │\n","│ (introduce, remove), (deficiency, quality), wet ->    │ ' wet'             │ ' dry'             │\n","└───────────────────────────────────────────────────────┴────────────────────┴────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                    Model's antonym completions                                    </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">                                                       </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n","┃<span style=\"font-weight: bold\"> Prompt (tuple representation)                         </span>┃<span style=\"font-weight: bold\"> (green=correct)    </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ (right, left), (maximum, minimum), melt -&gt;            │ ' melt'            │ ' freeze'          │\n","│ (minimum, maximum), (old, new), punishment -&gt;         │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' reward'</span>          │ ' reward'          │\n","│ (arrogant, humble), (blunt, sharp), compulsory -&gt;     │ ' optional'        │ ' voluntary'       │\n","│ (inside, outside), (freeze, melt), full -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' empty'</span>           │ ' empty'           │\n","│ (reject, accept), (awake, asleep), dusk -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' dawn'</span>            │ ' dawn'            │\n","│ (invisible, visible), (punishment, reward), heavy -&gt;  │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' light'</span>           │ ' light'           │\n","│ (victory, defeat), (forward, backward), young -&gt;      │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' old'</span>             │ ' old'             │\n","│ (up, down), (compulsory, voluntary), right -&gt;         │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' wrong'</span>           │ ' wrong'           │\n","│ (open, closed), (domestic, foreign), brave -&gt;         │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' cowardly'</span>        │ ' cowardly'        │\n","│ (under, over), (past, future), increase -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' decrease'</span>        │ ' decrease'        │\n","│ (inside, outside), (melt, freeze), over -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' under'</span>           │ ' under'           │\n","│ (solid, liquid), (backward, forward), open -&gt;         │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' closed'</span>          │ ' closed'          │\n","│ (optimist, pessimist), (invisible, visible), brave -&gt; │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' cowardly'</span>        │ ' cowardly'        │\n","│ (noisy, quiet), (sell, buy), north -&gt;                 │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' south'</span>           │ ' south'           │\n","│ (guilty, innocent), (birth, death), victory -&gt;        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' defeat'</span>          │ ' defeat'          │\n","│ (answer, question), (noisy, quiet), ancient -&gt;        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' modern'</span>          │ ' modern'          │\n","│ (on, off), (success, failure), flexible -&gt;            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' rigid'</span>           │ ' rigid'           │\n","│ (junior, senior), (arrive, depart), punishment -&gt;     │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' reward'</span>          │ ' reward'          │\n","│ (loose, tight), (learn, teach), new -&gt;                │ ' new'             │ ' old'             │\n","│ (introduce, remove), (deficiency, quality), wet -&gt;    │ ' wet'             │ ' dry'             │\n","└───────────────────────────────────────────────────────┴────────────────────┴────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["# Get uncorrupted dataset\n","dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)\n","\n","# Getting it from layer 12, cause the graph suggested this was where there was high accuracy\n","model_completions, h = calculate_h(model, dataset, layer=12)\n","\n","# Displaying the output\n","display_model_completions_on_antonyms(model, dataset, model_completions)"]},{"cell_type":"markdown","id":"08da53c0","metadata":{"id":"08da53c0"},"source":["### Exercise - intervene with $h$\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","You should fill in the function `intervene_with_h` below. This will involve:\n","\n","* Run two forward passes (within the same context manager) on a zero-shot dataset:\n","    * One with no intervention (i.e. `h` is unchanged),\n","    * One with an intervention on `h` (i.e. the residual stream value is set to `h`, at the layer which `h` was taken from). This is provided as an input argument.\n","* Return the completions for no intervention and intervention cases respectively (see docstring).\n","\n","The diagram below shows how all of this should work, when combined with the `calculate_h` function.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-2.png\" width=\"950\">\n","\n","Hint - you can use `tokenizer.batch_decode` to turn a list of tokens into a list of strings.\n","\n","<details>\n","<summary>Help - I'm not sure how best to get both the no-intervention and intervention completions.</summary>\n","\n","You can use `with runner.invoke...` more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention & intervention cases respectively.\n","\n","</details>\n","\n","<details>\n","<summary>Help - I'm not sure how to intervene on the hidden state.</summary>\n","\n","First, you can define the tensor of hidden states (i.e. using `.output[0]`, like you've done before).\n","\n","Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. `tensor += h`) or redefining the tensor (i.e. `tensor = tensor + h`); either work.\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"id":"c31eccf6","metadata":{"id":"c31eccf6"},"outputs":[],"source":["def intervene_with_h(\n","    model: LanguageModel,\n","    zero_shot_dataset: ICLDataset,\n","    h: Tensor,\n","    layer: int,\n",") -> Tuple[List[str], List[str]]:\n","    '''\n","    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the\n","    residual stream of a set of generated zero-shot prompts.\n","\n","    Inputs:\n","        model: the model we're using to generate completions\n","        zero_shot_dataset: the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector\n","        h: the `h`-vector we'll be adding to the residual stream\n","        layer: the layer we'll be extracting the `h`-vector from\n","\n","    Returns:\n","        completions_zero_shot: list of string completions for the zero-shot prompts, without intervention\n","        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention\n","    '''\n","\n","    with model.trace(remote=REMOTE) as runner:\n","\n","        # First, run a forward pass where we don't intervene, just save token id completions\n","        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n","            token_completions_zero_shot = model.lm_head.output[:, -1].argmax(dim=-1).save()\n","\n","        # Next, run a forward pass on the zero-shot prompts where we do intervene\n","        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n","            # Add the h-vector to the residual stream, at the last sequence position\n","            hidden_states = model.transformer.h[layer].output[0]\n","            hidden_states[:, -1] += h\n","            # Also save completions\n","            token_completions_intervention = model.lm_head.output[:, -1].argmax(dim=-1).save()\n","\n","    # Decode to get the string tokens\n","    completions_zero_shot = model.tokenizer.batch_decode(token_completions_zero_shot.value)\n","    completions_intervention = model.tokenizer.batch_decode(token_completions_intervention.value)\n","\n","    return completions_zero_shot, completions_intervention\n","\n","# tests.test_intervene_with_h(intervene_with_h, model, h, ANTONYM_PAIRS)\n"]},{"cell_type":"markdown","id":"2317b7eb","metadata":{"id":"2317b7eb"},"source":["Run the code below to calculate completions for the function.\n","\n","**Note, it's very important that we set a different random seed for the zero shot dataset, otherwise we'll be intervening on examples which were actually in the dataset we used to compute $h$!**"]},{"cell_type":"code","execution_count":null,"id":"968379fb","metadata":{"id":"968379fb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718375693804,"user_tz":-60,"elapsed":9736,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"a8d2b54e-147e-428a-eb43-81dadc8c607a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 18.0k/18.0k [00:00<00:00, 96.5kB/s]\n","Downloading result: 100%|██████████| 1.75k/1.75k [00:00<00:00, 10.5MB/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-shot completions:  [' minimum', ' arrogant', ' inside', ' reject', ' invisible', ' victory', ' up', ' open', ' under', ' inside', ' solid', '\\n', ' noisy', ' guilty', ' yes', ' I', ' senior', ' loose', ' introduce', ' innocent']\n","Completions with intervention:  [' maximum', ' arrogant', ' outside', ' reject', ' invisible', ' victory', ' down', ' closed', ' under', ' outside', ' solid', ' optim', ' noisy', ' guilty', ' answer', ' on', ' senior', ' loose', ' introduce', ' guilty']\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["layer = 12\n","dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)\n","zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)\n","\n","# Run previous function to get h-vector\n","h = calculate_h(model, dataset, layer=layer)[1]\n","\n","# Run new function to intervene with h-vector\n","completions_zero_shot, completions_intervention = intervene_with_h(model, zero_shot_dataset, h, layer=layer)\n","\n","print(\"\\nZero-shot completions: \", completions_zero_shot)\n","print(\"Completions with intervention: \", completions_intervention)"]},{"cell_type":"markdown","id":"e54fbd1d","metadata":{"id":"e54fbd1d"},"source":["Next, run the code below to visualise the completions in a table. You should see:\n","\n","* ~0% correct completions on the zero-shot prompt with no intervention, because the model usually just copies the first and only word in the prompt\n","* ~25% correct completions on the zero-shot prompt with intervention"]},{"cell_type":"code","execution_count":null,"id":"ebf13407","metadata":{"id":"ebf13407","colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"status":"ok","timestamp":1718375713633,"user_tz":-60,"elapsed":428,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"c6982cac-9ec3-43cb-d3c9-715e7ab46264"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[3m                          Model's antonym completions                          \u001b[0m\n","┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m              \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n","┃\u001b[1m \u001b[0m\u001b[1mPrompt      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention)    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ minimum ->   │ ' minimum'         │ \u001b[1;32m' maximum'\u001b[0m         │ ' maximum'         │\n","│ arrogant ->  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │\n","│ inside ->    │ ' inside'          │ \u001b[1;32m' outside'\u001b[0m         │ ' outside'         │\n","│ reject ->    │ ' reject'          │ ' reject'          │ ' accept'          │\n","│ invisible -> │ ' invisible'       │ ' invisible'       │ ' visible'         │\n","│ victory ->   │ ' victory'         │ ' victory'         │ ' defeat'          │\n","│ up ->        │ ' up'              │ \u001b[1;32m' down'\u001b[0m            │ ' down'            │\n","│ open ->      │ ' open'            │ \u001b[1;32m' closed'\u001b[0m          │ ' closed'          │\n","│ under ->     │ ' under'           │ ' under'           │ ' over'            │\n","│ inside ->    │ ' inside'          │ \u001b[1;32m' outside'\u001b[0m         │ ' outside'         │\n","│ solid ->     │ ' solid'           │ ' solid'           │ ' liquid'          │\n","│ optimist ->  │ '\\n'               │ ' optim'           │ ' pessimist'       │\n","│ noisy ->     │ ' noisy'           │ ' noisy'           │ ' quiet'           │\n","│ guilty ->    │ ' guilty'          │ ' guilty'          │ ' innocent'        │\n","│ answer ->    │ ' yes'             │ ' answer'          │ ' question'        │\n","│ on ->        │ ' I'               │ ' on'              │ ' off'             │\n","│ junior ->    │ ' senior'          │ \u001b[1;32m' senior'\u001b[0m          │ ' senior'          │\n","│ loose ->     │ ' loose'           │ ' loose'           │ ' tight'           │\n","│ introduce -> │ ' introduce'       │ ' introduce'       │ ' remove'          │\n","│ innocent ->  │ ' innocent'        │ \u001b[1;32m' guilty'\u001b[0m          │ ' guilty'          │\n","└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Model's antonym completions                          </span>\n","┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">              </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n","┃<span style=\"font-weight: bold\"> Prompt       </span>┃<span style=\"font-weight: bold\"> (no intervention)  </span>┃<span style=\"font-weight: bold\"> (intervention)     </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n","┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ minimum -&gt;   │ ' minimum'         │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' maximum'</span>         │ ' maximum'         │\n","│ arrogant -&gt;  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │\n","│ inside -&gt;    │ ' inside'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' outside'</span>         │ ' outside'         │\n","│ reject -&gt;    │ ' reject'          │ ' reject'          │ ' accept'          │\n","│ invisible -&gt; │ ' invisible'       │ ' invisible'       │ ' visible'         │\n","│ victory -&gt;   │ ' victory'         │ ' victory'         │ ' defeat'          │\n","│ up -&gt;        │ ' up'              │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' down'</span>            │ ' down'            │\n","│ open -&gt;      │ ' open'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' closed'</span>          │ ' closed'          │\n","│ under -&gt;     │ ' under'           │ ' under'           │ ' over'            │\n","│ inside -&gt;    │ ' inside'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' outside'</span>         │ ' outside'         │\n","│ solid -&gt;     │ ' solid'           │ ' solid'           │ ' liquid'          │\n","│ optimist -&gt;  │ '\\n'               │ ' optim'           │ ' pessimist'       │\n","│ noisy -&gt;     │ ' noisy'           │ ' noisy'           │ ' quiet'           │\n","│ guilty -&gt;    │ ' guilty'          │ ' guilty'          │ ' innocent'        │\n","│ answer -&gt;    │ ' yes'             │ ' answer'          │ ' question'        │\n","│ on -&gt;        │ ' I'               │ ' on'              │ ' off'             │\n","│ junior -&gt;    │ ' senior'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' senior'</span>          │ ' senior'          │\n","│ loose -&gt;     │ ' loose'           │ ' loose'           │ ' tight'           │\n","│ introduce -&gt; │ ' introduce'       │ ' introduce'       │ ' remove'          │\n","│ innocent -&gt;  │ ' innocent'        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' guilty'</span>          │ ' guilty'          │\n","└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["def display_model_completions_on_h_intervention(\n","    dataset: ICLDataset,\n","    completions: List[str],\n","    completions_intervention: List[str],\n","    num_to_display: int = 20,\n",") -> None:\n","    table = Table(\"Prompt\", \"Model's completion\\n(no intervention)\", \"Model's completion\\n(intervention)\", \"Correct completion\", title=\"Model's antonym completions\")\n","\n","    for i in range(min(len(completions), num_to_display)):\n","\n","        completion_ni = completions[i]\n","        completion_i = completions_intervention[i]\n","        correct_completion = dataset.completions[i]\n","        correct_completion_first_token = tokenizer.tokenize(correct_completion)[0].replace('Ġ', ' ')\n","        seq = dataset.seqs[i]\n","\n","        # Color code the completion based on whether it's correct\n","        is_correct = (completion_i == correct_completion_first_token)\n","        completion_i = f\"[b green]{repr(completion_i)}[/]\" if is_correct else repr(completion_i)\n","\n","        table.add_row(str(seq), repr(completion_ni), completion_i, repr(correct_completion))\n","\n","    rprint(table)\n","\n","\n","display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)"]},{"cell_type":"markdown","id":"fc4af4e1","metadata":{"id":"fc4af4e1"},"source":["### Exercise - combine the last two functions\n","\n","```c\n","Difficulty: 🔴🔴🔴⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-15 minutes on this exercise.\n","```\n","\n","One great feature of the `nnsight` library is its ability to parallelize forward passes and perform complex interventions within a single context manager.\n","\n","In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute $h$ within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same `model.trace` context manager. In other words, **we'll be using `with runner.invoke...` three times** in this context manager.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/h-intervention-3.png\" width=\"1000\">\n","\n","You should fill in the `calculate_h_and_intervene` function below, to do this. Mostly, this should involve combining your `calculate_h` and `intervene_with_h` functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).\n","\n","Your output should be exactly the same as before (since the `ICLDataset` class is deterministic), hence we've not provided test functions in this case - you can just compare the table you get to the one before! However, this time around your code should run twice as fast, because you're batching the operations of \"compute $h$\" and \"intervene with $h$\" together into a single forward pass.\n","\n","<details>\n","<summary>Help - I'm not sure how to use the <code>h</code> vector inside the context manager.</summary>\n","\n","You extract `h` the same way as before, but you don't need to save it, or ever reference its `.value` attribute. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.\n","\n","You shouldn't have to `.save()` anything inside your context manager, other than the token completions.\n","\n","</details>\n","<details>\n","<summary>Help - If I want to add <code>x</code> vector to a slice of my hidden state tensor <code>h</code>, is <code>h[slice]+=x</code> the same as <code>h2 = h[slice], h2 += x</code>?</summary>\n","\n","No, only `h[slice]+=x` does what you want. This is because when doing <code>h2 = h[slice], h2 += x</code>, the modification line <code>h2 += x</code> is no longer modifying the original tensor `h`, but a different tensor`h2`. In contrast, `h[slice]+=x` keeps the original tensor `h` in the modification line.\n","\n","A good rule to keep in mind is: If you're trying to modify a tensor some in-place operation, make sure that tensor is in the actual modification line!\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"id":"194f3025","metadata":{"id":"194f3025"},"outputs":[],"source":["def calculate_h_and_intervene(\n","    model: LanguageModel,\n","    dataset: ICLDataset,\n","    zero_shot_dataset: ICLDataset,\n","    layer: int,\n",") -> Tuple[List[str], List[str]]:\n","    '''\n","    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,\n","    all within the same forward pass. Returns the completions from this intervention.\n","\n","    Inputs:\n","        model: LanguageModel\n","            the model we're using to generate completions\n","        dataset: ICLDataset\n","            the dataset of clean prompts from which we'll extract the `h`-vector\n","        zero_shot_dataset: ICLDataset\n","            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector\n","        layer: int\n","            the layer we'll be extracting the `h`-vector from\n","\n","    Returns:\n","        completions_zero_shot: List[str]\n","            list of string completions for the zero-shot prompts, without intervention\n","        completions_intervention: List[str]\n","            list of string completions for the zero-shot prompts, with h-intervention\n","    '''\n","\n","    with model.trace(remote=REMOTE) as runner:\n","      with runner.invoke(dataset.prompts) as invoker:\n","        h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)\n","\n","      with runner.invoke(zero_shot_dataset.prompts) as invoker:\n","        clean_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()\n","\n","      with runner.invoke(zero_shot_dataset.prompts) as invoker:\n","        hidden = model.transformer.h[layer].output[0]\n","        hidden[:, -1]  += h\n","        intervene_tokens = model.lm_head.output[:, -1].argmax(dim=-1).save()\n","\n","    completions_zero_shot = tokenizer.batch_decode(clean_tokens.value)\n","    completions_intervention = tokenizer.batch_decode(intervene_tokens.value)\n","    return completions_zero_shot, completions_intervention\n"]},{"cell_type":"code","execution_count":null,"id":"700ee3ea","metadata":{"id":"700ee3ea","colab":{"base_uri":"https://localhost:8080/","height":464},"executionInfo":{"status":"ok","timestamp":1718375723444,"user_tz":-60,"elapsed":6701,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"b756df19-ba01-4c87-8688-6bf350a8f048"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 1.75k/1.75k [00:00<00:00, 12.0MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[3m                          Model's antonym completions                          \u001b[0m\n","┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m              \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n","┃\u001b[1m \u001b[0m\u001b[1mPrompt      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention)    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ minimum ->   │ ' minimum'         │ \u001b[1;32m' maximum'\u001b[0m         │ ' maximum'         │\n","│ arrogant ->  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │\n","│ inside ->    │ ' inside'          │ \u001b[1;32m' outside'\u001b[0m         │ ' outside'         │\n","│ reject ->    │ ' reject'          │ ' reject'          │ ' accept'          │\n","│ invisible -> │ ' invisible'       │ ' invisible'       │ ' visible'         │\n","│ victory ->   │ ' victory'         │ ' victory'         │ ' defeat'          │\n","│ up ->        │ ' up'              │ \u001b[1;32m' down'\u001b[0m            │ ' down'            │\n","│ open ->      │ ' open'            │ \u001b[1;32m' closed'\u001b[0m          │ ' closed'          │\n","│ under ->     │ ' under'           │ ' under'           │ ' over'            │\n","│ inside ->    │ ' inside'          │ \u001b[1;32m' outside'\u001b[0m         │ ' outside'         │\n","│ solid ->     │ ' solid'           │ ' solid'           │ ' liquid'          │\n","│ optimist ->  │ '\\n'               │ ' optim'           │ ' pessimist'       │\n","│ noisy ->     │ ' noisy'           │ ' noisy'           │ ' quiet'           │\n","│ guilty ->    │ ' guilty'          │ ' guilty'          │ ' innocent'        │\n","│ answer ->    │ ' yes'             │ ' answer'          │ ' question'        │\n","│ on ->        │ ' I'               │ ' on'              │ ' off'             │\n","│ junior ->    │ ' senior'          │ \u001b[1;32m' senior'\u001b[0m          │ ' senior'          │\n","│ loose ->     │ ' loose'           │ ' loose'           │ ' tight'           │\n","│ introduce -> │ ' introduce'       │ ' introduce'       │ ' remove'          │\n","│ innocent ->  │ ' innocent'        │ \u001b[1;32m' guilty'\u001b[0m          │ ' guilty'          │\n","└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Model's antonym completions                          </span>\n","┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">              </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n","┃<span style=\"font-weight: bold\"> Prompt       </span>┃<span style=\"font-weight: bold\"> (no intervention)  </span>┃<span style=\"font-weight: bold\"> (intervention)     </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n","┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n","│ minimum -&gt;   │ ' minimum'         │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' maximum'</span>         │ ' maximum'         │\n","│ arrogant -&gt;  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │\n","│ inside -&gt;    │ ' inside'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' outside'</span>         │ ' outside'         │\n","│ reject -&gt;    │ ' reject'          │ ' reject'          │ ' accept'          │\n","│ invisible -&gt; │ ' invisible'       │ ' invisible'       │ ' visible'         │\n","│ victory -&gt;   │ ' victory'         │ ' victory'         │ ' defeat'          │\n","│ up -&gt;        │ ' up'              │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' down'</span>            │ ' down'            │\n","│ open -&gt;      │ ' open'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' closed'</span>          │ ' closed'          │\n","│ under -&gt;     │ ' under'           │ ' under'           │ ' over'            │\n","│ inside -&gt;    │ ' inside'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' outside'</span>         │ ' outside'         │\n","│ solid -&gt;     │ ' solid'           │ ' solid'           │ ' liquid'          │\n","│ optimist -&gt;  │ '\\n'               │ ' optim'           │ ' pessimist'       │\n","│ noisy -&gt;     │ ' noisy'           │ ' noisy'           │ ' quiet'           │\n","│ guilty -&gt;    │ ' guilty'          │ ' guilty'          │ ' innocent'        │\n","│ answer -&gt;    │ ' yes'             │ ' answer'          │ ' question'        │\n","│ on -&gt;        │ ' I'               │ ' on'              │ ' off'             │\n","│ junior -&gt;    │ ' senior'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' senior'</span>          │ ' senior'          │\n","│ loose -&gt;     │ ' loose'           │ ' loose'           │ ' tight'           │\n","│ introduce -&gt; │ ' introduce'       │ ' introduce'       │ ' remove'          │\n","│ innocent -&gt;  │ ' innocent'        │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' guilty'</span>          │ ' guilty'          │\n","└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)\n","zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)\n","\n","completions_zero_shot, completions_intervention = calculate_h_and_intervene(model, dataset, zero_shot_dataset, layer=layer)\n","\n","display_model_completions_on_h_intervention(zero_shot_dataset, completions_zero_shot, completions_intervention)"]},{"cell_type":"markdown","id":"f6dd5e39","metadata":{"id":"f6dd5e39"},"source":["### Exercise - compute change in accuracy\n","\n","```c\n","Difficulty: 🔴🔴⚪⚪⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 10-20 minutes on this exercise.\n","```\n","\n","So far, all we've done is look at the most likely completions, and see what fraction of the time these were correct. But our forward pass doesn't just give us token completions, it gives us logits too!\n","\n","You should now rewrite the `calculate_h_and_intervene` function so that, rather than returning two lists of string completions, it returns two lists of floats containing the **logprobs assigned by the model to the correct antonym** in the no intervention / intervention cases respectively.\n","\n","<details>\n","<summary>Help - I don't know how to get the correct logprobs from the logits.</summary>\n","\n","First, apply log softmax to the logits, to get logprobs.\n","\n","Second, you can use `tokenizer(dataset.completions)[\"input_ids\"]` to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you're just picking the first token ID for each completion.)\n","\n","Note - we recommend doing all this inside the context manager, then saving and returning just the correct logprobs not all the logits (this means less to download from the server!).\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"id":"a73a3db9","metadata":{"id":"a73a3db9"},"outputs":[],"source":["def calculate_h_and_intervene_logprobs(\n","    model: LanguageModel,\n","    dataset: ICLDataset,\n","    zero_shot_dataset: ICLDataset,\n","    layer: int,\n",") -> Tuple[List[float], List[float]]:\n","    '''\n","    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,\n","    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.\n","\n","    Inputs:\n","        model: LanguageModel\n","            the model we're using to generate completions\n","        dataset: ICLDataset\n","            the dataset of clean prompts from which we'll extract the `h`-vector\n","        zero_shot_dataset: ICLDataset\n","            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector\n","        layer: int\n","            the layer we'll be extracting the `h`-vector from\n","\n","    Returns:\n","        correct_logprobs: List[float]\n","            list of correct-token logprobs for the zero-shot prompts, without intervention\n","        correct_logprobs_intervention: List[float]\n","            list of correct-token logprobs for the zero-shot prompts, with h-intervention\n","    '''\n","    correct_completion_ids = [toks[0] for toks in tokenizer(zero_shot_dataset.completions)[\"input_ids\"]]\n","\n","    with model.trace(remote=REMOTE) as runner:\n","      with runner.invoke(dataset.prompts) as invoker:\n","        h = model.transformer.h[layer].output[0][:, -1].mean(dim=0)\n","\n","      with runner.invoke(zero_shot_dataset.prompts) as invoker:\n","        clean_logprobs = model.lm_head.output.log_softmax(dim=-1)[list(range(len(correct_completion_ids))), -1, correct_completion_ids].save()\n","\n","      with runner.invoke(zero_shot_dataset.prompts) as invoker:\n","        hidden = model.transformer.h[layer].output[0]\n","        hidden[:, -1]  += h\n","        intervene_logprobs = model.lm_head.output.log_softmax(dim=-1)[list(range(len(correct_completion_ids))), -1, correct_completion_ids].save()\n","\n","\n","    return clean_logprobs.value, intervene_logprobs.value\n"]},{"cell_type":"markdown","id":"2c2e9822","metadata":{"id":"2c2e9822"},"source":["When you run the code below this function, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - **even if the maximum-likelihood token doesn't change, this doesn't mean that the intervention isn't having a significant effect.**"]},{"cell_type":"code","execution_count":null,"id":"d09fdabb","metadata":{"id":"d09fdabb"},"outputs":[],"source":["def display_model_logprobs_on_h_intervention(\n","    dataset: ICLDataset,\n","    correct_logprobs_zero_shot: List[float],\n","    correct_logprobs_intervention: List[float],\n","    num_to_display: int = 20,\n",") -> None:\n","    table = Table(\n","        \"Zero-shot prompt\", \"Model's logprob\\n(no intervention)\", \"Model's logprob\\n(intervention)\", \"Change in logprob\",\n","        title=\"Model's antonym logprobs, with zero-shot h-intervention\\n(green = intervention improves accuracy)\"\n","    )\n","\n","    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):\n","\n","        logprob_ni = correct_logprobs_zero_shot[i]\n","        logprob_i = correct_logprobs_intervention[i]\n","        delta_logprob = logprob_i - logprob_ni\n","        zero_shot_prompt = f\"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}\"\n","\n","        # Color code the logprob based on whether it's increased with this intervention\n","        is_improvement = (delta_logprob >= 0)\n","        delta_logprob = f\"[b green]{delta_logprob:+.2f}[/]\" if is_improvement else f\"{delta_logprob:+.2f}\"\n","\n","        table.add_row(zero_shot_prompt, f\"{logprob_ni:.2f}\", f\"{logprob_i:.2f}\", delta_logprob)\n","\n","    rprint(table)"]},{"cell_type":"code","execution_count":null,"id":"a823d788","metadata":{"id":"a823d788","colab":{"base_uri":"https://localhost:8080/","height":480},"executionInfo":{"status":"ok","timestamp":1718375742921,"user_tz":-60,"elapsed":9020,"user":{"displayName":"Chloe Li","userId":"15868003246433143362"}},"outputId":"983ab73a-ed28-4acc-8641-40fca5c26521"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 1.62k/1.62k [00:00<00:00, 11.1MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[3m              Model's antonym logprobs, with zero-shot h-intervention              \u001b[0m\n","\u001b[3m                     (green = intervention improves accuracy)                      \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m                       \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's logprob  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's logprob\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                   \u001b[0m┃\n","┃\u001b[1m \u001b[0m\u001b[1mZero-shot prompt     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mChange in logprob\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│  minimum -> maximum   │ -2.70             │ -0.62           │ \u001b[1;32m+2.09\u001b[0m             │\n","│ arrogant -> humble    │ -6.21             │ -3.93           │ \u001b[1;32m+2.28\u001b[0m             │\n","│   inside -> outside   │ -3.71             │ -1.02           │ \u001b[1;32m+2.68\u001b[0m             │\n","│   reject -> accept    │ -3.92             │ -1.97           │ \u001b[1;32m+1.96\u001b[0m             │\n","│ invisible -> visible  │ -3.76             │ -2.00           │ \u001b[1;32m+1.76\u001b[0m             │\n","│  victory -> defeat    │ -4.44             │ -2.27           │ \u001b[1;32m+2.16\u001b[0m             │\n","│       up -> down      │ -3.91             │ -1.22           │ \u001b[1;32m+2.69\u001b[0m             │\n","│     open -> closed    │ -5.05             │ -1.40           │ \u001b[1;32m+3.65\u001b[0m             │\n","│    under -> over      │ -4.79             │ -3.43           │ \u001b[1;32m+1.36\u001b[0m             │\n","│   inside -> outside   │ -3.71             │ -1.02           │ \u001b[1;32m+2.68\u001b[0m             │\n","│    solid -> liquid    │ -5.54             │ -3.03           │ \u001b[1;32m+2.51\u001b[0m             │\n","│ optimist -> pessimist │ -6.42             │ -3.38           │ \u001b[1;32m+3.05\u001b[0m             │\n","│    noisy -> quiet     │ -4.22             │ -3.35           │ \u001b[1;32m+0.87\u001b[0m             │\n","│   guilty -> innocent  │ -4.94             │ -2.72           │ \u001b[1;32m+2.23\u001b[0m             │\n","│   answer -> question  │ -5.14             │ -3.95           │ \u001b[1;32m+1.19\u001b[0m             │\n","│       on -> off       │ -7.04             │ -4.37           │ \u001b[1;32m+2.66\u001b[0m             │\n","│   junior -> senior    │ -2.23             │ -1.06           │ \u001b[1;32m+1.17\u001b[0m             │\n","│    loose -> tight     │ -5.53             │ -2.96           │ \u001b[1;32m+2.57\u001b[0m             │\n","│ introduce -> remove   │ -7.47             │ -6.16           │ \u001b[1;32m+1.32\u001b[0m             │\n","│ innocent -> guilty    │ -2.86             │ -1.66           │ \u001b[1;32m+1.20\u001b[0m             │\n","└───────────────────────┴───────────────────┴─────────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Model's antonym logprobs, with zero-shot h-intervention              </span>\n","<span style=\"font-style: italic\">                     (green = intervention improves accuracy)                      </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">                       </span>┃<span style=\"font-weight: bold\"> Model's logprob   </span>┃<span style=\"font-weight: bold\"> Model's logprob </span>┃<span style=\"font-weight: bold\">                   </span>┃\n","┃<span style=\"font-weight: bold\"> Zero-shot prompt      </span>┃<span style=\"font-weight: bold\"> (no intervention) </span>┃<span style=\"font-weight: bold\"> (intervention)  </span>┃<span style=\"font-weight: bold\"> Change in logprob </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│  minimum -&gt; maximum   │ -2.70             │ -0.62           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.09</span>             │\n","│ arrogant -&gt; humble    │ -6.21             │ -3.93           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.28</span>             │\n","│   inside -&gt; outside   │ -3.71             │ -1.02           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.68</span>             │\n","│   reject -&gt; accept    │ -3.92             │ -1.97           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.96</span>             │\n","│ invisible -&gt; visible  │ -3.76             │ -2.00           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.76</span>             │\n","│  victory -&gt; defeat    │ -4.44             │ -2.27           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.16</span>             │\n","│       up -&gt; down      │ -3.91             │ -1.22           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.69</span>             │\n","│     open -&gt; closed    │ -5.05             │ -1.40           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.65</span>             │\n","│    under -&gt; over      │ -4.79             │ -3.43           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.36</span>             │\n","│   inside -&gt; outside   │ -3.71             │ -1.02           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.68</span>             │\n","│    solid -&gt; liquid    │ -5.54             │ -3.03           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.51</span>             │\n","│ optimist -&gt; pessimist │ -6.42             │ -3.38           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.05</span>             │\n","│    noisy -&gt; quiet     │ -4.22             │ -3.35           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+0.87</span>             │\n","│   guilty -&gt; innocent  │ -4.94             │ -2.72           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.23</span>             │\n","│   answer -&gt; question  │ -5.14             │ -3.95           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.19</span>             │\n","│       on -&gt; off       │ -7.04             │ -4.37           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.66</span>             │\n","│   junior -&gt; senior    │ -2.23             │ -1.06           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.17</span>             │\n","│    loose -&gt; tight     │ -5.53             │ -2.96           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.57</span>             │\n","│ introduce -&gt; remove   │ -7.47             │ -6.16           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.32</span>             │\n","│ innocent -&gt; guilty    │ -2.86             │ -1.66           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.20</span>             │\n","└───────────────────────┴───────────────────┴─────────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)\n","zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)\n","\n","correct_logprobs_zero_shot, correct_logprobs_intervention = calculate_h_and_intervene_logprobs(model, dataset, zero_shot_dataset, layer=layer)\n","\n","display_model_logprobs_on_h_intervention(zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention)"]},{"cell_type":"markdown","id":"e99652a3","metadata":{"id":"e99652a3"},"source":["# 3️⃣ Function Vectors"]},{"cell_type":"markdown","id":"1LeHPthEiXLj","metadata":{"id":"1LeHPthEiXLj"},"source":["In this section, we'll replicate the crux of the paper's results, by identifying a set of attention heads whose outputs have a large effect on the model's ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.\n","\n","We'll also learn how to use `nnsight` for multi-token generation, and steer the model's behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you'll be able to steer the model to complete prompts like `\"When you think of Netherlands, you usually think of\"` by talking about Amsterdam.\n","\n","(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).\n","\n","> ##### Learning Objectives\n",">\n","> * Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task\n","> * Understand how to rearrange activations in a model during an `nnsight` forward pass, to extract activations corresponding to a particular attention head\n","> * Learn how to use `nnsight` for multi-token generation\n","\n","<br>\n","\n","---"]},{"cell_type":"markdown","id":"a1a9deb5","metadata":{"id":"a1a9deb5"},"source":["Here, we'll move from thinking about residual stream states to thinking about the **output of specific attention heads.**\n","\n"]},{"cell_type":"markdown","id":"wXRQ6pFtvABD","metadata":{"id":"wXRQ6pFtvABD"},"source":["## Extracting & using FVs"]},{"cell_type":"markdown","id":"X-aL04b3LOrx","metadata":{"id":"X-aL04b3LOrx"},"source":["### A note on `out_proj`\n","\n","First, a bit of a technical complication. Most HuggingFace models don't have the nice attention head representations. What we have is the linear layer `out_proj` which implicitly combines the \"projection per attention head\" and the \"sum over attention head\" operations (if you can't see how this is possible, see the section \"Attention Heads are Independent and Additive\" from Anthropic's [Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html)).\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rearrange-output-2.png\" width=\"950\">\n","\n","This presents some question for us, when it comes to causal interventions on attention heads. Use the dropdowns below to read them answer these questions (they'll be important for the coming exercises).\n","\n","<br>\n","\n","<details>\n","<summary>If we want to do a causal intervention on a particular head, should we intervene on <code>z</code> (the input of <code>out_proj</code>) or on <code>attn_output</code> (the output of <code>out_proj</code>) ?</summary>\n","\n","We should intervene on `z`, because we can just rearrange the `z` tensor of shape `(batch, seq, d_model)` into `(batch, seq, n_heads, d_head)`, in other words separating out all the heads. On the other hand, we can't do this with the `attn_output` because it's *already* summed over heads and we can't separate them out.\n","\n","</details>\n","\n","<br>\n","\n","<details>\n","<summary>How could we get the <code>attn_output</code> vector for a single head, if we had the ability to accss model weights within our context managers?</summary>\n","\n","We can take a slice of the `z` tensor corresponding to a single attention head:\n","\n","```python\n","z.reshape(batch, seq, n_heads, d_head)[:, :, head_idx]\n","```\n","\n","and we can take a slice of the `out_proj` weight matrix corresponding to a single attention head (remember that PyTorch stores linear layers in the shape `(out_feats, in_feats)`):\n","\n","```python\n","out_proj.weight.rearrange(d_model, n_heads, d_head)[:, head_idx]\n","```\n","\n","then finally we can multiply these together.\n","\n","</details>\n","\n","<br>\n","\n","<details>\n","<summary>How could we get the <code>attn_output</code> vector for a single head, if we </b>didn't have</b> the ability to accss model weights within our context managers? (This is currently the case for <code>nnsight</code>, since having access to the weights could allow users to change them!).</summary>\n","\n","We can be a bit clever, and ablate certain heads in the `z` vector before passing it through the output projection:\n","\n","```python\n","# ablate all heads except #2 (using a cloned activation)\n","heads_to_ablate = [0, 1, 3, 4, ...]\n","z_ablated = z.reshape(batch, seq, n_heads, d_head).clone()\n","z_ablated[:, :, heads_to_ablate] = 0\n","\n","# save the output\n","attn_head_output = out_proj(z_ablated).save()\n","```\n","\n","Illustration:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/rearrange-output-ablated-2.png\" width=\"950\">\n","\n","Note - this would actually fail if `out_proj` had a bias, because we want to just get an attention head's output, not the bias term as well. But if you look at the [documentation page](https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html) you'll see that `out_proj` doesn't have a bias term, so we're all good!\n","\n","</details>"]},{"cell_type":"markdown","id":"729112d6","metadata":{"id":"729112d6"},"source":["### Exercise - implement `calculate_fn_vectors_and_intervene`\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴🔴\n","Importance: 🔵🔵🔵🔵🔵\n","\n","You should spend up to 30-60 minutes on this exercise.\n","```\n","\n","This is probably the most important function in today's exercises. Implementing it will be pretty similar to the previous function `calculate_h_and_intervene`, but:\n","\n","* Rather than extracting the value of the residual stream `h` at some particular layer, you'll be extracting the output of the attention heads: iterating over each layer and each head in the model.\n","    * You'll only need to run one clean forward pass to compute all these values, but you'll need to run a separate corrupted forward pass for each head.\n","* Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).\n","    * You can use the method `create_corrupted_dataset` method of the `ICLDataset` class for this.\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/cie-intervention.png\" width=\"1200\">\n","\n","Before you actually start writing the code, it might be helpful to answer the following:\n","\n","<details>\n","<summary>How many different <code>invoker</code> objects will you need in total?</summary>\n","\n","You'll need `(N_LAYERS * N_HEADS) + 2`. To explain:\n","\n","- One for the clean prompts, which you'll extract internal activations from and patch them into corrupted prompts,\n","- One for the corrupted prompts, which you don't intervene on,\n","- One for the corrupted prompts **for every attention head**, which you'll patch into using the clean run activations.\n","\n","</details>\n","\n","<details>\n","<summary>Which proxy outputs (if any) will you need to use <code>.save()</code> on, in this function?</summary>\n","\n","You don't need to `.save()` the function vectors you're extracting from the model's internals, because these will only be used for causal interventions within the context manager.\n","\n","The only thing you need to save is the correct token logprobs for (1) the corrupted forward pass where we don't intervene, and (2) each corrupted forward pass where we do intervene on one of the heads. In other words, you'll need to save `(N_LAYERS * N_HEADS) + 1` tensors in total.\n","\n","</details>\n","\n","A few other notes:\n","\n","* We've added a `layers` argument, so you can iterate through different layers of the model (i.e. running the model with `layers = [3, 4, 5]` will only test the intervention on the attention heads in layers 3, 4 and 5). This is helpful if you're getting memory errors when trying to run all layers at once (remember we have 24 layers, 16 heads per layer, so even with few prompts per head this adds up fast!).\n","    * We've included code for you below showing how you can call the function multiple times, clearing memory between each run, then combine the results.\n","* When it comes to intervening, you can set the value of a reshaped tensor, i.e. `tensor.reshape(*new_shape)[index] = new_value` will change the values in `tensor` without actually reshaping it (for more on this, see the documentation for [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)).\n","* It's good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.\n","* If you're confused about dimensions, use `einops.rearrange` rather than `.reshape` - this is a wonderful tool, it's like using code annotations within your actual code!\n","\n","One last note - **if this function is proving impossible to run for computational reasons, you can skip the exercise and move on to the next ones. They don't rely on this function working.** However, you should definitely at least read & understand the solution."]},{"cell_type":"code","execution_count":null,"id":"fb24d120","metadata":{"id":"fb24d120"},"outputs":[],"source":["def calculate_fn_vectors_and_intervene(\n","    model: LanguageModel,\n","    dataset: ICLDataset,\n","    layers: Optional[List[int]] = None,\n",") -> Float[Tensor, \"layers heads\"]:\n","    '''\n","    Returns a tensor of shape (layers, heads), containing the CIE for each head.\n","\n","    Inputs:\n","        model: LanguageModel\n","            the transformer you're doing this computation with\n","        dataset: ICLDataset\n","            the dataset of clean prompts from which we'll extract the function vector (we'll also create a\n","            corrupted version of this dataset for interventions)\n","        layers: Optional[List[int]]\n","            the layers which this function will calculate the score for (if None, we assume all layers)\n","    '''\n","    layers = range(model.config.n_layer) if (layers is None) else layers\n","    heads = range(model.config.n_head)\n","    n_heads = len(layers) * len(heads)\n","\n","    # Get corrupted dataset\n","    corrupted_dataset = dataset.create_corrupted_dataset()\n","    N = len(dataset)\n","\n","    # Get correct token ids, so we can get correct token logprobs\n","    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)[\"input_ids\"]]\n","\n","    with model.trace(remote=REMOTE) as runner:\n","\n","        # Run a forward pass on clean prompts, where we store attention head outputs\n","        z_dict = {}\n","        with runner.invoke(dataset.prompts) as invoker:\n","            for layer in layers:\n","                # Get hidden states, reshape to get head dimension, store the mean tensor\n","                z = model.transformer.h[layer].attn.out_proj.input[0][0][:, -1]\n","                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)\n","                for head in heads:\n","                    z_dict[(layer, head)] = z_reshaped[head]\n","            # Get correct token logprobs\n","            logits_clean = model.lm_head.output[:, -1]\n","\n","        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can\n","        # get the correct-token logprobs to compare with our intervention)\n","        with runner.invoke(corrupted_dataset.prompts) as invoker:\n","            logits = model.lm_head.output[:, -1]\n","            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[t.arange(N), correct_completion_ids].save()\n","\n","        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes,\n","        # because we're doing different interventions each time)\n","        correct_logprobs_dict = {}\n","        for layer in layers:\n","            for head in heads:\n","                with runner.invoke(corrupted_dataset.prompts) as invoker:\n","                    # Get hidden states, reshape to get head dimension, then set it to the a-vector\n","                    z = model.transformer.h[layer].attn.out_proj.input[0][0][:, -1]\n","                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]\n","                    # Get logprobs at the end, which we'll compare with our corrupted logprobs\n","                    logits = model.lm_head.output[:, -1]\n","                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[t.arange(N), correct_completion_ids].save()\n","\n","    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim\n","    all_correct_logprobs_intervention = einops.rearrange(\n","        t.stack([v.value for v in correct_logprobs_dict.values()]),\n","        \"(layers heads) batch -> layers heads batch\",\n","        layers = len(layers),\n","    )\n","    logprobs_diff = all_correct_logprobs_intervention - correct_logprobs_corrupted.value # shape [layers heads batch]\n","\n","    # Return mean effect of intervention, over the batch dimension\n","    return logprobs_diff.mean(dim=-1)"]},{"cell_type":"markdown","id":"a2957f70","metadata":{"id":"a2957f70"},"source":["As mentioned, the code below calls the function multiple times separately and combines the results.\n","\n","When you run this code & plot the results, you should replicate Figure 3(a) in the Function Vectors paper (more or less). If the code is taking too long to run, we recommend just choosing a single layer to run, which has a distinctive pattern that can be compared to the paper's figure (e.g. layer 8, since head L8H1 has a much higher score than all the other heads in this layer)."]},{"cell_type":"code","execution_count":null,"id":"da3c6a95","metadata":{"id":"da3c6a95","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718091102662,"user_tz":-60,"elapsed":280010,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"4f627c3f-6286-4f21-8ffe-bef6de83f6de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computing layers in range(0, 1) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 26.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.73 seconds.\n","\n","Computing layers in range(1, 2) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 5.75MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.78 seconds.\n","\n","Computing layers in range(2, 3) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 16.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.70 seconds.\n","\n","Computing layers in range(3, 4) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 16.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.46 seconds.\n","\n","Computing layers in range(4, 5) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 24.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 23.03 seconds.\n","\n","Computing layers in range(5, 6) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 26.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.31 seconds.\n","\n","Computing layers in range(6, 7) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 7.88MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.61 seconds.\n","\n","Computing layers in range(7, 8) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 13.9MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.40 seconds.\n","\n","Computing layers in range(8, 9) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 8.64MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.75 seconds.\n","\n","Computing layers in range(9, 10) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 5.68MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.50 seconds.\n","\n","Computing layers in range(10, 11) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 17.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.40 seconds.\n","\n","Computing layers in range(11, 12) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 24.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 23.06 seconds.\n","\n"]}],"source":["dataset = ICLDataset(ANTONYM_PAIRS, size=4, n_prepended=2)\n","\n","def batch_process_layers(n_layers, batch_size):\n","    for i in range(0, n_layers, batch_size):\n","        yield range(n_layers)[i:i + batch_size]\n","\n","results = t.empty((0, N_HEADS), device=device)\n","\n","# If this fails to run, reduce the batch size so the fwd passes are split up more\n","for layers in batch_process_layers(N_LAYERS, batch_size=1):\n","\n","    if layers[0] == 12:\n","        break\n","\n","    print(f\"Computing layers in {layers} ...\")\n","    t0 = time.time()\n","    results = t.concat([results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)])\n","    print(f\"... finished in {time.time()-t0:.2f} seconds.\\n\")"]},{"cell_type":"code","execution_count":null,"id":"e52205c5","metadata":{"id":"e52205c5","colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"status":"ok","timestamp":1718091103855,"user_tz":-60,"elapsed":1195,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"22f2286e-ee63-4c6b-bcc9-bdbe3adc5648"},"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"5010ec24-b948-4ab6-8547-5fc9e4cba52e\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5010ec24-b948-4ab6-8547-5fc9e4cba52e\")) {                    Plotly.newPlot(                        \"5010ec24-b948-4ab6-8547-5fc9e4cba52e\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.0006710887,-0.0015367866,0.0045642853,0.029386759,0.005769968,-0.0045464635,0.0014292002,0.019259572,0.075071216,0.023168266,0.026321828,0.45840698],[-0.0001680851,0.0021122098,0.00055503845,0.004005432,-0.0022922754,-0.00933522,-0.004291177,0.03612566,0.95789814,-0.0126622915,-0.0314067,0.053909898],[-0.002013266,-0.0013784766,0.0034440756,-0.0029075742,-0.0020044446,0.0034734607,0.028127372,0.013636291,-0.060955226,0.08791721,0.0019454956,0.07498342],[-0.000049054623,0.0013535619,-0.00034582615,0.0020794272,0.018448055,0.0030163527,0.024113417,-0.0029663444,0.061067462,0.0054347515,0.034433067,-0.11865121],[-0.00018906593,0.004249513,0.001285553,-0.0069595575,0.018265009,0.002416253,-0.02589494,0.021171331,0.004044175,-0.021964312,-0.004916489,0.035579383],[0.0016909242,0.0009366274,-0.0019062757,-0.00380826,0.00003170967,-0.009887695,0.021523952,-0.0016192198,0.009649217,-0.00058811903,-0.017991245,-0.0051300526],[-0.0019481778,0.00035899878,-0.0011662245,0.011549175,-0.011023343,0.0018339157,0.08104247,0.03248614,-0.0013783574,0.026960611,-0.011238992,-0.03895229],[0.0013661385,0.0015881062,-0.0036014318,-0.0013359785,0.003030479,0.0033884048,0.0011571646,-0.0014570355,0.009240687,0.035497785,0.00017863512,-0.011308134],[0.004199624,0.0000054836273,0.013405442,0.0035325289,0.011097014,0.002689302,-0.0074944496,0.025859058,-0.0023872852,-0.026604116,-0.0051006675,0.0026549697],[-0.000082314014,0.00056505203,-0.01211983,-0.0065453053,0.005961597,-0.010892153,0.007451713,0.004142463,0.027902186,0.017367125,0.009658754,0.028231263],[0.0010209084,0.0005466342,-0.004313886,0.008128762,0.0007120371,0.002200961,-0.014210582,0.010336757,-0.006702423,0.06804931,-0.010445178,-0.022130907],[0.00021785498,-0.00047284365,-0.002280891,-0.022462726,0.0065621734,-0.0063081384,0.012264848,0.008434892,-0.03737545,-0.015684426,0.020868361,0.002006054],[-0.0070158243,-0.0024917722,-0.000031650066,-0.0075387955,0.022064209,-0.0008215904,0.0028398633,0.0052399635,0.009773672,-0.0028823614,-0.0013840795,0.0010800958],[-0.0020431876,-0.0001732111,-0.0035437942,-0.0010606647,-0.000045776367,0.002644658,-0.002434492,-0.027450323,-0.022456706,0.02083242,-0.008196473,0.030573845],[0.0006753206,0.0015137196,0.0025308132,-0.003031075,0.002803564,0.010183632,0.0028062463,0.023720562,0.01984346,0.05746621,0.031682134,0.041905582],[-0.000044226646,0.00035870075,-0.001026094,0.006145358,-0.0017725229,0.0028915405,-0.007125497,-0.0023777485,0.007018149,0.0031710267,-0.006789148,0.008567572]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Layer: %{x}\\u003cbr\\u003eHead: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Average indirect effect of function-vector intervention on antonym task\"},\"height\":600,\"width\":1000},                        {\"staticPlot\": false, \"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('5010ec24-b948-4ab6-8547-5fc9e4cba52e');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}],"source":["imshow(\n","    results.T,\n","    title = \"Average indirect effect of function-vector intervention on antonym task\",\n","    width = 1000,\n","    height = 600,\n","    labels = {\"x\": \"Layer\", \"y\": \"Head\"},\n","    aspect = \"equal\",\n",")"]},{"cell_type":"markdown","id":"77157855","metadata":{"id":"77157855"},"source":["<details>\n","<summary>Use this dropdown to see the figure you should get when doing this replication (very similar to the paper figure, with some small differences)</summary>\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/main-result.png\" width=\"800\">\n","\n","</details>"]},{"cell_type":"code","source":["dataset = ICLDataset(ANTONYM_PAIRS, size=4, n_prepended=2)\n","\n","def batch_process_layers(n_layers, batch_size):\n","    for i in range(0, n_layers, batch_size):\n","        yield range(n_layers)[i:i + batch_size]\n","\n","results = t.empty((0, N_HEADS), device=device)\n","\n","# If this fails to run, reduce the batch size so the fwd passes are split up more\n","for layers in batch_process_layers(N_LAYERS, batch_size=1):\n","\n","    print(f\"Computing layers in {layers} ...\")\n","    t0 = time.time()\n","    results = t.concat([results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)])\n","    print(f\"... finished in {time.time()-t0:.2f} seconds.\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMT7ok3qU6Sz","executionInfo":{"status":"ok","timestamp":1718094004211,"user_tz":-60,"elapsed":635813,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"8fdce972-0fc9-4f85-9641-575c7ec15bc3"},"id":"uMT7ok3qU6Sz","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing layers in range(0, 1) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 19.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 21.98 seconds.\n","\n","Computing layers in range(1, 2) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 25.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.11 seconds.\n","\n","Computing layers in range(2, 3) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 20.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.32 seconds.\n","\n","Computing layers in range(3, 4) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 5.48MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.15 seconds.\n","\n","Computing layers in range(4, 5) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 14.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.18 seconds.\n","\n","Computing layers in range(5, 6) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 25.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.56 seconds.\n","\n","Computing layers in range(6, 7) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 26.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.33 seconds.\n","\n","Computing layers in range(7, 8) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 24.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 21.89 seconds.\n","\n","Computing layers in range(8, 9) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 7.15MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.26 seconds.\n","\n","Computing layers in range(9, 10) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 26.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.11 seconds.\n","\n","Computing layers in range(10, 11) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 5.81MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.51 seconds.\n","\n","Computing layers in range(11, 12) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 18.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.11 seconds.\n","\n","Computing layers in range(12, 13) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 26.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.09 seconds.\n","\n","Computing layers in range(13, 14) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 24.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.16 seconds.\n","\n","Computing layers in range(14, 15) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 8.16MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.06 seconds.\n","\n","Computing layers in range(15, 16) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 18.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.32 seconds.\n","\n","Computing layers in range(16, 17) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 7.68MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.09 seconds.\n","\n","Computing layers in range(17, 18) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 24.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 21.99 seconds.\n","\n","Computing layers in range(18, 19) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 23.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.01 seconds.\n","\n","Computing layers in range(19, 20) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 5.66MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 23.86 seconds.\n","\n","Computing layers in range(20, 21) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 16.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 23.88 seconds.\n","\n","Computing layers in range(21, 22) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 26.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 21.95 seconds.\n","\n","Computing layers in range(22, 23) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 17.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.04 seconds.\n","\n","Computing layers in range(23, 24) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 18.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 23.39 seconds.\n","\n","Computing layers in range(24, 25) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 25.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.32 seconds.\n","\n","Computing layers in range(25, 26) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 27.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.04 seconds.\n","\n","Computing layers in range(26, 27) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 7.62MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 22.06 seconds.\n","\n","Computing layers in range(27, 28) ...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 5.35k/5.35k [00:00<00:00, 24.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["... finished in 24.79 seconds.\n","\n"]}]},{"cell_type":"code","source":["imshow(\n","    results.T,\n","    title = \"Average indirect effect of function-vector intervention on antonym task\",\n","    width = 1000,\n","    height = 600,\n","    labels = {\"x\": \"Layer\", \"y\": \"Head\"},\n","    aspect = \"equal\",\n",")"],"metadata":{"id":"PLLhJp3sYoSt","executionInfo":{"status":"ok","timestamp":1718094042448,"user_tz":-60,"elapsed":622,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"f8660d85-7abf-4988-9610-808d1220224f","colab":{"base_uri":"https://localhost:8080/","height":617}},"id":"PLLhJp3sYoSt","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c8894672-f208-401b-bb69-6dbd401ef274\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c8894672-f208-401b-bb69-6dbd401ef274\")) {                    Plotly.newPlot(                        \"c8894672-f208-401b-bb69-6dbd401ef274\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.0006710887,-0.0015367866,0.0045642853,0.029386759,0.005769968,-0.0045464635,0.0014292002,0.019259572,0.075071216,0.023168266,0.026321828,0.45840698,-0.0013726354,0.0032784939,-0.000018537045,0.0012410283,0.00088483095,-0.0698989,-0.010299623,0.00074607134,-0.021914423,-0.0046564937,-0.0041873455,-0.010363042,0.00063568354,-0.00028079748,0.0055425167,0.0006748438],[-0.0001680851,0.0021122098,0.00055503845,0.004005432,-0.0022922754,-0.00933522,-0.004291177,0.03612566,0.95789814,-0.0126622915,-0.0314067,0.053909898,0.0052500963,0.024037719,-0.15576357,0.012289405,0.0043238997,-0.021273017,0.0049815774,0.002181828,0.0033137798,-0.000023007393,-0.0017665029,0.0034084916,-0.0017822385,0.0008895993,-0.0012987256,0.00057023764],[-0.002013266,-0.0013784766,0.0034440756,-0.0029075742,-0.0020044446,0.0034734607,0.028127372,0.013636291,-0.060955226,0.08791721,0.0019454956,0.07498342,0.020974457,0.044335604,0.0031128526,-0.007025063,-0.00097101927,0.049492657,-0.008499384,-0.0005815029,0.0003720522,0.03377825,-0.005650103,-0.00028848648,-0.002391994,-0.006461382,0.0011726618,0.0021999478],[-0.000049054623,0.0013535619,-0.00034582615,0.0020794272,0.018448055,0.0030163527,0.024113417,-0.0029663444,0.061067462,0.0054347515,0.034433067,-0.11865121,0.0055075884,0.0001591444,0.00091457367,-0.04966104,0.046583056,0.006890714,0.008125544,-0.004432738,0.015599489,-0.0063278675,0.005286336,0.007878184,0.003348291,0.006476283,0.0013387799,0.004728496],[-0.00018906593,0.004249513,0.001285553,-0.0069595575,0.018265009,0.002416253,-0.02589494,0.021171331,0.004044175,-0.021964312,-0.004916489,0.035579383,0.09262359,-0.098452926,0.011289239,0.0031402707,-0.0022295713,0.018020988,0.003650546,-0.006686151,-0.0057966113,-0.0017117262,-0.00049489737,0.008720338,-0.0014511347,-0.0008674264,0.0006942749,-0.00016278028],[0.0016909242,0.0009366274,-0.0019062757,-0.00380826,0.00003170967,-0.009887695,0.021523952,-0.0016192198,0.009649217,-0.00058811903,-0.017991245,-0.0051300526,0.016326308,-0.033284903,-0.042170823,0.3116439,0.0014140606,0.014194846,0.0044076443,0.037439823,-0.0642544,0.027222693,0.012116492,0.0007555485,-0.00922662,0.010295033,-0.0007035732,-0.021918595],[-0.0019481778,0.00035899878,-0.0011662245,0.011549175,-0.011023343,0.0018339157,0.08104247,0.03248614,-0.0013783574,0.026960611,-0.011238992,-0.03895229,-0.028331459,-0.016723812,-0.012837768,-0.0049458146,0.0009033084,-0.0031713843,-0.037189245,0.002964735,0.0025572777,0.004527986,0.0017095208,-0.0010607243,0.12073368,0.006751716,-0.002693355,-0.004093051],[0.0013661385,0.0015881062,-0.0036014318,-0.0013359785,0.003030479,0.0033884048,0.0011571646,-0.0014570355,0.009240687,0.035497785,0.00017863512,-0.011308134,0.012139559,0.12030953,-0.1945895,-0.0015539527,0.018992782,-0.008690357,-0.0036782026,-0.0021626353,-0.0020633936,-0.008326113,0.02282685,0.005314231,0.08255565,-0.0010038018,0.00033915043,-0.01605612],[0.004199624,0.0000054836273,0.013405442,0.0035325289,0.011097014,0.002689302,-0.0074944496,0.025859058,-0.0023872852,-0.026604116,-0.0051006675,0.0026549697,-0.02688086,-0.0037167668,0.00008934736,-0.03888887,-0.0094771385,-0.005290389,-0.0045175552,0.027056992,0.005295098,-0.00012975931,0.004535556,0.029109538,0.0021802783,-0.015536964,0.0037621856,0.00023025274],[-0.000082314014,0.00056505203,-0.01211983,-0.0065453053,0.005961597,-0.010892153,0.007451713,0.004142463,0.027902186,0.017367125,0.009658754,0.028231263,-0.014263034,0.020067453,0.06737882,0.049818873,0.015875459,0.018749058,0.064528525,0.0013535619,0.0029507875,0.0010703206,-0.026109457,0.005456686,-0.0023310184,0.010286987,-0.002881825,-0.0040447116],[0.0010209084,0.0005466342,-0.004313886,0.008128762,0.0007120371,0.002200961,-0.014210582,0.010336757,-0.006702423,0.06804931,-0.010445178,-0.022130907,0.36659783,0.045805454,0.0036346316,-0.0019590259,-0.0033419132,0.020862281,-0.0013694763,0.0028890967,-0.0024058819,-0.0038295984,0.007946312,0.007181287,0.017240942,-0.001314342,-0.005495727,-0.0021131039],[0.00021785498,-0.00047284365,-0.002280891,-0.022462726,0.0065621734,-0.0063081384,0.012264848,0.008434892,-0.03737545,-0.015684426,0.020868361,0.002006054,0.010676444,0.001980245,0.009865463,0.031066895,-0.0069940686,0.029705167,0.020133078,0.00792706,0.019736648,0.0006483793,-0.0056475997,-0.01657021,0.00950259,0.0003966689,-0.008220673,0.0023966432],[-0.0070158243,-0.0024917722,-0.000031650066,-0.0075387955,0.022064209,-0.0008215904,0.0028398633,0.0052399635,0.009773672,-0.0028823614,-0.0013840795,0.0010800958,-0.013313115,0.1707766,0.0059931874,-0.0034251213,-0.0031006336,0.007349074,0.0031976104,-0.0017394423,0.007619679,0.0020967126,-0.010008395,-0.0021089911,0.018827021,0.0007869601,0.0034983754,0.0056283474],[-0.0020431876,-0.0001732111,-0.0035437942,-0.0010606647,-0.000045776367,0.002644658,-0.002434492,-0.027450323,-0.022456706,0.02083242,-0.008196473,0.030573845,0.018335521,-0.015870214,-0.007664919,0.031433344,-0.006906867,-0.019128025,0.0041928887,0.007638216,-0.010746121,0.011443377,-0.00035244226,0.008490145,0.017808437,-0.011129737,-0.0003862977,0.0058701634],[0.0006753206,0.0015137196,0.0025308132,-0.003031075,0.002803564,0.010183632,0.0028062463,0.023720562,0.01984346,0.05746621,0.031682134,0.041905582,0.014473021,0.014947295,-0.00094920397,-0.0014194846,-0.33352274,0.001499474,0.011317849,0.008471727,0.016795933,-0.06887722,-0.0031671524,-0.02857989,0.012739003,-0.0045114756,-0.0048769116,-0.0011677146],[-0.000044226646,0.00035870075,-0.001026094,0.006145358,-0.0017725229,0.0028915405,-0.007125497,-0.0023777485,0.007018149,0.0031710267,-0.006789148,0.008567572,0.11471391,-0.0077580214,-0.0026159286,0.008635104,0.016383648,-0.009791851,0.094096184,0.015006602,-0.0063770413,-0.017473817,-0.0009480715,0.01208967,-0.0034761429,-0.00016212463,0.009706974,-0.00015169382]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Layer: %{x}\\u003cbr\\u003eHead: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Average indirect effect of function-vector intervention on antonym task\"},\"height\":600,\"width\":1000},                        {\"staticPlot\": false, \"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('c8894672-f208-401b-bb69-6dbd401ef274');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","id":"cedb0f13","metadata":{"id":"cedb0f13"},"source":["### Exercise - calculate the function vector\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴🔴\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 25-50 minutes on this exercise.\n","```\n","\n","Your next task is to actually calculate and return the function vector, so we can do a few experiments with it. The function vector is the sum of the outputs of all the attention heads we found using the previous function (i.e. the sum of all of the vectors these heads write to the residual stream), averaged over the prompts in our dataset.\n","\n","There's a difficulty here - rather than just getting the `z` vectors, we're actually trying to get the `attn_out` vectors, but *before* they're summed over heads. As we discussed previously, this is a bit tricky to do for the model we're working with, because the `out_proj` linear map actually does the \"project up\" and \"sum over heads\" operations simultaneously. It would be nice to just take a slice of the `out_proj` matrix and multiply it with a slice of the `z` vector, but the `nnsight` library doesn't yet allow users to access weights directly (for security reasons). To understand how we can extract the `attn_out` vector for a head separately without accessing the underlying weights, you should go back to read the subsection **A note on `out_proj`** at the start of this section."]},{"cell_type":"code","execution_count":null,"id":"09526600","metadata":{"id":"09526600","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718091113937,"user_tz":-60,"elapsed":10084,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"1c5e8b9e-07c6-4953-9ca9-43aa3758d14f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for single head ... \n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 17.6k/17.6k [00:00<00:00, 79.5kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["tests for single head passed.\n","Testing for multiple heads ... \n"]},{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 50.8k/50.8k [00:00<00:00, 114kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["tests for multiple heads passed.\n","\n","All tests in `test_calculate_fn_vector` passed.\n"]}],"source":["def calculate_fn_vector(\n","    model: LanguageModel,\n","    dataset: ICLDataset,\n","    head_list: List[Tuple[int, int]],\n",")  -> Float[Tensor, \"d_model\"]:\n","    '''\n","    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream\n","    by the attention heads in `head_list`, averaged over all inputs in `dataset`.\n","\n","    Inputs:\n","        model: LanguageModel\n","            the transformer you're doing this computation with\n","        dataset: ICLDataset\n","            the dataset of clean prompts from which we'll extract the function vector (we'll also create a\n","            corrupted version of this dataset for interventions)\n","        head_list: List[Tuple[int, int]]\n","            list of attention heads we're calculating the function vector from\n","    '''\n","    # Turn head_list into a dict of {layer: heads we need in this layer}\n","    head_dict = defaultdict(set)\n","    for layer, head in head_list:\n","        head_dict[layer].add(head)\n","\n","    fn_vector_list = []\n","\n","    with model.trace(remote=REMOTE) as runner:\n","\n","        with runner.invoke(dataset.prompts) as invoker:\n","            for layer, head_list in head_dict.items():\n","\n","                # Get the output projection layer\n","                out_proj = model.transformer.h[layer].attn.out_proj\n","\n","                # Get the mean output projection input (note, since this is the mean, so we don't need to\n","                # worry about changing the values of this tensor having downstream effects)\n","                hidden_states = out_proj.input[0][0][:, -1].mean(dim=0)\n","                assert hidden_states.shape == (D_MODEL,)\n","\n","                # Zero-ablate all heads which aren't in our list, then get the output (which\n","                # will be the sum over the heads we actually do want!)\n","                heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]\n","                for head in heads_to_ablate:\n","                    hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0\n","\n","                # Now that we've zeroed all unimportant heads, get the output & add it to the list\n","                # (we need a single batch dimension so we can use `out_proj`)\n","                out_proj_output = out_proj(hidden_states.unsqueeze(0)).squeeze()\n","                fn_vector_list.append(out_proj_output.save())\n","\n","    # We sum all attention head outputs to get our function vector\n","    fn_vector = sum([v.value for v in fn_vector_list])\n","\n","    assert fn_vector.shape == (D_MODEL,)\n","    return fn_vector\n","\n","\n","tests.test_calculate_fn_vector(calculate_fn_vector, model)"]},{"cell_type":"markdown","id":"f5ed78a8","metadata":{"id":"f5ed78a8"},"source":["## Multi-token generation\n","\n","We're now going to replicate some of the results in Table 3, in the paper:\n","\n","<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/tab3.png\" width=\"700\">\n","\n","This will involve doing something we haven't done before - **intervening on multi-token prompt generation**.\n","\n","Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we're trying something different here: we're adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.\n","\n","The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, **and the final sequence position for each subsequent generation.** The reason we do this is to guide the model's behaviour over time. Our hypothesis is that the function vector induces \"next-token antonym behaviour\" (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL prompts).\n","\n","### Using `nnsight` for multi-token generation\n","\n","Previously, our context managers have looked like:\n","\n","```python\n","with model.trace(remote=True) as runner:\n","    with runner.invoke(prompt) as invoker:\n","\n","        # Do stuff to the model's internals\n","```\n","\n","But for multi-token generation, we'll be using the `generate` method rather than `trace`. Our context managers will look like:\n","\n","```python\n","with model.generate(max_new_tokens=max_new_tokens, remote=True) as generator:\n","    with generator.invoke(prompt) as invoker:\n","\n","        for n in range(max_new_tokens):\n","            # Do stuff to the model's internals, on the n-th forward pass\n","            model.next()\n","```\n","\n","The line `model.next()` denotes that the following interventions should be applied to the subsequent generations.\n","\n","Mostly, everything you learned during single-token generation generalizes to the multi-token case. For example, using `.save()` still saves proxies outside the context managers (although make sure that you don't use the same variable names over different generations, otherwise you'll overwrite them - it's easier to store your saved proxies in e.g. a list or dict).\n","\n","\n","### Caching\n","\n","One important concept we've left out of discussions so far is **caching**. To speed up inference, transformer models perform **key-value caching** to speed up text generation. This means that the time taken to generate $n$ tokens is ***much*** less than $n$ times longer than generating a single token. See [this blog post](https://kipp.ly/transformer-inference-arithmetic/) for more on transformer inference arithmetic.\n","\n","When caching takes place, and we're doing causal interventions, we have to be careful that the caching won't override our causal interventions. Sometimes caching has to be disabled to make sure that our causal intervention works correctly. For example, if we wanted to perform the intervention \"add the function vector to *only* the final sequence position of the prompt for each token we generate\" then we'd have to disable caching (since previous forward passes would contain cached values where we intervened on a sequence position which is no longer the final sequence position). However, here we're performing the intervention \"add the function vector to the final token of the original prompt, and to *all subsequent sequence positions*\", meaning enabling caching (the default behaviour) will give us the right causal intervention.\n","\n","(Note - if this seems confusing, please ask a TA or send a message in Slack.)\n","\n","### Generator Output\n","\n","The object `generator.output` is by default a tensor which contains the model's token ID completions (not the logits).\n","\n","By default the `generate` method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don't need to worry about changing this behaviour. But in future exercises we'll experiment with different sampling methods than greedy sampling (which generate uses by default), so `generator.output` and argmaxing over logits will not be identical!"]},{"cell_type":"markdown","id":"0b984451","metadata":{"id":"0b984451"},"source":["### Exercise - intervene with function vector, in multi-token generation\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴⚪\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 15-30 minutes on this exercise.\n","```\n","\n","You should now fill in the function `intervene_with_fn_vector` below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model's string completion on the given prompt template.\n","\n","We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym."]},{"cell_type":"code","execution_count":null,"id":"d21cbe74","metadata":{"id":"d21cbe74"},"outputs":[],"source":["def intervene_with_fn_vector(\n","    model: LanguageModel,\n","    word: str,\n","    layer: int,\n","    fn_vector: Float[Tensor, \"d_model\"],\n","    prompt_template = 'The word \"{x}\" means',\n","    n_tokens: int = 5,\n",") -> Tuple[str, str]:\n","    '''\n","    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.\n","\n","    Inputs:\n","        model: LanguageModel\n","            the transformer you're doing this computation with\n","        word: str\n","            The word which is substituted into the prompt template, via prompt_template.format(x=word)\n","        layer: int\n","            The layer we'll make the intervention (by adding the function vector)\n","        fn_vector: Float[Tensor, \"d_model\"]\n","            The vector we'll add to the final sequence position for each new token to be generated\n","        prompt_template:\n","            The template of the prompt we'll use to produce completions\n","        n_tokens: int\n","            The number of additional tokens we'll generate for our unsteered / steered completions\n","\n","    Returns:\n","        completion: str\n","            The full completion (including original prompt) for the no-intervention case\n","        completion_intervention: str\n","            The full completion (including original prompt) for the intervention case\n","    '''\n","    prompt_template.format(x=word)\n","\n","    with model.generate(remote=REMOTE, max_new_tokens=n_tokens) as generator:\n","      with generator.invoke(prompt_template) as invoker:\n","        for n in range(n_tokens-1):\n","          model.next()\n","        tokens = model.generator.output.save()\n","\n","      with generator.invoke(prompt_template) as invoker:\n","        for n in range(n_tokens):\n","          h = model.transformer.h[layer].output[0]\n","          h[:, -1] += fn_vector\n","          model.next()\n","        tokens_intervention = model.generator.output.save()\n","\n","    completion = tokenizer.batch_decode(tokens.value)\n","    completion_intervention = tokenizer.batch_decode(tokens_intervention.value)\n","    return completion,completion_intervention\n"]},{"cell_type":"markdown","id":"b9322f62","metadata":{"id":"b9322f62"},"source":["To test your function, run the code below. You should find that the first completion seems normal, but the second completion defines a word as its antonym. If this works, congratulations - **you've just successfully induced an OOD behavioural change in a 6b-parameter model!**"]},{"cell_type":"code","source":["# Remove \"light\" from our pairs, so it can be a holdout\n","word = \"light\"\n","_ANTONYM_PAIRS = [pair for pair in ANTONYM_PAIRS if word not in pair]\n","\n","# Define our dataset, and the attention heads we'll use\n","dataset = ICLDataset(_ANTONYM_PAIRS, size=20, n_prepended=5)\n","head_list = [(8, 0), (8, 1), (9, 14), (11, 0), (12, 10), (13, 12), (13, 13), (14, 9), (15, 5), (16, 14)]\n","\n","# Extract the function vector\n","fn_vector = calculate_fn_vector(model, dataset, head_list)\n"],"metadata":{"id":"KaxFikgOa6bH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718091122154,"user_tz":-60,"elapsed":8231,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"3e552b16-d58d-439c-ab03-4c712ee9dfde"},"id":"KaxFikgOa6bH","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 134k/134k [00:00<00:00, 200kB/s]\n"]}]},{"cell_type":"code","execution_count":null,"id":"yI0CM30XGFXv","metadata":{"id":"yI0CM30XGFXv","colab":{"base_uri":"https://localhost:8080/","height":151},"executionInfo":{"status":"ok","timestamp":1718091133127,"user_tz":-60,"elapsed":10976,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"b6b62b43-2c96-4a36-8637-b98fb47b43df"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 2.14k/2.14k [00:00<00:00, 13.4MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mNo intervention                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mintervention                                          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ ['The word \"{x}\" means \"{y}\" and \"{z}\".\\',\\n           │ ['The word \"{x}\" means opposite\".\\n\\nA:\\n\\nThe word    │\n","│ ']                                                     │ \"opposite\" is not a word in English. \\nThe word        │\n","│                                                        │ \"opposite\" is a word in Latin. \\nThe word \"']          │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> No intervention                                        </span>┃<span style=\"font-weight: bold\"> intervention                                           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ ['The word \"{x}\" means \"{y}\" and \"{z}\".\\',\\n           │ ['The word \"{x}\" means opposite\".\\n\\nA:\\n\\nThe word    │\n","│ ']                                                     │ \"opposite\" is not a word in English. \\nThe word        │\n","│                                                        │ \"opposite\" is a word in Latin. \\nThe word \"']          │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["# Intervene with the function vector\n","completion, completion_intervention = intervene_with_fn_vector(\n","    model,\n","    word = word,\n","    layer = 9,\n","    fn_vector = fn_vector,\n","    prompt_template = 'The word \"{x}\" means',\n","    n_tokens = 40,\n",")\n","\n","table = Table(\"No intervention\", \"intervention\")\n","table.add_row(repr(completion), repr(completion_intervention))\n","rprint(table)"]},{"cell_type":"markdown","source":["### Optional questions - multi-token generation with NNsight\n","\n","Here are a few quick optional questions to test your understanding of how multi-generation works with NNsight. These are non-essential, and only mentioned here as potentially helpful pointers.  \n","\n","\n","<details>\n","<summary>How do I add vector <code>h</code> to all the tokens in the original prompt but not to the generated tokens? </summary>\n","\n","```python\n","with model.generate(max_new_tokens=max_new_tokens, remote=True) as generator:\n","    with generator.invoke(prompt) as invoker:\n","        # Add vectors to the model's internals on the first forward pass\n","        model.transformer.h[layer].output[0][:, :seq_len] += h\n","\n","```\n","You don't have to call `model.next()` because you're only adding the vector once to tokens in the original prompt. This will be cached when the model is subsequently generating tokens.\n","\n","</details>\n","\n","<details>\n","<summary>How do I intervene with vector <code>h</code> during the generation of the first k generated tokens? </summary>\n","\n","To intervene during the generation of the first `k` generated tokens:\n","```python\n","with model.generate(max_new_tokens=max_new_tokens, remote=True) as generator:\n","    with generator.invoke(prompt) as invoker:\n","\n","        for n in range(k+1):\n","            # Add vector to the model's internals, on the k-th forward pass\n","            model.transformer.h[layer].output[0] += h\n","            model.next()\n","```\n","When `n=0`, you are adding to tokens in the original prompt before a new token is a generated. After calling `model.next()`, you are accessing the hidden state of the last token that was generated (with seq_len=1).\n","\n","</details>\n","\n","</details>\n","\n","<details>\n","<summary>How do I intervene with vector <code>h</code> only during the generation of the first k tokens, but not to tokens in the original prompt before the first generated token? </summary>\n","\n","```python\n","with model.generate(max_new_tokens=max_new_tokens, remote=True) as generator:\n","    with generator.invoke(prompt) as invoker:\n","\n","        for n in range(k+1):\n","            model.next()\n","            # Add vector AFTER calling model.next() to add to the token that just got generated\n","            model.transformer.h[layer].output[0] += h\n","\n","```\n","By not adding things before `model.next()`, we never add to the original prompt but always after a new token has been generated.\n","\n","</details>\n","\n","</details>\n","\n","<details>\n","<summary>What is the difference between adding vector <code>h</code> before and after vector <code>model.next()</code>? </summary>\n","\n","As explained in Q3, adding vector before `model.next()` means the operation is always done to the current sequence **before** a new generated token is appended. Adding vector after `model.next()` means the operation is always done to the newly generated token.\n","\n","</details>\n"],"metadata":{"id":"JsDtEesh7ln_"},"id":"JsDtEesh7ln_"},{"cell_type":"markdown","id":"OTSFzIAvF2U5","metadata":{"id":"OTSFzIAvF2U5"},"source":["### Exercise - generalize results to another task (optional)\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴⚪\n","Importance: 🔵🔵🔵⚪⚪\n","\n","You should spend up to 15-30 minutes on this exercise.\n","```\n","\n","In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).\n","\n","We'll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you'd like some guidance you can use the dropdown below.\n","\n","<details>\n","<summary>Guidance for exercise</summary>\n","\n","Whatever your task, you'll want to generate a new set of words. You can repurpose the `generate_dataset` function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating & using an OpenAI api key, if you haven't already), or you can just find an appropriate dataset online.\n","\n","When you define the `ICLDataset`, you might want to use `bidirectional=False`, if your task isn't symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.\n","\n","You'll need to supply a new prompt template for the `intervene_with_fn_vector` function, but otherwise most of your code should stay the same.\n","\n","</details>\n"]},{"cell_type":"code","execution_count":null,"id":"Ma_hz8G_Hd5X","metadata":{"id":"Ma_hz8G_Hd5X"},"outputs":[],"source":["# YOUR CODE HERE - generalize to another task"]},{"cell_type":"markdown","id":"ddc7a4fe","metadata":{"id":"ddc7a4fe"},"source":["# 4️⃣ Steering Vectors in GPT2-XL"]},{"cell_type":"markdown","id":"BqA1xq5YieY6","metadata":{"id":"BqA1xq5YieY6"},"source":["Here, we discuss a different but related set of research: Alex Turner's work on steering vectors. This also falls under the umbrella of \"interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour\", but it has a different setup, objectives, and approach.\n","\n","> ##### Learning Objectives\n",">\n","> * Understand the goals & main results from Alex Turner et al's work on steering vectors\n","> * Reproduce the changes in behaviour described in their initial post\n","\n","<br>\n","\n","---"]},{"cell_type":"markdown","id":"zhAxp1SUz7rs","metadata":{"id":"zhAxp1SUz7rs"},"source":["## Steering model behaviour\n"]},{"cell_type":"markdown","id":"e17be93f","metadata":{"id":"e17be93f"},"source":["\n","In the final non-bonus exercise of the previous section, we touched on the idea of using function vectors to induce behavioural changes in the model's completions, rather than specifically making it solve zero-shot or corrupted prompts with the right completion. In these next exercises, we'll explore this kind of work in more detail. We'll be primarily using Turner et al's work on [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector).\n","\n","Summary of the way in which this work differs from the function vector work we've done so far:\n","\n","* Function vectors focused on the model performing a particular function (e.g. mapping a word to its opposite), whereas this work focuses on behavioural changes (e.g. completing a prompt which has negative tone in a positive way).\n","* Function vectors work looked at very large models (our exercises used Pythia-7B, the smallest model which was examined in the function vectors paper). This particular steering vectors post focused on the smaller models GPT2-Small (85m) and GPT2-XL (1.5B). We'll be focusing on GPT2-XL.\n","* The second half of our function vectors work identified important attention heads and focused on their outputs, rather than just adding to the residual stream directly. In this steering vector setup, we'll go back to the simpler method of adding directly into the residual stream.\n","\n","Despite these differences, much of the work which was done here overlaps with function vector work, since they both fall into the broader category of *\"finding vectors using forward-pass-based methods (i.e. not with SGD) and using them to intervene on models during forward passes & change the model's output\"*. This description would also include the following:\n","\n","* [Inference-time intervention](https://www.lesswrong.com/posts/kuQfnotjkQA4Kkfou/inference-time-intervention-eliciting-truthful-answers-from), which focuses on inducing the behavioural change of \"making the model tell the truth\". It also looks at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.\n","* [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681), which can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).\n","\n","We'll discuss some of this work more in the bonus section, but for now, let's get on with the exercises!"]},{"cell_type":"markdown","id":"7ec578b9","metadata":{"id":"7ec578b9"},"source":["First, we'll load in GPT2-XL, then we'll replicate some of the examples in the main post."]},{"cell_type":"code","execution_count":null,"id":"3f24b05c","metadata":{"id":"3f24b05c","colab":{"base_uri":"https://localhost:8080/","height":249,"referenced_widgets":["6eeda3815c234c89b20e04d1e0d2e252","c50df945a1c7420abf31660f8456b479","67da4ec85f31455ea85ff777d47a68ed","15069aee049f47638a722c8e008b2d18","a5556689fb2c47c29a72fe2240e430dc","be1237c7038549f691c03f0ef2bb4208","5fbb24555d2a47418f36029845ce29f2","7bd225996ea04f16b2e803775522af2f","f3eb746d1a2647ef8f87971e2f4ec380","91d1563b4d1b41b0b0ba81393ff7a022","7eb6cda0b30c4264b4c219eac9a7b1ad","0185870b55224ca4bbe658a1e4346106","3665bc8b9e584e5294d1ae7e18a2cfec","4fade92a64544803813e2655c5929bd1","82500b5704124674b4e309a29c7d7fb1","be40d8f2a0d14a3e8c50a70db0875bd5","0c310797dde64995bd6674a54d62aa75","be399d10e61544d98fa9bf8502c67e3c","dbcd187a3a7246d5b6e0171997f21c2a","3a2b7294bf5044c8842453d9f90692ef","55d56eabbbfb49f8b9b920a869679c67","ad7b9435c0f44691915b00f459a9e973","c44a494d5e2640338871e6ff2fd6d8fc","bc10a13a2e0148e4aa39340116a365ba","db26a58593144261ade2f2bccb1c5f9a","11fb98ce337f4bc3ab1258aae93bb855","62578c7e3d9344728013c696484d233d","6d8a0379cfbe4de4ae0bce536b7020c5","8b3526b81c7c4fd291be4c4eb7b12152","50d2c96090ec4b0fae42ed50b9179acf","e52919e341a048749554880739725f87","0b7aa0d9993942468ee8f84847d97e9a","30bc6bce9da94929b31dc244fa26eea1","b93c7bf7e126436a92eb15f6b09ab6de","99b65b114e70403eb9312b2e0bc8cc75","944d7eb4b3284be3b93928f181ebd164","ee0e5d00406f4f7aa1d4b980278b1197","79a6bbb549294f85a911a273b714d5af","d89dabcfca7f41e0b5b5454abd988f22","773dda3b246543838ebc464dafe82703","b07e4f754f714acc90cdf6bbd81a2f19","103450e9c16d488aac25795631106604","21207640df0e48948f1c87804a9a7454","c62bff2698fe42ebb2563522d652c8ee","23adc7c94de146e3bb42417977d8d817","1d539feb88314664b88a85f4f8feead3","57d4437f6c884b258892455526c11493","d909c3f92234429c8207d9f665b78495","48139d7169794c6a89320ee18de1a99c","7288d04dc5e945ed94d27abcce46818e","c4902000155d4d8484b4d70371f8a821","9c79b5444f434d22980e60ef01e212a0","93430c1d78834bd0b2fb70223da6c685","d101bd1ad64146f5943b8d756c638321","4e405fb94de64f3ba0556773fd30ff0c"]},"executionInfo":{"status":"ok","timestamp":1718091139089,"user_tz":-60,"elapsed":5971,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"e12f7493-05ea-45ba-cdb8-075e0c6d172d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n","\n","`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eeda3815c234c89b20e04d1e0d2e252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0185870b55224ca4bbe658a1e4346106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44a494d5e2640338871e6ff2fd6d8fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93c7bf7e126436a92eb15f6b09ab6de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23adc7c94de146e3bb42417977d8d817"}},"metadata":{}}],"source":["model = LanguageModel('gpt2-xl')\n","tokenizer = model.tokenizer"]},{"cell_type":"markdown","id":"5a3d619d","metadata":{"id":"5a3d619d"},"source":["### Exercise - replicate the steering vector results\n","\n","```c\n","Difficulty: 🔴🔴🔴🔴🔴\n","Importance: 🔵🔵🔵🔵⚪\n","\n","You should spend up to 30-50 minutes on this exercise.\n","```\n","\n","Replicate the results in the LessWrong post [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fnrefcvnfx3e6sfu); specifically the \"demonstrations of additions that work well\" section.\n","\n","Read the \"How activation additions work\" section of [Steering GPT-2-XL by adding an activation vector](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#How_activation_additions_work) to understand how vectors are extracted and added. We've provided a function template as well as some example code to run; your main job will be to fill in the function. This will be like a hybrid of several previous exercises (with most similarity to the function `calculate_and_intervene_with_h`), although there will be a few methodological differences.\n","\n","This is the last exercise in this set, and hopefully it'll provide an opportunity to draw together all the threads of what you've learned so far!\n","\n","### Caching\n","\n","This is a different kind of causal intervention than we performed in previous sections. Rather than adding a single vector to the final sequence position at each token generation, we're adding a slice of vectors to the first sequence positions of the original prompt (see tables like in [this section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) for an illustration). How do you think this will affect our function? Should we still cache? Should we be using `.generate()` or `.trace()`? If using `.generate()`, do we need to call `model.next()` ?\n","\n","<details>\n","<summary>Click this dropdown for answers to the questions above.</summary>\n","\n","Rather than adding to each final sequence position for every token generated, we just add the vectors once, to the end of the prompt. This means that:\n","\n","- We can still use caching (because the values we cache shouldn't be different in subsequent token generations),\n","- We should be using `.generate()` (because we're doing multi-token generation),\n","- We don't need to call `model.next()` (because we only intervene once, and our intervention will be cached & applied to all subsequent tokens which are generated).\n","\n","Again, if any of this is confusing then please ask a TA or message in the Slack channel.\n","\n","</details>\n","\n","### Padding\n","\n","The [tables](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate) show the activations being added on the left (i.e. the sequences are padded on the right), but by default padding is applied on the left. There are 2 possible ways you can get around this:\n","\n","1. Right-pad the input sequences manually, i.e. use something like `len(tokenizer.tokenize(prompt))` to see how long each of the prompts is, and add copies of `tokenizer.pad_token` to the end of each sequence.\n","2. Don't manually pad the input sequences, instead slice the sequences you add to the original prompt from the right side of the activation addition sequences, rather than from the left side.\n","\n","The solutions use (2), but you can use either of these methods.\n","\n","### Sampling\n","\n","Following the post, we'll use top-p sampling with probability 0.3 to generate our sequences. We'll also use a small frequency penalty to penalize repetition (so the model gets stuck in loops less). If you've done earlier exercises in this section then you might have implemented `freq_penalty` during sampling; this is supported by TransformerLens models, but HuggingFace uses the somewhat similar `repetition_penalty` (default value is 1.0 indicating no penalty, values higher than 1.0 apply a penalty to repeated tokens).\n","\n","We apply these sampling methods by passing keyword arguments into the `generate` method:\n","\n","```python\n","{\n","    \"do_sample\": True, # necessary whenever we're sampling rather than doing greedy decoding\n","    \"top_p\": 0.3,\n","    \"repetition_penalty\": 1.1,\n","}\n","```\n","\n","Note that the sequences are generated stochastically rather than greedily - this means we'll get different results if we input multiple different copies of the same sequence. We've given you the `n_comparisons` argument in the function below, i.e. you should generate this many steered *and* this many unsteered completions.\n","\n","### Other tips / notes\n","\n","We recommend starting with example #9 (the \"talking about weddings\" one). It seems quite robust to the exact conditions of the forward pass, unlike the `Love - Hate` example. You can use any of the template cells we've given you below.\n","\n","We've given you a `use_bos` argument; if this is True then you should append `tokenizer.bos_token` to the start of all the prompts. This is just to be true to the LessWrong post's implementation; it won't change behaviour much and you can probably ignore it and still get good results.\n"]},{"cell_type":"code","execution_count":null,"id":"0f2e2ef3","metadata":{"id":"0f2e2ef3"},"outputs":[],"source":["sampling_kwargs = {\n","    \"do_sample\": True,\n","    \"top_p\": 0.3,\n","    \"repetition_penalty\": 1.1,\n","}\n","\n","def calculate_and_apply_steering_vector(\n","    model: LanguageModel,\n","    prompt: str,\n","    activation_additions: List[Tuple[int, float, str]],\n","    n_tokens: int,\n","    n_comparisons: int = 1,\n","    use_bos: bool = True,\n",") -> Tuple[List[str], List[str]]:\n","    '''\n","    Performs the steering vector experiments described in the LessWrong post.\n","\n","    Args:\n","        model: LanguageModel\n","            the transformer you're doing this computation with\n","        prompt: str\n","            The original prompt, which we'll be doing activation steering on.\n","\n","        activation_additions: List[Tuple[int, float, str]], each tuple contains:\n","            layer - the layer we're applying these steering vectors to\n","            coefficient - the value we're multiplying it by\n","            prompt - the prompt we're inputting\n","            e.g. activation_additions[0] = [6, 5.0, \" Love\"] means we add the \" Love\" vector at layer 6, scaled by 5x\n","\n","        n_tokens: int\n","            Number of tokens which will be generated for each completion\n","\n","        n_comparisons: int\n","            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and\n","            the same number which are steered).\n","\n","    Returns:\n","        unsteered_completions: List[str]\n","            List of length `n_comparisons`, containing all the unsteered completions.\n","\n","        steered_completions: List[str]\n","            List of length `n_comparisons`, containing all the steered completions.\n","    '''\n","    # Add the BOS token manually, if we're including it\n","    if use_bos:\n","        bos = model.tokenizer.bos_token\n","        prompt = bos + prompt\n","        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]\n","\n","\n","    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths & check they're all the same\n","    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)\n","    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]\n","    assert len(set(act_add_seq_lens)) == 1, \"All activation addition prompts must be the same length.\"\n","    assert act_add_seq_lens[0] <= len(tokenizer.tokenize(prompt)), \"All act_add prompts should be shorter than original prompt.\"\n","\n","    # Get the prompts we'll intervene on (unsteered and steered)\n","    steered_prompts = [prompt for _ in range(n_comparisons)]\n","    unsteered_prompts = [prompt for _ in range(n_comparisons)]\n","\n","    with model.generate(max_new_tokens=n_tokens, remote=REMOTE, **sampling_kwargs) as generator:\n","\n","        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations\n","        with generator.invoke(act_add_prompts) as invoker:\n","            # Get all the prompts from the activation additions, and put them in a list\n","            # (note, we slice from the end of the sequence because of left-padding)\n","            act_add_vectors = [\n","                model.transformer.h[layer].output[0][i, -seq_len:]\n","                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))\n","            ]\n","\n","        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)\n","        with generator.invoke(unsteered_prompts) as invoker:\n","            unsteered_out = model.generator.output.save()\n","\n","        # Forward pass on steered prompts (we add in the results from the act_add prompts)\n","        with generator.invoke(steered_prompts) as invoker:\n","            # For each act_add prompt, add the vector to residual stream, at the start of the sequence\n","            for i, (layer, coeff, seq_len) in enumerate(zip(act_add_layers, act_add_coeffs, act_add_seq_lens)):\n","                model.transformer.h[layer].output[0][:, :seq_len] += act_add_vectors[i] * coeff\n","            steered_out = model.generator.output.save()\n","\n","    # Decode steered & unsteered completions (discarding the sequences we only used for extracting activations) & return results\n","    unsteered_completions = tokenizer.batch_decode(unsteered_out[-n_comparisons:])\n","    steered_completions = tokenizer.batch_decode(steered_out[-n_comparisons:])\n","\n","    return unsteered_completions, steered_completions\n","\n","\n"]},{"cell_type":"markdown","id":"hu3KM-Yj0YIi","metadata":{"id":"hu3KM-Yj0YIi"},"source":["To test your function, use any of the following code snippets (as mentioned, we recommend starting with the weddings example, since the results tend to be pretty robust)."]},{"cell_type":"code","execution_count":null,"id":"77d03f41","metadata":{"id":"77d03f41","colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"status":"ok","timestamp":1718091150716,"user_tz":-60,"elapsed":11631,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"112636df-ef89-483c-b191-96c13af601d7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 4.06k/4.06k [00:00<00:00, 20.4MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[3m                                                    Completions                                                    \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mUnsteered                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSteered                                               \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ <|endoftext|>I hate you because you're not me.         │ <|endoftext|>I hate you because I love you.            │\n","│                                                        │                                                        │\n","│ I'm not a man, I'm a woman.                            │ The words are so simple, yet so powerful. The truth is │\n","│                                                        │ that I love you because I love you. And the truth is   │\n","│ I'm not a boy, I'm a girl.                             │ that I love you because I love you.                    │\n","│                                                        │                                                        │\n","│ I'm not a boy, I'm a girl.                             │ And I know that you love me                            │\n","│                                                        │                                                        │\n","│ I'm not a                                              │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ <|endoftext|>I hate you because you're a fucking       │ <|endoftext|>I hate you because I love you.            │\n","│ asshole.                                               │                                                        │\n","│                                                        │ I'm not sure what to say about this song, but I think  │\n","│ That's the message that comes through loud and clear   │ it's pretty good. It's a little sad, a little sadder,  │\n","│ in this video of a man who, according to his Facebook  │ and a lot sadder. The music is just so sad, and        │\n","│ page, is \"a self-proclaimed Christian, conservative,   │                                                        │\n","│ and Republican.\"                                       │                                                        │\n","│                                                        │                                                        │\n","│ The video begins                                       │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ <|endoftext|>I hate you because you're so much better  │ <|endoftext|>I hate you because you're the only person │\n","│ than me.                                               │ I know who can make me laugh.                          │\n","│                                                        │                                                        │\n","│ I hate you because you're so much better than me.      │ You're my best friend, my lover, my lover-to-be, my    │\n","│                                                        │ soulmate, my love. You're the one who makes me feel    │\n","│ I hate you because you're so much better than me.      │ like I'm not                                           │\n","│                                                        │                                                        │\n","│ I hate you because you're so much better than me.      │                                                        │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Completions                                                    </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Unsteered                                              </span>┃<span style=\"font-weight: bold\"> Steered                                                </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ &lt;|endoftext|&gt;I hate you because you're not me.         │ &lt;|endoftext|&gt;I hate you because I love you.            │\n","│                                                        │                                                        │\n","│ I'm not a man, I'm a woman.                            │ The words are so simple, yet so powerful. The truth is │\n","│                                                        │ that I love you because I love you. And the truth is   │\n","│ I'm not a boy, I'm a girl.                             │ that I love you because I love you.                    │\n","│                                                        │                                                        │\n","│ I'm not a boy, I'm a girl.                             │ And I know that you love me                            │\n","│                                                        │                                                        │\n","│ I'm not a                                              │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ &lt;|endoftext|&gt;I hate you because you're a fucking       │ &lt;|endoftext|&gt;I hate you because I love you.            │\n","│ asshole.                                               │                                                        │\n","│                                                        │ I'm not sure what to say about this song, but I think  │\n","│ That's the message that comes through loud and clear   │ it's pretty good. It's a little sad, a little sadder,  │\n","│ in this video of a man who, according to his Facebook  │ and a lot sadder. The music is just so sad, and        │\n","│ page, is \"a self-proclaimed Christian, conservative,   │                                                        │\n","│ and Republican.\"                                       │                                                        │\n","│                                                        │                                                        │\n","│ The video begins                                       │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ &lt;|endoftext|&gt;I hate you because you're so much better  │ &lt;|endoftext|&gt;I hate you because you're the only person │\n","│ than me.                                               │ I know who can make me laugh.                          │\n","│                                                        │                                                        │\n","│ I hate you because you're so much better than me.      │ You're my best friend, my lover, my lover-to-be, my    │\n","│                                                        │ soulmate, my love. You're the one who makes me feel    │\n","│ I hate you because you're so much better than me.      │ like I'm not                                           │\n","│                                                        │                                                        │\n","│ I hate you because you're so much better than me.      │                                                        │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["unsteered_completions, steered_completions = calculate_and_apply_steering_vector(\n","    model,\n","    prompt = \"I hate you because\",\n","    activation_additions = [\n","        (6, +5.0, \"Love \"),\n","        (6, -5.0, \"Hate\"),\n","    ],\n","    n_tokens = 50,\n","    n_comparisons = 3,\n","    use_bos = True,\n",")\n","\n","table = Table(\"Unsteered\", \"Steered\", title=\"Completions\", show_lines=True)\n","for usc, sc in zip(unsteered_completions, steered_completions):\n","    table.add_row(usc, sc)\n","rprint(table)"]},{"cell_type":"code","source":["tokenizer.tokenize(\"I hate you because\")"],"metadata":{"id":"45AUyU7Bk_fL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718091150717,"user_tz":-60,"elapsed":7,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"3d836534-f0d7-4c28-d187-acff5cae560a"},"id":"45AUyU7Bk_fL","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I', 'Ġhate', 'Ġyou', 'Ġbecause']"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","execution_count":null,"id":"FPi1CqvpBg0j","metadata":{"id":"FPi1CqvpBg0j","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"ok","timestamp":1718091159991,"user_tz":-60,"elapsed":9280,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"15592f3f-20bd-48ae-a983-d0de9d07fc7a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 4.18k/4.18k [00:00<00:00, 6.72MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[3m                                                    Completions                                                    \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mUnsteered                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSteered                                               \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ I went up to my friend and said, 'Hey, I think I'm     │ I went up to my friend and said, \"I want to talk about │\n","│ going to go to the bathroom.' He was like, 'Oh, you're │ weddings!\"                                             │\n","│ not going to the bathroom?' And I was like, 'No, I'm   │                                                        │\n","│ just going to go pee.' And he was like                 │ I have a lot of wedding-related posts. I'm a big fan   │\n","│                                                        │ of the wedding blogosphere. I think it's a great time  │\n","│                                                        │ to be a bride and a bridesmaid                         │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ I went up to my friend and said, 'I think I'm going to │ I went up to my friend and said, \"What do you think of │\n","│ be sick.' He said, 'You're going to be fine. You're    │ the wedding?\" I've been to a lot of weddings. It's a   │\n","│ just going to feel a little bit weird.'\"               │ big deal for me.                                       │\n","│                                                        │                                                        │\n","│ She was diagnosed with the virus on Friday.            │ The thing that really got me excited about it was that │\n","│                                                        │ I had never done anything like this before.            │\n","│ \"It's not                                              │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ I went up to my friend and said, 'Hey, I'm going to    │ I went up to my friend and said, \"What's the worst     │\n","│ the mall.' He was like, 'What? You're going to the     │ thing that could happen?\"  It's not even a bad thing.  │\n","│ mall?' And I was like, 'Yeah, I'm going to the mall.'\" │ It's just something that could happen. I don't want to │\n","│                                                        │ be in the wedding planning business. I'm not going to  │\n","│ She added: \"He's a really                              │ say that it's                                          │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Completions                                                    </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Unsteered                                              </span>┃<span style=\"font-weight: bold\"> Steered                                                </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ I went up to my friend and said, 'Hey, I think I'm     │ I went up to my friend and said, \"I want to talk about │\n","│ going to go to the bathroom.' He was like, 'Oh, you're │ weddings!\"                                             │\n","│ not going to the bathroom?' And I was like, 'No, I'm   │                                                        │\n","│ just going to go pee.' And he was like                 │ I have a lot of wedding-related posts. I'm a big fan   │\n","│                                                        │ of the wedding blogosphere. I think it's a great time  │\n","│                                                        │ to be a bride and a bridesmaid                         │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ I went up to my friend and said, 'I think I'm going to │ I went up to my friend and said, \"What do you think of │\n","│ be sick.' He said, 'You're going to be fine. You're    │ the wedding?\" I've been to a lot of weddings. It's a   │\n","│ just going to feel a little bit weird.'\"               │ big deal for me.                                       │\n","│                                                        │                                                        │\n","│ She was diagnosed with the virus on Friday.            │ The thing that really got me excited about it was that │\n","│                                                        │ I had never done anything like this before.            │\n","│ \"It's not                                              │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ I went up to my friend and said, 'Hey, I'm going to    │ I went up to my friend and said, \"What's the worst     │\n","│ the mall.' He was like, 'What? You're going to the     │ thing that could happen?\"  It's not even a bad thing.  │\n","│ mall?' And I was like, 'Yeah, I'm going to the mall.'\" │ It's just something that could happen. I don't want to │\n","│                                                        │ be in the wedding planning business. I'm not going to  │\n","│ She added: \"He's a really                              │ say that it's                                          │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["unsteered_completions, steered_completions = calculate_and_apply_steering_vector(\n","    model,\n","    prompt = \"I went up to my friend and said\",\n","    activation_additions = [\n","        (20, +4.0, \"I talk about weddings constantly  \"),\n","        (20, -4.0, \"I do not talk about weddings constantly\"),\n","    ],\n","    n_tokens = 50,\n","    n_comparisons = 3,\n","    use_bos = False,\n",")\n","\n","table = Table(\"Unsteered\", \"Steered\", title=\"Completions\", show_lines=True)\n","for usc, sc in zip(unsteered_completions, steered_completions):\n","    table.add_row(usc, sc)\n","rprint(table)"]},{"cell_type":"code","execution_count":null,"id":"fBf-lYQxDSTE","metadata":{"id":"fBf-lYQxDSTE","colab":{"base_uri":"https://localhost:8080/","height":629},"executionInfo":{"status":"ok","timestamp":1718091171092,"user_tz":-60,"elapsed":11106,"user":{"displayName":"Callum McDougall","userId":"13944815271305687103"}},"outputId":"cabb61e4-5b10-4a72-c33f-096037a7caa9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading result: 100%|██████████| 4.31k/4.31k [00:00<00:00, 6.24MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[3m                                                    Completions                                                    \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mUnsteered                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSteered                                               \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ To see the eiffel tower, people flock to it,\" he said. │ To see the eiffel tower, people flock to the           │\n","│ \"It's a very beautiful place.\"                         │ Colosseum, but they are not alone. The Colosseum is a  │\n","│                                                        │ public place, and the people come to it because they   │\n","│ He said that he was not surprised by the attacks on    │ want to be part of the city's history. It is a symbol  │\n","│ the monument.                                          │ of the city's importance,                              │\n","│                                                        │                                                        │\n","│ \"The French are very sensitive about their history and │                                                        │\n","│ monuments,\" he said. \"They don't                       │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ To see the eiffel tower, people flock to it. They want │ To see the eiffel tower, people flock to the           │\n","│ to be close to it and to feel its presence. It is a    │ Colosseum.                                             │\n","│ symbol of France's greatness.                          │                                                        │\n","│                                                        │ Rome was a great city.                                 │\n","│ \"The eiffel tower is a monument that represents the    │                                                        │\n","│ strength of France. It is a symbol of our culture,     │ The ancient Greek historian Flavius Josephus wrote     │\n","│                                                        │ that the city was so famous that it had been called    │\n","│                                                        │ \"the greatest of all cities.\"                          │\n","│                                                        │                                                        │\n","│                                                        │ The first Christians were                              │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ To see the eiffel tower, people flock to the Eiffel    │ To see the eiffel tower, people flock to the           │\n","│ Tower.                                                 │ Colosseum.                                             │\n","│                                                        │                                                        │\n","│ It's a place of pilgrimage for many.                   │ The city is full of ancient monuments, including the   │\n","│                                                        │ Colosseum and the Pantheon.                            │\n","│ The Eiffel Tower is one of the most visited landmarks  │                                                        │\n","│ in Paris.                                              │ The city is also famous for its gardens, which are a   │\n","│                                                        │ UNESCO World Heritage site.                            │\n","│ But it's also a place of danger.                       │                                                        │\n","│                                                        │ The area                                               │\n","│ In the summer                                          │                                                        │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Completions                                                    </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Unsteered                                              </span>┃<span style=\"font-weight: bold\"> Steered                                                </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ To see the eiffel tower, people flock to it,\" he said. │ To see the eiffel tower, people flock to the           │\n","│ \"It's a very beautiful place.\"                         │ Colosseum, but they are not alone. The Colosseum is a  │\n","│                                                        │ public place, and the people come to it because they   │\n","│ He said that he was not surprised by the attacks on    │ want to be part of the city's history. It is a symbol  │\n","│ the monument.                                          │ of the city's importance,                              │\n","│                                                        │                                                        │\n","│ \"The French are very sensitive about their history and │                                                        │\n","│ monuments,\" he said. \"They don't                       │                                                        │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ To see the eiffel tower, people flock to it. They want │ To see the eiffel tower, people flock to the           │\n","│ to be close to it and to feel its presence. It is a    │ Colosseum.                                             │\n","│ symbol of France's greatness.                          │                                                        │\n","│                                                        │ Rome was a great city.                                 │\n","│ \"The eiffel tower is a monument that represents the    │                                                        │\n","│ strength of France. It is a symbol of our culture,     │ The ancient Greek historian Flavius Josephus wrote     │\n","│                                                        │ that the city was so famous that it had been called    │\n","│                                                        │ \"the greatest of all cities.\"                          │\n","│                                                        │                                                        │\n","│                                                        │ The first Christians were                              │\n","├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n","│ To see the eiffel tower, people flock to the Eiffel    │ To see the eiffel tower, people flock to the           │\n","│ Tower.                                                 │ Colosseum.                                             │\n","│                                                        │                                                        │\n","│ It's a place of pilgrimage for many.                   │ The city is full of ancient monuments, including the   │\n","│                                                        │ Colosseum and the Pantheon.                            │\n","│ The Eiffel Tower is one of the most visited landmarks  │                                                        │\n","│ in Paris.                                              │ The city is also famous for its gardens, which are a   │\n","│                                                        │ UNESCO World Heritage site.                            │\n","│ But it's also a place of danger.                       │                                                        │\n","│                                                        │ The area                                               │\n","│ In the summer                                          │                                                        │\n","└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["unsteered_completions, steered_completions = calculate_and_apply_steering_vector(\n","    model,\n","    prompt = \"To see the eiffel tower, people flock to\",\n","    activation_additions = [\n","        (24, +10.0, \"The Eiffel Tower is in Rome\"),\n","        (24, -10.0, \"The Eiffel Tower is in France\"),\n","    ],\n","    n_tokens = 50,\n","    n_comparisons = 3,\n","    use_bos = False,\n",")\n","\n","table = Table(\"Unsteered\", \"Steered\", title=\"Completions\", show_lines=True)\n","for usc, sc in zip(unsteered_completions, steered_completions):\n","    table.add_row(usc, sc)\n","rprint(table)"]},{"cell_type":"markdown","id":"f591a732","metadata":{"id":"f591a732"},"source":["# 5️⃣ Bonus"]},{"cell_type":"markdown","id":"c6aed3ed","metadata":{"id":"c6aed3ed"},"source":["## Extensions of the Function Vectors Paper\n","\n","There are two other interesting results from the paper, although neither of them are as important as the ones we've covered so far. If you have time, you can try to reproduce these results yourself.\n","\n","### The Decoded Vocabulary of Function Vectors (3.2)\n","\n","In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:\n","\n","* For the antonyms task, the top words evoke the idea of antonyms, e.g. `\" negate\"`, `\" counterpart\"`, `\" lesser\"`.\n","* For the country-capitals task, the top words are actually the names of capitals, e.g. `\" Moscow\"`, `\" Paris\"`, `\" Madrid\"`.\n","\n","Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?\n","\n","An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?\n","\n","<details>\n","<summary>My results for this (spoiler!)</summary>\n","\n","In the Country-Capitals task, I found:\n","\n","* The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.\n","* The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.\n","\n","</details>\n","\n","### Vector Algebra on Function Vectors (3.3)\n","\n","In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?\n","\n","The authors test this on a variety of different tasks. They find that it's effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn't as effective as function vectors. Do you get these same results?"]},{"cell_type":"code","execution_count":null,"id":"551de49a","metadata":{"id":"551de49a"},"outputs":[],"source":["# Code to calculate decoded vocabulary:\n","logits = model._model.lm_head(fn_vector)\n","max_logits = logits.topk(20).indices.tolist()\n","tokens = model.tokenizer.batch_decode(max_logits)\n","print(tokens)"]},{"cell_type":"markdown","id":"eab20444","metadata":{"id":"eab20444"},"source":["## Extensions of the Steering Vectors Post\n","\n","We only implemented one small subset of the results from the steering vectors post (and did it in a fairly slap-dash way). But there are many others you can play around with. For example:\n","\n","* The authors note that they were unsuccessful in finding a \"speak in French\" vector. One of the top comments on the LessWrong post describes a process they used to create a French vector which happened to work (link to comment [here](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector?commentId=sqsS9QaDy2bG83XKP)). Can you replicate their results? (They also linked a Colab in this comment, which can help if you're stuck.)\n","* In a [later section](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#Perplexity_on_lots_of_sentences_about_weddings_or_about_shipping) of the paper, the authors extensively discuss perplexity (a measure which is related to entropy). They find that the \"weddings\" vector reduces perplexity on wedding-related sentences, and maintains perplexity on unrelated sentences. Can you replicate their results - in particular, their graph of perplexity ratios against injection layers for wedding and non-wedding-related sentences?\n","* The authors wrote up the post into a full paper, which you can find [here](https://arxiv.org/abs/2308.10248). Can you replicate some of the extra results in this paper?"]},{"cell_type":"markdown","id":"1d5fb46c","metadata":{"id":"1d5fb46c"},"source":["## Suggested paper replications\n","\n","### [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341)\n","\n","In this paper, the authors focus on inducing the behavioural change of \"making the model tell the truth\". They also look at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we've been using so far work the best.\n","\n","This might be a good replication for you if:\n","\n","* You enjoyed the exercises in this section, but are also interested in experimenting with techniques which weren't covered in this section (e.g. linear probing),\n","* You're comfortable working with very large models, possibly via the `nnsight` library,\n","* You're interested in studying [model truthfulness](https://arxiv.org/abs/2109.07958).\n","\n","### [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681)\n","\n","This paper can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model's change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).\n","\n","This might be a good replication for you if:\n","\n","* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context\n","* You're comfortable working with very large models, possibly via the `nnsight` library,\n","* You're interested in [evaluating models](https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written) on traits like myopia, power seeking, etc,\n","* You're comfortable doing prompt-engineering, and working with large datasets (like the ones linked above).\n","\n","*Update* - there is now a [LessWrong post](https://www.lesswrong.com/posts/v7f8ayBxLhmMFRzpa/steering-llama-2-with-contrastive-activation-additions) associated with this paper, which also briefly discusses related areas. We strongly recommend reading this post if you're interested in this replication, or any of the other suggested replications in this section.\n","\n","### [Red-teaming language models via activation engineering](https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering)\n","\n","This work, done by Nina Rimsky, extends the results from much of the work we've seen previously, but applied to the domain of **refusal** - what determines whether the LLM will refuse to answer your request, and how can you affect this behaviour? From her post:\n","\n","> *Validating if finetuning and RLHF have robustly achieved the intended outcome is challenging ... We can try to trigger unwanted behaviors in models more efficiently by manipulating their internal states during inference rather than searching through many inputs. The idea is that if a behavior can be easily triggered through techniques such as activation engineering, it may also occur in deployment. The inability to elicit behaviors via small internal perturbations could serve as a stronger guarantee of safety.*\n","\n","This might be a good replication for you if:\n","\n","* You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context,\n","* You're comfortable working with very large models, possibly via the `nnsight` library,\n","* You're interested in RLHF, adversarial attacks and jailbreaking,\n","* You're comfortable doing prompt-engineering (although some of the data you'd need for this replication is available on Nina's [GitHub repo](https://github.com/nrimsky/LM-exp/tree/main)).\n","\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","Note - for a week of work, we weakly suggest participants don't try one of these paper replications, because they're quite compute-heavy (even considering the fact that participants have the `nnsight` library at their disposal). There are many possible replications and extensions that can be done from the function vectors or GPT2-XL work, and this might be a better option for you if you enjoyed the exercises in this section and want to do more things like them.\n","\n","However, if you do feel comfortable working with large models (e.g. you have some past experience of this) and you're interested in this work, then you're certainly welcome to try one of these replications!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["zTqg0RAxiN-8","28a06fca-66e8-4c42-abe1-0e3c09d65c27","4UNdoTCCiU-z","X-aL04b3LOrx","BqA1xq5YieY6"],"gpuType":"T4","provenance":[{"file_id":"1TB1FJX5TxBXGzBBaSvRxSY_E5OPz4jQ3","timestamp":1715248182123}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6eeda3815c234c89b20e04d1e0d2e252":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c50df945a1c7420abf31660f8456b479","IPY_MODEL_67da4ec85f31455ea85ff777d47a68ed","IPY_MODEL_15069aee049f47638a722c8e008b2d18"],"layout":"IPY_MODEL_a5556689fb2c47c29a72fe2240e430dc"}},"c50df945a1c7420abf31660f8456b479":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be1237c7038549f691c03f0ef2bb4208","placeholder":"​","style":"IPY_MODEL_5fbb24555d2a47418f36029845ce29f2","value":"config.json: 100%"}},"67da4ec85f31455ea85ff777d47a68ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bd225996ea04f16b2e803775522af2f","max":689,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3eb746d1a2647ef8f87971e2f4ec380","value":689}},"15069aee049f47638a722c8e008b2d18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91d1563b4d1b41b0b0ba81393ff7a022","placeholder":"​","style":"IPY_MODEL_7eb6cda0b30c4264b4c219eac9a7b1ad","value":" 689/689 [00:00&lt;00:00, 63.6kB/s]"}},"a5556689fb2c47c29a72fe2240e430dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be1237c7038549f691c03f0ef2bb4208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fbb24555d2a47418f36029845ce29f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bd225996ea04f16b2e803775522af2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3eb746d1a2647ef8f87971e2f4ec380":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91d1563b4d1b41b0b0ba81393ff7a022":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eb6cda0b30c4264b4c219eac9a7b1ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0185870b55224ca4bbe658a1e4346106":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3665bc8b9e584e5294d1ae7e18a2cfec","IPY_MODEL_4fade92a64544803813e2655c5929bd1","IPY_MODEL_82500b5704124674b4e309a29c7d7fb1"],"layout":"IPY_MODEL_be40d8f2a0d14a3e8c50a70db0875bd5"}},"3665bc8b9e584e5294d1ae7e18a2cfec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c310797dde64995bd6674a54d62aa75","placeholder":"​","style":"IPY_MODEL_be399d10e61544d98fa9bf8502c67e3c","value":"tokenizer_config.json: 100%"}},"4fade92a64544803813e2655c5929bd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbcd187a3a7246d5b6e0171997f21c2a","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a2b7294bf5044c8842453d9f90692ef","value":26}},"82500b5704124674b4e309a29c7d7fb1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55d56eabbbfb49f8b9b920a869679c67","placeholder":"​","style":"IPY_MODEL_ad7b9435c0f44691915b00f459a9e973","value":" 26.0/26.0 [00:00&lt;00:00, 2.21kB/s]"}},"be40d8f2a0d14a3e8c50a70db0875bd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c310797dde64995bd6674a54d62aa75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be399d10e61544d98fa9bf8502c67e3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbcd187a3a7246d5b6e0171997f21c2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a2b7294bf5044c8842453d9f90692ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55d56eabbbfb49f8b9b920a869679c67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad7b9435c0f44691915b00f459a9e973":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c44a494d5e2640338871e6ff2fd6d8fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc10a13a2e0148e4aa39340116a365ba","IPY_MODEL_db26a58593144261ade2f2bccb1c5f9a","IPY_MODEL_11fb98ce337f4bc3ab1258aae93bb855"],"layout":"IPY_MODEL_62578c7e3d9344728013c696484d233d"}},"bc10a13a2e0148e4aa39340116a365ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d8a0379cfbe4de4ae0bce536b7020c5","placeholder":"​","style":"IPY_MODEL_8b3526b81c7c4fd291be4c4eb7b12152","value":"vocab.json: 100%"}},"db26a58593144261ade2f2bccb1c5f9a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50d2c96090ec4b0fae42ed50b9179acf","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e52919e341a048749554880739725f87","value":1042301}},"11fb98ce337f4bc3ab1258aae93bb855":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b7aa0d9993942468ee8f84847d97e9a","placeholder":"​","style":"IPY_MODEL_30bc6bce9da94929b31dc244fa26eea1","value":" 1.04M/1.04M [00:00&lt;00:00, 4.92MB/s]"}},"62578c7e3d9344728013c696484d233d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d8a0379cfbe4de4ae0bce536b7020c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b3526b81c7c4fd291be4c4eb7b12152":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50d2c96090ec4b0fae42ed50b9179acf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e52919e341a048749554880739725f87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b7aa0d9993942468ee8f84847d97e9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30bc6bce9da94929b31dc244fa26eea1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b93c7bf7e126436a92eb15f6b09ab6de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99b65b114e70403eb9312b2e0bc8cc75","IPY_MODEL_944d7eb4b3284be3b93928f181ebd164","IPY_MODEL_ee0e5d00406f4f7aa1d4b980278b1197"],"layout":"IPY_MODEL_79a6bbb549294f85a911a273b714d5af"}},"99b65b114e70403eb9312b2e0bc8cc75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d89dabcfca7f41e0b5b5454abd988f22","placeholder":"​","style":"IPY_MODEL_773dda3b246543838ebc464dafe82703","value":"merges.txt: 100%"}},"944d7eb4b3284be3b93928f181ebd164":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b07e4f754f714acc90cdf6bbd81a2f19","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_103450e9c16d488aac25795631106604","value":456318}},"ee0e5d00406f4f7aa1d4b980278b1197":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21207640df0e48948f1c87804a9a7454","placeholder":"​","style":"IPY_MODEL_c62bff2698fe42ebb2563522d652c8ee","value":" 456k/456k [00:00&lt;00:00, 33.4MB/s]"}},"79a6bbb549294f85a911a273b714d5af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d89dabcfca7f41e0b5b5454abd988f22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"773dda3b246543838ebc464dafe82703":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b07e4f754f714acc90cdf6bbd81a2f19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"103450e9c16d488aac25795631106604":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21207640df0e48948f1c87804a9a7454":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62bff2698fe42ebb2563522d652c8ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23adc7c94de146e3bb42417977d8d817":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d539feb88314664b88a85f4f8feead3","IPY_MODEL_57d4437f6c884b258892455526c11493","IPY_MODEL_d909c3f92234429c8207d9f665b78495"],"layout":"IPY_MODEL_48139d7169794c6a89320ee18de1a99c"}},"1d539feb88314664b88a85f4f8feead3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7288d04dc5e945ed94d27abcce46818e","placeholder":"​","style":"IPY_MODEL_c4902000155d4d8484b4d70371f8a821","value":"tokenizer.json: 100%"}},"57d4437f6c884b258892455526c11493":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c79b5444f434d22980e60ef01e212a0","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93430c1d78834bd0b2fb70223da6c685","value":1355256}},"d909c3f92234429c8207d9f665b78495":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d101bd1ad64146f5943b8d756c638321","placeholder":"​","style":"IPY_MODEL_4e405fb94de64f3ba0556773fd30ff0c","value":" 1.36M/1.36M [00:00&lt;00:00, 2.92MB/s]"}},"48139d7169794c6a89320ee18de1a99c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7288d04dc5e945ed94d27abcce46818e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4902000155d4d8484b4d70371f8a821":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c79b5444f434d22980e60ef01e212a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93430c1d78834bd0b2fb70223da6c685":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d101bd1ad64146f5943b8d756c638321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e405fb94de64f3ba0556773fd30ff0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}